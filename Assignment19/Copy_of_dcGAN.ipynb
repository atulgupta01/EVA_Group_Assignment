{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of dcGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1m20kYzXRF7ZU1DiJc8Xc1Mq_Ca5f0YNa",
      "authorship_tag": "ABX9TyNHdQ2wDJPD6jdTcAZ+uzdF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atulgupta01/EVA_Group_Assignment/blob/master/Assignment19/Copy_of_dcGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCWBX0LyNZCi",
        "colab_type": "code",
        "outputId": "579add04-f968-4b03-bf55-309b0136ab8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R2lakZ-Qg_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 64\n",
        "        self.img_cols = 64\n",
        "        self.channels = 3\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(512 * 8 * 8, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((8, 8, 512)))\n",
        "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        #model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, X_train, epochs, batch_size=128, save_interval=50,start_epoch_no=0):\n",
        "\n",
        "        \n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        #X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random half of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # Sample noise and generate a batch of new images\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the generator (wants discriminator to mistake images as real)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch+start_epoch_no, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if ((epoch+start_epoch_no) % save_interval) == 0:\n",
        "                self.save_imgs(epoch+start_epoch_no)\n",
        "                self.save_model()\n",
        "\n",
        "    def save_model(self):\n",
        "          self.generator.save_weights(\"/content/drive/My Drive/EVA_Assignment_19/modelGAN_generatorCopy.h5\")                \n",
        "          self.discriminator.save_weights(\"/content/drive/My Drive/EVA_Assignment_19/modelGAN_discriminatorCopy.h5\")                \n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "          self.generator.load_weights(\"/content/drive/My Drive/EVA_Assignment_19/modelGAN_generatorCopy.h5\")                \n",
        "          self.discriminator.load_weights(\"/content/drive/My Drive/EVA_Assignment_19/modelGAN_discriminatorCopy.h5\")  \n",
        "\n",
        "    def save_imgs(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,:], )\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"/content/drive/My Drive/Session19images/guns_%d.png\" % (epoch))\n",
        "        plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjek1673bXWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "DATA_PATH = '/content/drive/My Drive/EVA_Assignment_19'\n",
        "# make a list of frames in the aligned frames folder\n",
        "imgList = list(glob.iglob(os.path.join(DATA_PATH, '*.*')))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cQOwLjfWoBGO",
        "outputId": "5c6b15b8-86d5-4110-ded6-a40f089317f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_train=[]\n",
        "img1 = cv2.imread('/content/drive/My Drive/EVA_Assignment_19/91.jpg')\n",
        "img1 = cv2.resize(img1, (64,64), interpolation = cv2.INTER_AREA)\n",
        "X_train = np.expand_dims(img1,axis=0)\n",
        "for imgName in imgList:\n",
        "  print('Reading {}'.format(imgName))\n",
        "  img = cv2.imread(imgName)\n",
        "  rw,cl,ch = img.shape\n",
        "  print('size->({},{})'.format(rw,cl))\n",
        "  img = cv2.resize(img, (64,64), interpolation = cv2.INTER_AREA)\n",
        "  img = np.expand_dims(img,axis=0)\n",
        "  X_train=np.append(X_train,img,axis=0)\n",
        "  \n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading /content/drive/My Drive/EVA_Assignment_19/83.jpg\n",
            "size->(366,600)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/86.jpg\n",
            "size->(1000,1000)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/87.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/84.jpg\n",
            "size->(194,259)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/85.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/91.jpg\n",
            "size->(224,224)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/88.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/92.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/89.jpg\n",
            "size->(458,458)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/90.jpg\n",
            "size->(375,375)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/93.jpg\n",
            "size->(496,661)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/97.jpg\n",
            "size->(1200,1200)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/94.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/98.jpg\n",
            "size->(600,600)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/96.jpg\n",
            "size->(350,350)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/95.jpg\n",
            "size->(496,661)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/2.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/100.jpg\n",
            "size->(163,310)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/99.jpg\n",
            "size->(609,1024)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/1.jpg\n",
            "size->(439,679)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/3.jpg\n",
            "size->(187,269)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/10.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/7.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/8.jpg\n",
            "size->(183,275)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/4.jpg\n",
            "size->(189,267)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/9.jpeg\n",
            "size->(450,450)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/12.jpg\n",
            "size->(529,800)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/11.jpg\n",
            "size->(194,259)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/6.jpg\n",
            "size->(1200,1600)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/5.jpg\n",
            "size->(900,900)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/13.jpg\n",
            "size->(191,264)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/22.jpg\n",
            "size->(700,700)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/14.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/17.jpg\n",
            "size->(203,248)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/18.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/16.jpg\n",
            "size->(224,224)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/15.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/20.jpg\n",
            "size->(224,224)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/21.jpg\n",
            "size->(183,275)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/19.jpg\n",
            "size->(212,238)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/23.jpg\n",
            "size->(800,800)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/24.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/33.jpg\n",
            "size->(213,319)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/26.jpg\n",
            "size->(341,500)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/27.jpg\n",
            "size->(370,370)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/25.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/34.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/30.jpg\n",
            "size->(224,224)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/31.jpg\n",
            "size->(193,262)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/28.jpg\n",
            "size->(375,375)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/32.jpg\n",
            "size->(340,340)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/29.jpg\n",
            "size->(194,259)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/35.jpg\n",
            "size->(1300,1300)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/36.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/40.jpg\n",
            "size->(460,700)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/37.jpg\n",
            "size->(1000,1000)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/39.jpg\n",
            "size->(600,600)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/38.jpg\n",
            "size->(500,500)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/45.jpg\n",
            "size->(1280,1280)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/43.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/42.jpg\n",
            "size->(324,324)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/41.jpg\n",
            "size->(680,1024)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/44.jpg\n",
            "size->(1280,1280)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/46.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/47.jpg\n",
            "size->(500,427)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/48.jpeg\n",
            "size->(523,832)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/51.jpeg\n",
            "size->(594,832)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/50.jpg\n",
            "size->(995,850)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/49.jpg\n",
            "size->(223,500)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/54.jpg\n",
            "size->(720,1280)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/53.jpg\n",
            "size->(500,500)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/55.jpg\n",
            "size->(800,800)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/52.jpg\n",
            "size->(191,263)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/56.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/57.jpg\n",
            "size->(184,274)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/62.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/64.jpg\n",
            "size->(188,268)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/58.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/65.jpg\n",
            "size->(177,285)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/60.jpg\n",
            "size->(299,512)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/61.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/66.jpg\n",
            "size->(179,282)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/63.jpg\n",
            "size->(1200,1200)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/59.jpg\n",
            "size->(1600,2376)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/67.jpg\n",
            "size->(372,600)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/68.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/77.jpg\n",
            "size->(229,458)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/72.jpg\n",
            "size->(366,600)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/71.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/76.jpg\n",
            "size->(190,266)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/74.jpg\n",
            "size->(408,700)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/70.png\n",
            "size->(370,370)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/69.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/75.jpg\n",
            "size->(1200,1600)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/78.jpg\n",
            "size->(500,700)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/73.jpg\n",
            "size->(1200,1600)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/79.jpg\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/80.jpg\n",
            "size->(500,700)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/82.jpeg\n",
            "size->(900,900)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/81.jpg\n",
            "size->(1230,1230)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images.jpeg\n",
            "size->(302,301)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_003.jpeg\n",
            "size->(191,241)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_004.jpeg\n",
            "size->(183,275)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_012.jpeg\n",
            "size->(194,260)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_009.jpeg\n",
            "size->(133,193)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_014.jpeg\n",
            "size->(164,262)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_019.jpeg\n",
            "size->(274,312)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_022.png\n",
            "size->(225,225)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_026.jpeg\n",
            "size->(183,275)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_029.jpeg\n",
            "size->(165,267)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_033.jpeg\n",
            "size->(165,262)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_035.jpeg\n",
            "size->(239,278)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_036.jpeg\n",
            "size->(173,291)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_044.jpeg\n",
            "size->(169,262)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_089.jpeg\n",
            "size->(207,284)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_086.jpeg\n",
            "size->(241,301)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_066.jpeg\n",
            "size->(183,275)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_093.jpeg\n",
            "size->(174,266)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_060.jpeg\n",
            "size->(171,264)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_058.jpeg\n",
            "size->(166,304)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_072.jpeg\n",
            "size->(170,262)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_081.jpeg\n",
            "size->(192,262)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_092.jpeg\n",
            "size->(129,169)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_052.jpeg\n",
            "size->(152,189)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_082.jpeg\n",
            "size->(180,249)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_053.jpeg\n",
            "size->(171,263)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_094.jpeg\n",
            "size->(140,263)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_099.jpeg\n",
            "size->(162,217)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_101.jpeg\n",
            "size->(130,199)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_106.jpeg\n",
            "size->(105,210)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_114.jpeg\n",
            "size->(137,190)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_113.jpeg\n",
            "size->(165,272)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_121.jpeg\n",
            "size->(175,263)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_123.jpeg\n",
            "size->(149,213)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_124.jpeg\n",
            "size->(136,198)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_125.jpeg\n",
            "size->(97,145)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_131.jpeg\n",
            "size->(151,210)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_127.jpeg\n",
            "size->(131,200)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_137.jpeg\n",
            "size->(138,194)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_146.jpeg\n",
            "size->(145,192)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_140.jpeg\n",
            "size->(116,191)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_147.jpeg\n",
            "size->(133,195)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_156.jpeg\n",
            "size->(148,212)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_160.jpeg\n",
            "size->(183,275)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_162.jpeg\n",
            "size->(188,268)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_163.jpeg\n",
            "size->(156,223)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_112.jpeg\n",
            "size->(144,239)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_149.jpeg\n",
            "size->(150,200)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_178.jpeg\n",
            "size->(157,192)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_179.jpeg\n",
            "size->(120,203)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_175.jpeg\n",
            "size->(136,203)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_194.jpeg\n",
            "size->(113,180)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_180.jpeg\n",
            "size->(178,223)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_196.jpeg\n",
            "size->(134,253)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_200.jpeg\n",
            "size->(166,247)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_204.jpeg\n",
            "size->(172,219)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_198.jpeg\n",
            "size->(183,275)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_184.jpeg\n",
            "size->(142,189)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_199.jpeg\n",
            "size->(183,275)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_212.jpeg\n",
            "size->(167,263)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_195.jpeg\n",
            "size->(95,137)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_214.jpeg\n",
            "size->(168,251)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_216.jpeg\n",
            "size->(185,229)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_219.jpeg\n",
            "size->(163,202)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_222.jpeg\n",
            "size->(130,157)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_226.jpeg\n",
            "size->(165,305)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_233.jpeg\n",
            "size->(183,275)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_187.jpeg\n",
            "size->(168,299)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_241.jpeg\n",
            "size->(184,241)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_244.jpeg\n",
            "size->(183,275)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_235.jpeg\n",
            "size->(144,199)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_236.jpeg\n",
            "size->(119,181)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_237.jpeg\n",
            "size->(160,223)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_267.jpeg\n",
            "size->(190,234)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_269.jpeg\n",
            "size->(145,215)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_270.jpeg\n",
            "size->(141,204)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_308.jpeg\n",
            "size->(148,214)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_281.jpeg\n",
            "size->(158,220)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_285.jpeg\n",
            "size->(144,205)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_288.jpeg\n",
            "size->(147,193)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_325.jpeg\n",
            "size->(115,156)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_252.jpeg\n",
            "size->(135,194)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_352.jpeg\n",
            "size->(157,206)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_246.jpeg\n",
            "size->(145,214)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_360.jpeg\n",
            "size->(169,262)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_333.jpeg\n",
            "size->(133,191)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_348.jpeg\n",
            "size->(135,223)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_357.jpeg\n",
            "size->(173,263)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_305.jpeg\n",
            "size->(151,210)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_300.jpeg\n",
            "size->(152,199)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_336.jpeg\n",
            "size->(192,237)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/images_354.jpeg\n",
            "size->(164,262)\n",
            "Reading /content/drive/My Drive/EVA_Assignment_19/modelGAN_generatorCopy.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7501cd8a4d68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mrw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'size->({},{})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QPEFhSx_vW2s",
        "outputId": "9f94b775-ade3-4a6d-8506-20762aeb1167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dcgan = DCGAN()\n",
        "\n",
        "dcgan.train(X_train, epochs=4000, batch_size=32, save_interval=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 8193      \n",
            "=================================================================\n",
            "Total params: 2,171,713\n",
            "Trainable params: 2,169,281\n",
            "Non-trainable params: 2,432\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 32768)             3309568   \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 256)         1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 32, 32, 128)       295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 64, 64, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 64, 64, 3)         1731      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 64, 64, 3)         0         \n",
            "=================================================================\n",
            "Total params: 5,452,931\n",
            "Trainable params: 5,451,523\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c77813f3ff93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-ae7f0a4953ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, epochs, batch_size, save_interval, start_epoch_no)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m# Sample noise and generate a batch of new images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# Train the discriminator (real classified as ones and generated as zeros)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2959\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s6ma4POO53r",
        "colab_type": "code",
        "outputId": "967e0f0d-0acf-4756-bf4d-783f1a907171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dcgan = DCGAN()\n",
        "dcgan.load_model()\n",
        "\n",
        "dcgan.train(X_train, epochs=4000, batch_size=32, save_interval=50,start_epoch_no=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 8193      \n",
            "=================================================================\n",
            "Total params: 2,171,713\n",
            "Trainable params: 2,169,281\n",
            "Non-trainable params: 2,432\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 32768)             3309568   \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 8, 8, 256)         1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 32, 32, 128)       295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2 (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 64, 64, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 64, 64, 3)         1731      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 64, 64, 3)         0         \n",
            "=================================================================\n",
            "Total params: 5,452,931\n",
            "Trainable params: 5,451,523\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1020 [D loss: 0.884710, acc.: 62.50%] [G loss: 2.239958]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1021 [D loss: 0.111507, acc.: 96.88%] [G loss: 4.840251]\n",
            "1022 [D loss: 0.780687, acc.: 57.81%] [G loss: 2.519314]\n",
            "1023 [D loss: 0.656842, acc.: 75.00%] [G loss: 3.776372]\n",
            "1024 [D loss: 0.198959, acc.: 93.75%] [G loss: 3.485632]\n",
            "1025 [D loss: 0.568985, acc.: 70.31%] [G loss: 3.143655]\n",
            "1026 [D loss: 0.173976, acc.: 98.44%] [G loss: 4.074000]\n",
            "1027 [D loss: 0.325928, acc.: 89.06%] [G loss: 3.588336]\n",
            "1028 [D loss: 0.420398, acc.: 79.69%] [G loss: 2.809765]\n",
            "1029 [D loss: 0.357959, acc.: 85.94%] [G loss: 3.538410]\n",
            "1030 [D loss: 0.155805, acc.: 98.44%] [G loss: 2.949097]\n",
            "1031 [D loss: 0.525502, acc.: 78.12%] [G loss: 4.009979]\n",
            "1032 [D loss: 0.263236, acc.: 89.06%] [G loss: 4.205490]\n",
            "1033 [D loss: 0.527381, acc.: 75.00%] [G loss: 3.551036]\n",
            "1034 [D loss: 0.328756, acc.: 87.50%] [G loss: 3.767285]\n",
            "1035 [D loss: 0.431808, acc.: 75.00%] [G loss: 2.293175]\n",
            "1036 [D loss: 0.357438, acc.: 82.81%] [G loss: 4.456250]\n",
            "1037 [D loss: 0.590288, acc.: 76.56%] [G loss: 3.216221]\n",
            "1038 [D loss: 0.446631, acc.: 76.56%] [G loss: 3.375249]\n",
            "1039 [D loss: 0.193630, acc.: 92.19%] [G loss: 2.909678]\n",
            "1040 [D loss: 0.332113, acc.: 85.94%] [G loss: 3.269778]\n",
            "1041 [D loss: 0.120557, acc.: 98.44%] [G loss: 3.215279]\n",
            "1042 [D loss: 0.424921, acc.: 75.00%] [G loss: 3.683554]\n",
            "1043 [D loss: 0.277636, acc.: 89.06%] [G loss: 2.919501]\n",
            "1044 [D loss: 0.213957, acc.: 90.62%] [G loss: 4.161827]\n",
            "1045 [D loss: 0.231476, acc.: 93.75%] [G loss: 3.802263]\n",
            "1046 [D loss: 0.452001, acc.: 82.81%] [G loss: 3.658143]\n",
            "1047 [D loss: 0.243594, acc.: 87.50%] [G loss: 3.883357]\n",
            "1048 [D loss: 0.352416, acc.: 84.38%] [G loss: 4.139219]\n",
            "1049 [D loss: 0.289337, acc.: 90.62%] [G loss: 3.410995]\n",
            "1050 [D loss: 0.299933, acc.: 89.06%] [G loss: 4.355859]\n",
            "1051 [D loss: 0.747676, acc.: 56.25%] [G loss: 3.100597]\n",
            "1052 [D loss: 0.393623, acc.: 85.94%] [G loss: 4.563288]\n",
            "1053 [D loss: 0.266257, acc.: 87.50%] [G loss: 3.903913]\n",
            "1054 [D loss: 0.370453, acc.: 84.38%] [G loss: 3.322914]\n",
            "1055 [D loss: 0.141354, acc.: 98.44%] [G loss: 3.226911]\n",
            "1056 [D loss: 0.201892, acc.: 93.75%] [G loss: 3.597332]\n",
            "1057 [D loss: 0.567465, acc.: 78.12%] [G loss: 2.570048]\n",
            "1058 [D loss: 0.176999, acc.: 93.75%] [G loss: 3.653058]\n",
            "1059 [D loss: 0.521861, acc.: 81.25%] [G loss: 2.674475]\n",
            "1060 [D loss: 0.440537, acc.: 81.25%] [G loss: 2.852215]\n",
            "1061 [D loss: 0.329547, acc.: 84.38%] [G loss: 3.189125]\n",
            "1062 [D loss: 0.254866, acc.: 90.62%] [G loss: 3.523550]\n",
            "1063 [D loss: 0.364270, acc.: 84.38%] [G loss: 2.656432]\n",
            "1064 [D loss: 0.234664, acc.: 93.75%] [G loss: 2.634717]\n",
            "1065 [D loss: 0.573981, acc.: 76.56%] [G loss: 3.599536]\n",
            "1066 [D loss: 0.432537, acc.: 78.12%] [G loss: 4.529294]\n",
            "1067 [D loss: 0.292994, acc.: 90.62%] [G loss: 3.208828]\n",
            "1068 [D loss: 0.233734, acc.: 92.19%] [G loss: 2.874260]\n",
            "1069 [D loss: 0.203335, acc.: 93.75%] [G loss: 3.190943]\n",
            "1070 [D loss: 0.292563, acc.: 87.50%] [G loss: 3.082905]\n",
            "1071 [D loss: 0.284220, acc.: 85.94%] [G loss: 3.552182]\n",
            "1072 [D loss: 0.273619, acc.: 89.06%] [G loss: 3.810348]\n",
            "1073 [D loss: 0.736953, acc.: 59.38%] [G loss: 4.715357]\n",
            "1074 [D loss: 0.190010, acc.: 90.62%] [G loss: 3.655525]\n",
            "1075 [D loss: 0.262119, acc.: 89.06%] [G loss: 3.628112]\n",
            "1076 [D loss: 0.387340, acc.: 82.81%] [G loss: 4.191464]\n",
            "1077 [D loss: 0.358069, acc.: 89.06%] [G loss: 3.887558]\n",
            "1078 [D loss: 0.481578, acc.: 84.38%] [G loss: 3.319291]\n",
            "1079 [D loss: 0.163654, acc.: 96.88%] [G loss: 4.633820]\n",
            "1080 [D loss: 0.195780, acc.: 90.62%] [G loss: 3.149669]\n",
            "1081 [D loss: 0.316745, acc.: 89.06%] [G loss: 4.851379]\n",
            "1082 [D loss: 0.187728, acc.: 93.75%] [G loss: 3.330350]\n",
            "1083 [D loss: 0.307568, acc.: 85.94%] [G loss: 3.333610]\n",
            "1084 [D loss: 0.268053, acc.: 89.06%] [G loss: 3.282181]\n",
            "1085 [D loss: 0.385372, acc.: 79.69%] [G loss: 4.908806]\n",
            "1086 [D loss: 0.378195, acc.: 87.50%] [G loss: 3.091017]\n",
            "1087 [D loss: 0.626095, acc.: 65.62%] [G loss: 5.456092]\n",
            "1088 [D loss: 0.318721, acc.: 89.06%] [G loss: 3.527920]\n",
            "1089 [D loss: 0.723877, acc.: 67.19%] [G loss: 3.671866]\n",
            "1090 [D loss: 0.270386, acc.: 85.94%] [G loss: 4.853333]\n",
            "1091 [D loss: 0.364239, acc.: 76.56%] [G loss: 3.318865]\n",
            "1092 [D loss: 0.323125, acc.: 87.50%] [G loss: 5.148787]\n",
            "1093 [D loss: 0.577459, acc.: 76.56%] [G loss: 3.597508]\n",
            "1094 [D loss: 0.471349, acc.: 75.00%] [G loss: 4.342163]\n",
            "1095 [D loss: 0.096394, acc.: 98.44%] [G loss: 4.692186]\n",
            "1096 [D loss: 0.659252, acc.: 59.38%] [G loss: 3.275999]\n",
            "1097 [D loss: 0.391496, acc.: 81.25%] [G loss: 4.526923]\n",
            "1098 [D loss: 0.216587, acc.: 92.19%] [G loss: 3.753361]\n",
            "1099 [D loss: 0.175752, acc.: 95.31%] [G loss: 3.179739]\n",
            "1100 [D loss: 0.254213, acc.: 89.06%] [G loss: 4.138474]\n",
            "1101 [D loss: 0.502457, acc.: 76.56%] [G loss: 3.878930]\n",
            "1102 [D loss: 0.215290, acc.: 89.06%] [G loss: 4.390928]\n",
            "1103 [D loss: 0.281865, acc.: 90.62%] [G loss: 2.859033]\n",
            "1104 [D loss: 0.234113, acc.: 95.31%] [G loss: 4.108165]\n",
            "1105 [D loss: 0.270882, acc.: 89.06%] [G loss: 4.084908]\n",
            "1106 [D loss: 0.257714, acc.: 89.06%] [G loss: 4.215734]\n",
            "1107 [D loss: 0.188466, acc.: 95.31%] [G loss: 4.175426]\n",
            "1108 [D loss: 0.261712, acc.: 89.06%] [G loss: 3.530914]\n",
            "1109 [D loss: 0.331069, acc.: 82.81%] [G loss: 3.087137]\n",
            "1110 [D loss: 0.288865, acc.: 82.81%] [G loss: 4.181624]\n",
            "1111 [D loss: 0.071618, acc.: 98.44%] [G loss: 4.436161]\n",
            "1112 [D loss: 0.274634, acc.: 90.62%] [G loss: 2.795486]\n",
            "1113 [D loss: 0.170357, acc.: 98.44%] [G loss: 4.202928]\n",
            "1114 [D loss: 0.153928, acc.: 96.88%] [G loss: 3.669641]\n",
            "1115 [D loss: 0.836492, acc.: 54.69%] [G loss: 4.917849]\n",
            "1116 [D loss: 0.162625, acc.: 96.88%] [G loss: 4.063357]\n",
            "1117 [D loss: 0.497088, acc.: 71.88%] [G loss: 3.318693]\n",
            "1118 [D loss: 0.329929, acc.: 84.38%] [G loss: 3.643989]\n",
            "1119 [D loss: 0.111661, acc.: 98.44%] [G loss: 4.082666]\n",
            "1120 [D loss: 0.372506, acc.: 84.38%] [G loss: 3.899217]\n",
            "1121 [D loss: 0.150651, acc.: 95.31%] [G loss: 4.064066]\n",
            "1122 [D loss: 0.278854, acc.: 89.06%] [G loss: 3.527318]\n",
            "1123 [D loss: 0.168917, acc.: 95.31%] [G loss: 4.937444]\n",
            "1124 [D loss: 0.141939, acc.: 96.88%] [G loss: 4.544050]\n",
            "1125 [D loss: 0.453790, acc.: 75.00%] [G loss: 4.032755]\n",
            "1126 [D loss: 0.337180, acc.: 85.94%] [G loss: 5.717539]\n",
            "1127 [D loss: 0.257544, acc.: 92.19%] [G loss: 5.721487]\n",
            "1128 [D loss: 0.308356, acc.: 84.38%] [G loss: 4.012794]\n",
            "1129 [D loss: 0.140489, acc.: 98.44%] [G loss: 3.997069]\n",
            "1130 [D loss: 0.357892, acc.: 82.81%] [G loss: 4.548367]\n",
            "1131 [D loss: 0.399434, acc.: 76.56%] [G loss: 4.878782]\n",
            "1132 [D loss: 0.165143, acc.: 93.75%] [G loss: 3.923247]\n",
            "1133 [D loss: 0.183538, acc.: 93.75%] [G loss: 4.153803]\n",
            "1134 [D loss: 0.890317, acc.: 56.25%] [G loss: 6.439256]\n",
            "1135 [D loss: 0.428365, acc.: 73.44%] [G loss: 3.976654]\n",
            "1136 [D loss: 0.061407, acc.: 100.00%] [G loss: 3.505901]\n",
            "1137 [D loss: 0.123447, acc.: 93.75%] [G loss: 4.383720]\n",
            "1138 [D loss: 0.277345, acc.: 84.38%] [G loss: 4.468431]\n",
            "1139 [D loss: 0.230666, acc.: 90.62%] [G loss: 3.948134]\n",
            "1140 [D loss: 0.401568, acc.: 84.38%] [G loss: 3.437148]\n",
            "1141 [D loss: 0.339240, acc.: 85.94%] [G loss: 3.207869]\n",
            "1142 [D loss: 0.188633, acc.: 92.19%] [G loss: 3.901142]\n",
            "1143 [D loss: 0.185267, acc.: 92.19%] [G loss: 3.954029]\n",
            "1144 [D loss: 0.234779, acc.: 92.19%] [G loss: 3.164787]\n",
            "1145 [D loss: 0.466342, acc.: 75.00%] [G loss: 4.240623]\n",
            "1146 [D loss: 0.178246, acc.: 93.75%] [G loss: 4.367556]\n",
            "1147 [D loss: 0.352793, acc.: 85.94%] [G loss: 3.613783]\n",
            "1148 [D loss: 0.099466, acc.: 98.44%] [G loss: 4.329238]\n",
            "1149 [D loss: 0.469546, acc.: 76.56%] [G loss: 3.448468]\n",
            "1150 [D loss: 0.477734, acc.: 78.12%] [G loss: 4.747447]\n",
            "1151 [D loss: 0.264476, acc.: 87.50%] [G loss: 5.297854]\n",
            "1152 [D loss: 0.285420, acc.: 85.94%] [G loss: 4.094117]\n",
            "1153 [D loss: 0.119633, acc.: 98.44%] [G loss: 3.752279]\n",
            "1154 [D loss: 0.061102, acc.: 100.00%] [G loss: 4.128752]\n",
            "1155 [D loss: 0.505221, acc.: 70.31%] [G loss: 3.329730]\n",
            "1156 [D loss: 0.094278, acc.: 100.00%] [G loss: 3.514631]\n",
            "1157 [D loss: 0.324737, acc.: 89.06%] [G loss: 4.511671]\n",
            "1158 [D loss: 0.173241, acc.: 96.88%] [G loss: 3.433619]\n",
            "1159 [D loss: 0.305230, acc.: 89.06%] [G loss: 4.389631]\n",
            "1160 [D loss: 0.151187, acc.: 95.31%] [G loss: 3.597496]\n",
            "1161 [D loss: 0.194811, acc.: 90.62%] [G loss: 3.459129]\n",
            "1162 [D loss: 0.194672, acc.: 89.06%] [G loss: 2.979566]\n",
            "1163 [D loss: 0.144729, acc.: 92.19%] [G loss: 4.056486]\n",
            "1164 [D loss: 0.290356, acc.: 89.06%] [G loss: 5.546618]\n",
            "1165 [D loss: 0.618017, acc.: 73.44%] [G loss: 4.699714]\n",
            "1166 [D loss: 0.163044, acc.: 93.75%] [G loss: 4.409553]\n",
            "1167 [D loss: 0.224906, acc.: 87.50%] [G loss: 4.083255]\n",
            "1168 [D loss: 0.214227, acc.: 90.62%] [G loss: 3.550034]\n",
            "1169 [D loss: 0.623973, acc.: 76.56%] [G loss: 6.824789]\n",
            "1170 [D loss: 0.379194, acc.: 76.56%] [G loss: 3.842159]\n",
            "1171 [D loss: 0.156383, acc.: 95.31%] [G loss: 3.864387]\n",
            "1172 [D loss: 0.107164, acc.: 100.00%] [G loss: 4.791383]\n",
            "1173 [D loss: 0.201031, acc.: 93.75%] [G loss: 4.562055]\n",
            "1174 [D loss: 0.215943, acc.: 92.19%] [G loss: 4.553229]\n",
            "1175 [D loss: 0.160578, acc.: 93.75%] [G loss: 4.325512]\n",
            "1176 [D loss: 0.344205, acc.: 85.94%] [G loss: 2.403117]\n",
            "1177 [D loss: 0.394244, acc.: 81.25%] [G loss: 4.388278]\n",
            "1178 [D loss: 0.126321, acc.: 96.88%] [G loss: 4.469955]\n",
            "1179 [D loss: 0.226292, acc.: 89.06%] [G loss: 3.517325]\n",
            "1180 [D loss: 0.364460, acc.: 82.81%] [G loss: 6.428966]\n",
            "1181 [D loss: 0.388800, acc.: 82.81%] [G loss: 3.510063]\n",
            "1182 [D loss: 0.251708, acc.: 87.50%] [G loss: 4.599020]\n",
            "1183 [D loss: 0.102288, acc.: 98.44%] [G loss: 4.318060]\n",
            "1184 [D loss: 0.079819, acc.: 98.44%] [G loss: 3.928522]\n",
            "1185 [D loss: 0.109580, acc.: 96.88%] [G loss: 2.048974]\n",
            "1186 [D loss: 0.360700, acc.: 82.81%] [G loss: 5.443495]\n",
            "1187 [D loss: 0.128060, acc.: 95.31%] [G loss: 4.125276]\n",
            "1188 [D loss: 0.409040, acc.: 82.81%] [G loss: 3.179749]\n",
            "1189 [D loss: 0.251924, acc.: 92.19%] [G loss: 4.958091]\n",
            "1190 [D loss: 0.206899, acc.: 90.62%] [G loss: 4.392566]\n",
            "1191 [D loss: 0.290869, acc.: 89.06%] [G loss: 4.192062]\n",
            "1192 [D loss: 0.150016, acc.: 98.44%] [G loss: 4.273794]\n",
            "1193 [D loss: 0.182776, acc.: 93.75%] [G loss: 3.010758]\n",
            "1194 [D loss: 0.395767, acc.: 85.94%] [G loss: 5.119008]\n",
            "1195 [D loss: 0.175411, acc.: 93.75%] [G loss: 3.454867]\n",
            "1196 [D loss: 0.241306, acc.: 90.62%] [G loss: 4.866602]\n",
            "1197 [D loss: 0.289520, acc.: 85.94%] [G loss: 4.000513]\n",
            "1198 [D loss: 0.150798, acc.: 93.75%] [G loss: 4.657328]\n",
            "1199 [D loss: 0.462171, acc.: 76.56%] [G loss: 5.424148]\n",
            "1200 [D loss: 0.215537, acc.: 93.75%] [G loss: 4.457438]\n",
            "1201 [D loss: 0.054536, acc.: 100.00%] [G loss: 3.672511]\n",
            "1202 [D loss: 0.108233, acc.: 96.88%] [G loss: 3.716933]\n",
            "1203 [D loss: 0.267389, acc.: 90.62%] [G loss: 3.442513]\n",
            "1204 [D loss: 0.132311, acc.: 95.31%] [G loss: 4.273367]\n",
            "1205 [D loss: 0.349714, acc.: 85.94%] [G loss: 3.965803]\n",
            "1206 [D loss: 0.141778, acc.: 96.88%] [G loss: 4.991471]\n",
            "1207 [D loss: 0.141159, acc.: 95.31%] [G loss: 4.641174]\n",
            "1208 [D loss: 0.239038, acc.: 93.75%] [G loss: 4.493214]\n",
            "1209 [D loss: 0.429315, acc.: 78.12%] [G loss: 4.570082]\n",
            "1210 [D loss: 0.109450, acc.: 96.88%] [G loss: 4.248392]\n",
            "1211 [D loss: 0.208595, acc.: 89.06%] [G loss: 4.295512]\n",
            "1212 [D loss: 0.232296, acc.: 89.06%] [G loss: 5.160703]\n",
            "1213 [D loss: 0.126569, acc.: 96.88%] [G loss: 4.780599]\n",
            "1214 [D loss: 0.619134, acc.: 62.50%] [G loss: 5.441429]\n",
            "1215 [D loss: 0.044007, acc.: 100.00%] [G loss: 5.709249]\n",
            "1216 [D loss: 0.274673, acc.: 87.50%] [G loss: 4.595910]\n",
            "1217 [D loss: 0.236446, acc.: 93.75%] [G loss: 2.971698]\n",
            "1218 [D loss: 0.114938, acc.: 98.44%] [G loss: 4.827815]\n",
            "1219 [D loss: 0.391695, acc.: 82.81%] [G loss: 4.541427]\n",
            "1220 [D loss: 0.065560, acc.: 100.00%] [G loss: 3.695817]\n",
            "1221 [D loss: 0.207050, acc.: 92.19%] [G loss: 4.423370]\n",
            "1222 [D loss: 0.153539, acc.: 98.44%] [G loss: 3.369490]\n",
            "1223 [D loss: 0.204041, acc.: 90.62%] [G loss: 2.944653]\n",
            "1224 [D loss: 0.089858, acc.: 100.00%] [G loss: 3.119548]\n",
            "1225 [D loss: 0.297406, acc.: 87.50%] [G loss: 5.982679]\n",
            "1226 [D loss: 0.141342, acc.: 95.31%] [G loss: 5.481024]\n",
            "1227 [D loss: 0.302327, acc.: 90.62%] [G loss: 3.784782]\n",
            "1228 [D loss: 0.146477, acc.: 96.88%] [G loss: 3.438967]\n",
            "1229 [D loss: 0.085396, acc.: 98.44%] [G loss: 4.506186]\n",
            "1230 [D loss: 0.169293, acc.: 95.31%] [G loss: 2.608605]\n",
            "1231 [D loss: 0.063955, acc.: 100.00%] [G loss: 4.824527]\n",
            "1232 [D loss: 0.138481, acc.: 98.44%] [G loss: 3.007244]\n",
            "1233 [D loss: 0.244495, acc.: 90.62%] [G loss: 4.425594]\n",
            "1234 [D loss: 0.168580, acc.: 95.31%] [G loss: 4.824860]\n",
            "1235 [D loss: 0.200439, acc.: 93.75%] [G loss: 4.158095]\n",
            "1236 [D loss: 0.158671, acc.: 95.31%] [G loss: 4.542061]\n",
            "1237 [D loss: 0.112494, acc.: 98.44%] [G loss: 5.489345]\n",
            "1238 [D loss: 0.453679, acc.: 78.12%] [G loss: 3.346987]\n",
            "1239 [D loss: 0.080265, acc.: 98.44%] [G loss: 3.580686]\n",
            "1240 [D loss: 0.105186, acc.: 100.00%] [G loss: 4.959324]\n",
            "1241 [D loss: 0.086525, acc.: 100.00%] [G loss: 4.682927]\n",
            "1242 [D loss: 0.122204, acc.: 96.88%] [G loss: 3.356604]\n",
            "1243 [D loss: 0.081154, acc.: 96.88%] [G loss: 3.312126]\n",
            "1244 [D loss: 0.099114, acc.: 96.88%] [G loss: 4.116609]\n",
            "1245 [D loss: 0.742606, acc.: 62.50%] [G loss: 8.599757]\n",
            "1246 [D loss: 0.327477, acc.: 78.12%] [G loss: 5.535960]\n",
            "1247 [D loss: 0.130434, acc.: 98.44%] [G loss: 4.152938]\n",
            "1248 [D loss: 0.318134, acc.: 87.50%] [G loss: 3.948151]\n",
            "1249 [D loss: 0.057355, acc.: 100.00%] [G loss: 4.447591]\n",
            "1250 [D loss: 0.148223, acc.: 95.31%] [G loss: 4.060746]\n",
            "1251 [D loss: 0.303342, acc.: 85.94%] [G loss: 5.439204]\n",
            "1252 [D loss: 0.145848, acc.: 95.31%] [G loss: 4.243117]\n",
            "1253 [D loss: 0.495300, acc.: 78.12%] [G loss: 4.231270]\n",
            "1254 [D loss: 0.137538, acc.: 95.31%] [G loss: 4.620858]\n",
            "1255 [D loss: 0.399378, acc.: 84.38%] [G loss: 3.784937]\n",
            "1256 [D loss: 0.064597, acc.: 98.44%] [G loss: 4.745182]\n",
            "1257 [D loss: 0.180348, acc.: 93.75%] [G loss: 6.271242]\n",
            "1258 [D loss: 0.276968, acc.: 87.50%] [G loss: 4.073681]\n",
            "1259 [D loss: 0.298074, acc.: 89.06%] [G loss: 4.037283]\n",
            "1260 [D loss: 0.049267, acc.: 100.00%] [G loss: 3.081650]\n",
            "1261 [D loss: 0.274814, acc.: 87.50%] [G loss: 5.408920]\n",
            "1262 [D loss: 0.275623, acc.: 90.62%] [G loss: 6.189128]\n",
            "1263 [D loss: 0.324246, acc.: 85.94%] [G loss: 4.500670]\n",
            "1264 [D loss: 0.187408, acc.: 93.75%] [G loss: 4.175506]\n",
            "1265 [D loss: 0.081994, acc.: 100.00%] [G loss: 4.152648]\n",
            "1266 [D loss: 0.103568, acc.: 98.44%] [G loss: 3.617200]\n",
            "1267 [D loss: 0.151318, acc.: 93.75%] [G loss: 5.030980]\n",
            "1268 [D loss: 0.245624, acc.: 90.62%] [G loss: 5.076293]\n",
            "1269 [D loss: 0.322046, acc.: 84.38%] [G loss: 6.927385]\n",
            "1270 [D loss: 0.193173, acc.: 92.19%] [G loss: 4.544845]\n",
            "1271 [D loss: 0.061483, acc.: 100.00%] [G loss: 4.003646]\n",
            "1272 [D loss: 0.075764, acc.: 100.00%] [G loss: 4.017880]\n",
            "1273 [D loss: 0.099252, acc.: 96.88%] [G loss: 4.854563]\n",
            "1274 [D loss: 0.128186, acc.: 93.75%] [G loss: 3.460478]\n",
            "1275 [D loss: 0.063638, acc.: 100.00%] [G loss: 4.577459]\n",
            "1276 [D loss: 0.101632, acc.: 98.44%] [G loss: 3.841267]\n",
            "1277 [D loss: 0.242642, acc.: 90.62%] [G loss: 3.326251]\n",
            "1278 [D loss: 0.107600, acc.: 96.88%] [G loss: 6.075931]\n",
            "1279 [D loss: 0.218175, acc.: 89.06%] [G loss: 3.768499]\n",
            "1280 [D loss: 0.237147, acc.: 92.19%] [G loss: 5.510306]\n",
            "1281 [D loss: 0.658673, acc.: 65.62%] [G loss: 6.232310]\n",
            "1282 [D loss: 0.092580, acc.: 95.31%] [G loss: 5.888825]\n",
            "1283 [D loss: 0.150403, acc.: 95.31%] [G loss: 3.835965]\n",
            "1284 [D loss: 0.205718, acc.: 96.88%] [G loss: 5.357984]\n",
            "1285 [D loss: 0.136258, acc.: 96.88%] [G loss: 5.077110]\n",
            "1286 [D loss: 0.131429, acc.: 96.88%] [G loss: 3.468162]\n",
            "1287 [D loss: 0.369305, acc.: 89.06%] [G loss: 7.240687]\n",
            "1288 [D loss: 0.414190, acc.: 76.56%] [G loss: 2.969446]\n",
            "1289 [D loss: 0.126732, acc.: 96.88%] [G loss: 3.462026]\n",
            "1290 [D loss: 0.057113, acc.: 100.00%] [G loss: 4.259176]\n",
            "1291 [D loss: 0.195428, acc.: 96.88%] [G loss: 3.159559]\n",
            "1292 [D loss: 0.150124, acc.: 95.31%] [G loss: 5.804487]\n",
            "1293 [D loss: 0.080735, acc.: 98.44%] [G loss: 4.767649]\n",
            "1294 [D loss: 0.299025, acc.: 87.50%] [G loss: 3.492366]\n",
            "1295 [D loss: 0.177086, acc.: 95.31%] [G loss: 3.601063]\n",
            "1296 [D loss: 0.172102, acc.: 96.88%] [G loss: 4.182036]\n",
            "1297 [D loss: 0.089474, acc.: 98.44%] [G loss: 5.047795]\n",
            "1298 [D loss: 0.305031, acc.: 87.50%] [G loss: 3.558058]\n",
            "1299 [D loss: 0.152682, acc.: 96.88%] [G loss: 3.940361]\n",
            "1300 [D loss: 0.363653, acc.: 84.38%] [G loss: 7.423607]\n",
            "1301 [D loss: 0.201382, acc.: 90.62%] [G loss: 4.993115]\n",
            "1302 [D loss: 0.146669, acc.: 95.31%] [G loss: 5.482577]\n",
            "1303 [D loss: 0.212288, acc.: 92.19%] [G loss: 3.838863]\n",
            "1304 [D loss: 0.183704, acc.: 93.75%] [G loss: 4.372149]\n",
            "1305 [D loss: 0.246814, acc.: 85.94%] [G loss: 7.121387]\n",
            "1306 [D loss: 0.484242, acc.: 75.00%] [G loss: 3.359734]\n",
            "1307 [D loss: 0.314481, acc.: 85.94%] [G loss: 5.349380]\n",
            "1308 [D loss: 0.111972, acc.: 96.88%] [G loss: 5.879479]\n",
            "1309 [D loss: 0.281767, acc.: 89.06%] [G loss: 3.569274]\n",
            "1310 [D loss: 0.046804, acc.: 100.00%] [G loss: 5.638085]\n",
            "1311 [D loss: 0.194595, acc.: 90.62%] [G loss: 4.936341]\n",
            "1312 [D loss: 0.208805, acc.: 95.31%] [G loss: 5.326956]\n",
            "1313 [D loss: 0.235774, acc.: 93.75%] [G loss: 4.687288]\n",
            "1314 [D loss: 0.218841, acc.: 90.62%] [G loss: 3.535738]\n",
            "1315 [D loss: 0.077574, acc.: 96.88%] [G loss: 5.382318]\n",
            "1316 [D loss: 0.725326, acc.: 65.62%] [G loss: 4.916790]\n",
            "1317 [D loss: 0.093046, acc.: 96.88%] [G loss: 5.827161]\n",
            "1318 [D loss: 0.130492, acc.: 93.75%] [G loss: 5.531363]\n",
            "1319 [D loss: 0.292231, acc.: 89.06%] [G loss: 4.499201]\n",
            "1320 [D loss: 0.133561, acc.: 93.75%] [G loss: 4.502692]\n",
            "1321 [D loss: 0.103269, acc.: 96.88%] [G loss: 4.913042]\n",
            "1322 [D loss: 0.105181, acc.: 96.88%] [G loss: 4.595882]\n",
            "1323 [D loss: 0.197314, acc.: 92.19%] [G loss: 5.670857]\n",
            "1324 [D loss: 0.135127, acc.: 95.31%] [G loss: 4.422173]\n",
            "1325 [D loss: 0.182707, acc.: 92.19%] [G loss: 5.939337]\n",
            "1326 [D loss: 0.333407, acc.: 87.50%] [G loss: 3.404500]\n",
            "1327 [D loss: 0.398186, acc.: 82.81%] [G loss: 6.067380]\n",
            "1328 [D loss: 0.213000, acc.: 92.19%] [G loss: 5.213870]\n",
            "1329 [D loss: 0.291008, acc.: 84.38%] [G loss: 6.519264]\n",
            "1330 [D loss: 0.141270, acc.: 92.19%] [G loss: 3.935115]\n",
            "1331 [D loss: 0.072413, acc.: 98.44%] [G loss: 4.917954]\n",
            "1332 [D loss: 0.126142, acc.: 95.31%] [G loss: 5.697269]\n",
            "1333 [D loss: 0.106648, acc.: 96.88%] [G loss: 4.861139]\n",
            "1334 [D loss: 0.375197, acc.: 81.25%] [G loss: 3.757546]\n",
            "1335 [D loss: 0.075722, acc.: 98.44%] [G loss: 4.521371]\n",
            "1336 [D loss: 0.195211, acc.: 90.62%] [G loss: 3.596278]\n",
            "1337 [D loss: 0.084897, acc.: 98.44%] [G loss: 4.921536]\n",
            "1338 [D loss: 0.305440, acc.: 89.06%] [G loss: 4.083826]\n",
            "1339 [D loss: 0.128455, acc.: 95.31%] [G loss: 4.388877]\n",
            "1340 [D loss: 0.033292, acc.: 100.00%] [G loss: 5.350032]\n",
            "1341 [D loss: 0.203811, acc.: 90.62%] [G loss: 4.295494]\n",
            "1342 [D loss: 0.059524, acc.: 100.00%] [G loss: 3.698884]\n",
            "1343 [D loss: 0.045337, acc.: 100.00%] [G loss: 2.937833]\n",
            "1344 [D loss: 0.094981, acc.: 95.31%] [G loss: 3.846646]\n",
            "1345 [D loss: 0.846665, acc.: 59.38%] [G loss: 9.611621]\n",
            "1346 [D loss: 0.395502, acc.: 81.25%] [G loss: 6.572956]\n",
            "1347 [D loss: 0.150938, acc.: 95.31%] [G loss: 5.793832]\n",
            "1348 [D loss: 0.035125, acc.: 98.44%] [G loss: 5.435551]\n",
            "1349 [D loss: 0.107985, acc.: 96.88%] [G loss: 3.702531]\n",
            "1350 [D loss: 0.248340, acc.: 87.50%] [G loss: 6.371212]\n",
            "1351 [D loss: 0.190395, acc.: 93.75%] [G loss: 4.673980]\n",
            "1352 [D loss: 0.272824, acc.: 87.50%] [G loss: 3.730705]\n",
            "1353 [D loss: 0.117443, acc.: 96.88%] [G loss: 5.155413]\n",
            "1354 [D loss: 0.113949, acc.: 96.88%] [G loss: 4.289744]\n",
            "1355 [D loss: 0.192259, acc.: 93.75%] [G loss: 5.229818]\n",
            "1356 [D loss: 0.104824, acc.: 98.44%] [G loss: 6.329144]\n",
            "1357 [D loss: 0.125876, acc.: 96.88%] [G loss: 4.290240]\n",
            "1358 [D loss: 0.207111, acc.: 93.75%] [G loss: 3.973364]\n",
            "1359 [D loss: 0.130788, acc.: 93.75%] [G loss: 4.002122]\n",
            "1360 [D loss: 0.060646, acc.: 98.44%] [G loss: 5.244534]\n",
            "1361 [D loss: 0.045683, acc.: 100.00%] [G loss: 3.386192]\n",
            "1362 [D loss: 0.297759, acc.: 82.81%] [G loss: 5.810958]\n",
            "1363 [D loss: 0.152615, acc.: 93.75%] [G loss: 4.660727]\n",
            "1364 [D loss: 0.117724, acc.: 96.88%] [G loss: 3.922420]\n",
            "1365 [D loss: 0.267295, acc.: 90.62%] [G loss: 4.221138]\n",
            "1366 [D loss: 0.053983, acc.: 100.00%] [G loss: 5.087473]\n",
            "1367 [D loss: 0.109736, acc.: 98.44%] [G loss: 3.586053]\n",
            "1368 [D loss: 0.051369, acc.: 98.44%] [G loss: 3.589170]\n",
            "1369 [D loss: 0.274316, acc.: 87.50%] [G loss: 5.772453]\n",
            "1370 [D loss: 0.293764, acc.: 87.50%] [G loss: 5.727751]\n",
            "1371 [D loss: 0.181670, acc.: 93.75%] [G loss: 3.274916]\n",
            "1372 [D loss: 0.057660, acc.: 100.00%] [G loss: 4.390141]\n",
            "1373 [D loss: 0.250716, acc.: 92.19%] [G loss: 3.152975]\n",
            "1374 [D loss: 0.017079, acc.: 100.00%] [G loss: 3.055249]\n",
            "1375 [D loss: 0.083475, acc.: 95.31%] [G loss: 5.219577]\n",
            "1376 [D loss: 0.019939, acc.: 100.00%] [G loss: 5.966604]\n",
            "1377 [D loss: 0.198397, acc.: 93.75%] [G loss: 4.080468]\n",
            "1378 [D loss: 0.322518, acc.: 84.38%] [G loss: 7.288828]\n",
            "1379 [D loss: 0.290058, acc.: 90.62%] [G loss: 6.167375]\n",
            "1380 [D loss: 0.284655, acc.: 89.06%] [G loss: 5.317662]\n",
            "1381 [D loss: 0.022150, acc.: 100.00%] [G loss: 5.465070]\n",
            "1382 [D loss: 0.034553, acc.: 98.44%] [G loss: 5.887274]\n",
            "1383 [D loss: 0.307884, acc.: 85.94%] [G loss: 4.820895]\n",
            "1384 [D loss: 0.105645, acc.: 95.31%] [G loss: 4.456908]\n",
            "1385 [D loss: 0.105640, acc.: 96.88%] [G loss: 4.873796]\n",
            "1386 [D loss: 0.186024, acc.: 92.19%] [G loss: 3.986756]\n",
            "1387 [D loss: 0.045448, acc.: 98.44%] [G loss: 4.886825]\n",
            "1388 [D loss: 0.298871, acc.: 87.50%] [G loss: 7.775191]\n",
            "1389 [D loss: 0.289403, acc.: 84.38%] [G loss: 4.383129]\n",
            "1390 [D loss: 0.069787, acc.: 100.00%] [G loss: 4.399855]\n",
            "1391 [D loss: 0.217149, acc.: 90.62%] [G loss: 6.123392]\n",
            "1392 [D loss: 0.064272, acc.: 100.00%] [G loss: 5.340417]\n",
            "1393 [D loss: 0.109246, acc.: 98.44%] [G loss: 5.628216]\n",
            "1394 [D loss: 0.272510, acc.: 92.19%] [G loss: 6.037329]\n",
            "1395 [D loss: 0.025148, acc.: 100.00%] [G loss: 5.534013]\n",
            "1396 [D loss: 0.115817, acc.: 100.00%] [G loss: 2.764460]\n",
            "1397 [D loss: 0.081682, acc.: 100.00%] [G loss: 5.214349]\n",
            "1398 [D loss: 0.076028, acc.: 100.00%] [G loss: 4.482015]\n",
            "1399 [D loss: 0.106999, acc.: 98.44%] [G loss: 4.175673]\n",
            "1400 [D loss: 0.081976, acc.: 100.00%] [G loss: 2.968678]\n",
            "1401 [D loss: 0.107690, acc.: 98.44%] [G loss: 4.931315]\n",
            "1402 [D loss: 0.089862, acc.: 100.00%] [G loss: 6.150648]\n",
            "1403 [D loss: 0.026870, acc.: 100.00%] [G loss: 4.805712]\n",
            "1404 [D loss: 0.129411, acc.: 98.44%] [G loss: 6.353045]\n",
            "1405 [D loss: 0.141838, acc.: 95.31%] [G loss: 3.804627]\n",
            "1406 [D loss: 0.084624, acc.: 100.00%] [G loss: 4.010675]\n",
            "1407 [D loss: 0.032517, acc.: 100.00%] [G loss: 4.961510]\n",
            "1408 [D loss: 0.086566, acc.: 98.44%] [G loss: 3.630581]\n",
            "1409 [D loss: 0.095129, acc.: 96.88%] [G loss: 4.755471]\n",
            "1410 [D loss: 0.176494, acc.: 93.75%] [G loss: 5.859406]\n",
            "1411 [D loss: 0.619942, acc.: 67.19%] [G loss: 5.823369]\n",
            "1412 [D loss: 0.013650, acc.: 100.00%] [G loss: 4.229780]\n",
            "1413 [D loss: 0.081409, acc.: 98.44%] [G loss: 4.298658]\n",
            "1414 [D loss: 0.141115, acc.: 95.31%] [G loss: 5.247060]\n",
            "1415 [D loss: 0.193484, acc.: 89.06%] [G loss: 6.649980]\n",
            "1416 [D loss: 0.122806, acc.: 98.44%] [G loss: 3.782733]\n",
            "1417 [D loss: 0.149419, acc.: 96.88%] [G loss: 2.588644]\n",
            "1418 [D loss: 0.219388, acc.: 93.75%] [G loss: 7.288821]\n",
            "1419 [D loss: 0.053304, acc.: 100.00%] [G loss: 7.019956]\n",
            "1420 [D loss: 0.702556, acc.: 57.81%] [G loss: 5.550389]\n",
            "1421 [D loss: 0.056372, acc.: 98.44%] [G loss: 5.836871]\n",
            "1422 [D loss: 0.060693, acc.: 98.44%] [G loss: 5.563034]\n",
            "1423 [D loss: 0.161320, acc.: 95.31%] [G loss: 4.802430]\n",
            "1424 [D loss: 0.220621, acc.: 89.06%] [G loss: 8.862194]\n",
            "1425 [D loss: 0.865653, acc.: 59.38%] [G loss: 5.719592]\n",
            "1426 [D loss: 0.176952, acc.: 95.31%] [G loss: 6.005767]\n",
            "1427 [D loss: 0.333187, acc.: 81.25%] [G loss: 4.406365]\n",
            "1428 [D loss: 0.082527, acc.: 98.44%] [G loss: 5.423126]\n",
            "1429 [D loss: 0.089661, acc.: 100.00%] [G loss: 5.116555]\n",
            "1430 [D loss: 0.264039, acc.: 93.75%] [G loss: 4.352564]\n",
            "1431 [D loss: 0.023744, acc.: 100.00%] [G loss: 4.526146]\n",
            "1432 [D loss: 0.072528, acc.: 96.88%] [G loss: 4.735518]\n",
            "1433 [D loss: 0.066059, acc.: 100.00%] [G loss: 5.806287]\n",
            "1434 [D loss: 0.062827, acc.: 98.44%] [G loss: 5.491022]\n",
            "1435 [D loss: 0.087443, acc.: 98.44%] [G loss: 4.506851]\n",
            "1436 [D loss: 0.066174, acc.: 100.00%] [G loss: 4.122846]\n",
            "1437 [D loss: 0.103075, acc.: 96.88%] [G loss: 3.536718]\n",
            "1438 [D loss: 0.075708, acc.: 98.44%] [G loss: 6.322261]\n",
            "1439 [D loss: 0.154982, acc.: 93.75%] [G loss: 5.271306]\n",
            "1440 [D loss: 0.057608, acc.: 100.00%] [G loss: 4.256882]\n",
            "1441 [D loss: 0.206641, acc.: 93.75%] [G loss: 4.079591]\n",
            "1442 [D loss: 0.030198, acc.: 100.00%] [G loss: 4.745376]\n",
            "1443 [D loss: 0.255958, acc.: 87.50%] [G loss: 5.129100]\n",
            "1444 [D loss: 0.188931, acc.: 90.62%] [G loss: 4.827583]\n",
            "1445 [D loss: 0.067598, acc.: 98.44%] [G loss: 4.499116]\n",
            "1446 [D loss: 0.028256, acc.: 100.00%] [G loss: 4.991852]\n",
            "1447 [D loss: 0.148290, acc.: 95.31%] [G loss: 4.355821]\n",
            "1448 [D loss: 0.037561, acc.: 100.00%] [G loss: 5.846634]\n",
            "1449 [D loss: 0.057777, acc.: 100.00%] [G loss: 4.689676]\n",
            "1450 [D loss: 0.082188, acc.: 98.44%] [G loss: 3.762680]\n",
            "1451 [D loss: 0.246111, acc.: 87.50%] [G loss: 6.320828]\n",
            "1452 [D loss: 0.183103, acc.: 96.88%] [G loss: 4.092372]\n",
            "1453 [D loss: 0.181875, acc.: 90.62%] [G loss: 6.888735]\n",
            "1454 [D loss: 0.670474, acc.: 62.50%] [G loss: 6.761494]\n",
            "1455 [D loss: 0.010912, acc.: 100.00%] [G loss: 7.928625]\n",
            "1456 [D loss: 0.157152, acc.: 93.75%] [G loss: 3.607850]\n",
            "1457 [D loss: 0.461935, acc.: 78.12%] [G loss: 9.379906]\n",
            "1458 [D loss: 0.273511, acc.: 84.38%] [G loss: 6.720876]\n",
            "1459 [D loss: 0.132556, acc.: 96.88%] [G loss: 4.028101]\n",
            "1460 [D loss: 0.144974, acc.: 96.88%] [G loss: 5.870727]\n",
            "1461 [D loss: 0.012699, acc.: 100.00%] [G loss: 6.345422]\n",
            "1462 [D loss: 0.460469, acc.: 82.81%] [G loss: 4.427247]\n",
            "1463 [D loss: 0.070914, acc.: 100.00%] [G loss: 3.867581]\n",
            "1464 [D loss: 0.116703, acc.: 98.44%] [G loss: 4.444396]\n",
            "1465 [D loss: 0.325421, acc.: 85.94%] [G loss: 9.028198]\n",
            "1466 [D loss: 0.392642, acc.: 78.12%] [G loss: 6.682207]\n",
            "1467 [D loss: 0.044381, acc.: 96.88%] [G loss: 6.947351]\n",
            "1468 [D loss: 0.506297, acc.: 73.44%] [G loss: 8.384343]\n",
            "1469 [D loss: 0.025154, acc.: 100.00%] [G loss: 10.223133]\n",
            "1470 [D loss: 0.578090, acc.: 73.44%] [G loss: 3.146729]\n",
            "1471 [D loss: 0.156888, acc.: 93.75%] [G loss: 4.791960]\n",
            "1472 [D loss: 0.030503, acc.: 100.00%] [G loss: 5.996318]\n",
            "1473 [D loss: 0.244267, acc.: 87.50%] [G loss: 3.882240]\n",
            "1474 [D loss: 0.144260, acc.: 93.75%] [G loss: 6.645927]\n",
            "1475 [D loss: 0.054282, acc.: 98.44%] [G loss: 6.411873]\n",
            "1476 [D loss: 0.141678, acc.: 93.75%] [G loss: 4.419316]\n",
            "1477 [D loss: 0.065840, acc.: 100.00%] [G loss: 4.902062]\n",
            "1478 [D loss: 0.172258, acc.: 96.88%] [G loss: 6.312097]\n",
            "1479 [D loss: 0.037613, acc.: 100.00%] [G loss: 5.825433]\n",
            "1480 [D loss: 0.057820, acc.: 100.00%] [G loss: 4.619055]\n",
            "1481 [D loss: 0.593696, acc.: 68.75%] [G loss: 10.126925]\n",
            "1482 [D loss: 0.335746, acc.: 85.94%] [G loss: 8.847136]\n",
            "1483 [D loss: 0.106975, acc.: 96.88%] [G loss: 4.703629]\n",
            "1484 [D loss: 0.042166, acc.: 100.00%] [G loss: 4.220262]\n",
            "1485 [D loss: 0.055561, acc.: 98.44%] [G loss: 5.289074]\n",
            "1486 [D loss: 0.025488, acc.: 100.00%] [G loss: 5.535908]\n",
            "1487 [D loss: 0.111802, acc.: 96.88%] [G loss: 6.171003]\n",
            "1488 [D loss: 0.078033, acc.: 98.44%] [G loss: 4.694813]\n",
            "1489 [D loss: 0.174525, acc.: 90.62%] [G loss: 3.787608]\n",
            "1490 [D loss: 0.127369, acc.: 95.31%] [G loss: 4.925969]\n",
            "1491 [D loss: 0.123344, acc.: 96.88%] [G loss: 4.973778]\n",
            "1492 [D loss: 0.163284, acc.: 92.19%] [G loss: 6.071655]\n",
            "1493 [D loss: 0.134757, acc.: 96.88%] [G loss: 5.075524]\n",
            "1494 [D loss: 0.071502, acc.: 100.00%] [G loss: 4.504937]\n",
            "1495 [D loss: 0.044320, acc.: 100.00%] [G loss: 5.309447]\n",
            "1496 [D loss: 0.040790, acc.: 100.00%] [G loss: 4.091836]\n",
            "1497 [D loss: 0.418682, acc.: 78.12%] [G loss: 7.894561]\n",
            "1498 [D loss: 0.130625, acc.: 95.31%] [G loss: 8.884140]\n",
            "1499 [D loss: 0.242577, acc.: 92.19%] [G loss: 3.786187]\n",
            "1500 [D loss: 0.140849, acc.: 93.75%] [G loss: 6.225974]\n",
            "1501 [D loss: 0.020102, acc.: 100.00%] [G loss: 7.804307]\n",
            "1502 [D loss: 0.038061, acc.: 100.00%] [G loss: 5.992846]\n",
            "1503 [D loss: 0.033368, acc.: 100.00%] [G loss: 3.987659]\n",
            "1504 [D loss: 0.031183, acc.: 100.00%] [G loss: 4.512438]\n",
            "1505 [D loss: 0.181365, acc.: 95.31%] [G loss: 5.978236]\n",
            "1506 [D loss: 0.136887, acc.: 93.75%] [G loss: 7.156400]\n",
            "1507 [D loss: 0.879136, acc.: 56.25%] [G loss: 7.983344]\n",
            "1508 [D loss: 0.053534, acc.: 100.00%] [G loss: 8.332237]\n",
            "1509 [D loss: 0.183002, acc.: 93.75%] [G loss: 5.439285]\n",
            "1510 [D loss: 0.313190, acc.: 81.25%] [G loss: 8.183035]\n",
            "1511 [D loss: 0.080786, acc.: 96.88%] [G loss: 10.552079]\n",
            "1512 [D loss: 0.377636, acc.: 81.25%] [G loss: 4.225440]\n",
            "1513 [D loss: 0.298263, acc.: 84.38%] [G loss: 6.569332]\n",
            "1514 [D loss: 0.014681, acc.: 100.00%] [G loss: 9.535660]\n",
            "1515 [D loss: 0.504249, acc.: 82.81%] [G loss: 1.936360]\n",
            "1516 [D loss: 0.212754, acc.: 89.06%] [G loss: 5.076555]\n",
            "1517 [D loss: 0.069783, acc.: 98.44%] [G loss: 7.186143]\n",
            "1518 [D loss: 0.287981, acc.: 87.50%] [G loss: 3.597918]\n",
            "1519 [D loss: 0.970908, acc.: 57.81%] [G loss: 11.596099]\n",
            "1520 [D loss: 0.914106, acc.: 64.06%] [G loss: 5.433328]\n",
            "1521 [D loss: 0.124947, acc.: 96.88%] [G loss: 5.303469]\n",
            "1522 [D loss: 0.181329, acc.: 96.88%] [G loss: 3.899345]\n",
            "1523 [D loss: 0.237317, acc.: 90.62%] [G loss: 5.106874]\n",
            "1524 [D loss: 0.250433, acc.: 92.19%] [G loss: 5.073765]\n",
            "1525 [D loss: 0.081992, acc.: 98.44%] [G loss: 4.171550]\n",
            "1526 [D loss: 0.045685, acc.: 100.00%] [G loss: 4.839303]\n",
            "1527 [D loss: 0.034978, acc.: 100.00%] [G loss: 3.837397]\n",
            "1528 [D loss: 0.160331, acc.: 92.19%] [G loss: 4.872680]\n",
            "1529 [D loss: 0.103500, acc.: 98.44%] [G loss: 6.461478]\n",
            "1530 [D loss: 0.678871, acc.: 67.19%] [G loss: 4.274680]\n",
            "1531 [D loss: 0.079433, acc.: 96.88%] [G loss: 6.718199]\n",
            "1532 [D loss: 0.038044, acc.: 98.44%] [G loss: 6.027512]\n",
            "1533 [D loss: 0.581092, acc.: 71.88%] [G loss: 4.939145]\n",
            "1534 [D loss: 0.033190, acc.: 98.44%] [G loss: 6.612188]\n",
            "1535 [D loss: 0.048433, acc.: 98.44%] [G loss: 4.002575]\n",
            "1536 [D loss: 0.212635, acc.: 95.31%] [G loss: 6.457757]\n",
            "1537 [D loss: 0.454376, acc.: 78.12%] [G loss: 4.419401]\n",
            "1538 [D loss: 0.150408, acc.: 96.88%] [G loss: 4.028512]\n",
            "1539 [D loss: 0.102139, acc.: 98.44%] [G loss: 4.668443]\n",
            "1540 [D loss: 0.111332, acc.: 98.44%] [G loss: 6.115214]\n",
            "1541 [D loss: 0.068184, acc.: 98.44%] [G loss: 4.725396]\n",
            "1542 [D loss: 0.077633, acc.: 98.44%] [G loss: 4.446531]\n",
            "1543 [D loss: 0.144865, acc.: 98.44%] [G loss: 4.582109]\n",
            "1544 [D loss: 0.028106, acc.: 100.00%] [G loss: 6.340187]\n",
            "1545 [D loss: 0.128673, acc.: 96.88%] [G loss: 4.379572]\n",
            "1546 [D loss: 0.112586, acc.: 98.44%] [G loss: 4.825120]\n",
            "1547 [D loss: 0.117392, acc.: 96.88%] [G loss: 4.024058]\n",
            "1548 [D loss: 0.177419, acc.: 93.75%] [G loss: 8.461170]\n",
            "1549 [D loss: 0.215710, acc.: 87.50%] [G loss: 5.446057]\n",
            "1550 [D loss: 0.078603, acc.: 98.44%] [G loss: 6.432027]\n",
            "1551 [D loss: 0.057100, acc.: 100.00%] [G loss: 6.258430]\n",
            "1552 [D loss: 0.109869, acc.: 100.00%] [G loss: 4.816914]\n",
            "1553 [D loss: 0.066350, acc.: 100.00%] [G loss: 5.062501]\n",
            "1554 [D loss: 0.072614, acc.: 100.00%] [G loss: 4.549203]\n",
            "1555 [D loss: 0.084340, acc.: 98.44%] [G loss: 5.642007]\n",
            "1556 [D loss: 0.063907, acc.: 100.00%] [G loss: 5.742511]\n",
            "1557 [D loss: 0.688588, acc.: 70.31%] [G loss: 6.973825]\n",
            "1558 [D loss: 0.055612, acc.: 100.00%] [G loss: 6.809985]\n",
            "1559 [D loss: 0.091322, acc.: 98.44%] [G loss: 5.982565]\n",
            "1560 [D loss: 0.068822, acc.: 100.00%] [G loss: 3.666443]\n",
            "1561 [D loss: 0.023814, acc.: 100.00%] [G loss: 4.959427]\n",
            "1562 [D loss: 0.052856, acc.: 100.00%] [G loss: 4.941500]\n",
            "1563 [D loss: 0.052010, acc.: 98.44%] [G loss: 4.656512]\n",
            "1564 [D loss: 0.152604, acc.: 92.19%] [G loss: 5.645358]\n",
            "1565 [D loss: 0.048452, acc.: 100.00%] [G loss: 6.432364]\n",
            "1566 [D loss: 0.817176, acc.: 54.69%] [G loss: 8.103140]\n",
            "1567 [D loss: 0.057947, acc.: 100.00%] [G loss: 8.014278]\n",
            "1568 [D loss: 0.265529, acc.: 87.50%] [G loss: 5.200315]\n",
            "1569 [D loss: 0.157056, acc.: 92.19%] [G loss: 6.087205]\n",
            "1570 [D loss: 0.007192, acc.: 100.00%] [G loss: 6.682360]\n",
            "1571 [D loss: 0.059855, acc.: 100.00%] [G loss: 4.247263]\n",
            "1572 [D loss: 0.090155, acc.: 98.44%] [G loss: 4.982964]\n",
            "1573 [D loss: 0.254020, acc.: 87.50%] [G loss: 4.762853]\n",
            "1574 [D loss: 0.037713, acc.: 98.44%] [G loss: 5.062389]\n",
            "1575 [D loss: 0.070589, acc.: 100.00%] [G loss: 5.010173]\n",
            "1576 [D loss: 0.297641, acc.: 84.38%] [G loss: 9.433564]\n",
            "1577 [D loss: 0.401152, acc.: 81.25%] [G loss: 3.819647]\n",
            "1578 [D loss: 0.360738, acc.: 81.25%] [G loss: 6.260924]\n",
            "1579 [D loss: 0.021378, acc.: 100.00%] [G loss: 6.564743]\n",
            "1580 [D loss: 0.086467, acc.: 95.31%] [G loss: 5.035063]\n",
            "1581 [D loss: 0.022576, acc.: 100.00%] [G loss: 4.095663]\n",
            "1582 [D loss: 0.183974, acc.: 92.19%] [G loss: 6.288433]\n",
            "1583 [D loss: 0.068692, acc.: 98.44%] [G loss: 7.241933]\n",
            "1584 [D loss: 0.346527, acc.: 81.25%] [G loss: 3.463947]\n",
            "1585 [D loss: 0.333760, acc.: 81.25%] [G loss: 7.976963]\n",
            "1586 [D loss: 0.040915, acc.: 100.00%] [G loss: 9.865496]\n",
            "1587 [D loss: 0.259354, acc.: 89.06%] [G loss: 7.076436]\n",
            "1588 [D loss: 0.094193, acc.: 96.88%] [G loss: 4.864906]\n",
            "1589 [D loss: 0.028003, acc.: 100.00%] [G loss: 3.635763]\n",
            "1590 [D loss: 0.078493, acc.: 96.88%] [G loss: 6.103487]\n",
            "1591 [D loss: 0.048455, acc.: 98.44%] [G loss: 5.648113]\n",
            "1592 [D loss: 0.186953, acc.: 92.19%] [G loss: 3.555727]\n",
            "1593 [D loss: 0.035564, acc.: 100.00%] [G loss: 6.297489]\n",
            "1594 [D loss: 0.135804, acc.: 96.88%] [G loss: 6.663354]\n",
            "1595 [D loss: 0.038505, acc.: 100.00%] [G loss: 5.651914]\n",
            "1596 [D loss: 0.038008, acc.: 98.44%] [G loss: 5.725659]\n",
            "1597 [D loss: 0.075342, acc.: 100.00%] [G loss: 4.914426]\n",
            "1598 [D loss: 0.038306, acc.: 100.00%] [G loss: 5.590184]\n",
            "1599 [D loss: 0.030042, acc.: 100.00%] [G loss: 3.636434]\n",
            "1600 [D loss: 0.086701, acc.: 100.00%] [G loss: 5.066259]\n",
            "1601 [D loss: 0.171184, acc.: 92.19%] [G loss: 6.328489]\n",
            "1602 [D loss: 0.055894, acc.: 100.00%] [G loss: 6.688213]\n",
            "1603 [D loss: 0.100205, acc.: 95.31%] [G loss: 4.938737]\n",
            "1604 [D loss: 0.074332, acc.: 98.44%] [G loss: 6.127665]\n",
            "1605 [D loss: 0.082450, acc.: 96.88%] [G loss: 6.569270]\n",
            "1606 [D loss: 0.086778, acc.: 100.00%] [G loss: 4.923066]\n",
            "1607 [D loss: 0.102105, acc.: 96.88%] [G loss: 5.510036]\n",
            "1608 [D loss: 0.057423, acc.: 100.00%] [G loss: 3.459083]\n",
            "1609 [D loss: 0.015475, acc.: 100.00%] [G loss: 5.991645]\n",
            "1610 [D loss: 0.196010, acc.: 93.75%] [G loss: 5.421172]\n",
            "1611 [D loss: 0.057548, acc.: 98.44%] [G loss: 5.742724]\n",
            "1612 [D loss: 0.038566, acc.: 98.44%] [G loss: 5.792772]\n",
            "1613 [D loss: 0.409303, acc.: 78.12%] [G loss: 9.550511]\n",
            "1614 [D loss: 0.170518, acc.: 95.31%] [G loss: 6.977757]\n",
            "1615 [D loss: 0.134945, acc.: 95.31%] [G loss: 4.786566]\n",
            "1616 [D loss: 0.089110, acc.: 95.31%] [G loss: 7.903934]\n",
            "1617 [D loss: 0.022290, acc.: 100.00%] [G loss: 7.741889]\n",
            "1618 [D loss: 0.032252, acc.: 100.00%] [G loss: 5.981514]\n",
            "1619 [D loss: 0.015990, acc.: 100.00%] [G loss: 4.565862]\n",
            "1620 [D loss: 0.236916, acc.: 89.06%] [G loss: 5.891800]\n",
            "1621 [D loss: 0.046835, acc.: 100.00%] [G loss: 6.237163]\n",
            "1622 [D loss: 0.051728, acc.: 100.00%] [G loss: 6.779785]\n",
            "1623 [D loss: 0.281737, acc.: 90.62%] [G loss: 4.464690]\n",
            "1624 [D loss: 0.021050, acc.: 100.00%] [G loss: 5.711401]\n",
            "1625 [D loss: 0.054719, acc.: 100.00%] [G loss: 4.649146]\n",
            "1626 [D loss: 0.075901, acc.: 98.44%] [G loss: 4.911321]\n",
            "1627 [D loss: 0.127787, acc.: 96.88%] [G loss: 4.382227]\n",
            "1628 [D loss: 0.106919, acc.: 96.88%] [G loss: 7.170840]\n",
            "1629 [D loss: 0.063706, acc.: 100.00%] [G loss: 7.543085]\n",
            "1630 [D loss: 0.075500, acc.: 98.44%] [G loss: 4.338559]\n",
            "1631 [D loss: 0.050012, acc.: 100.00%] [G loss: 4.753104]\n",
            "1632 [D loss: 0.035938, acc.: 100.00%] [G loss: 4.994816]\n",
            "1633 [D loss: 0.029053, acc.: 100.00%] [G loss: 5.349545]\n",
            "1634 [D loss: 0.095923, acc.: 98.44%] [G loss: 6.183185]\n",
            "1635 [D loss: 0.021982, acc.: 100.00%] [G loss: 5.483496]\n",
            "1636 [D loss: 0.119109, acc.: 98.44%] [G loss: 6.875824]\n",
            "1637 [D loss: 0.318880, acc.: 92.19%] [G loss: 5.361131]\n",
            "1638 [D loss: 0.095941, acc.: 96.88%] [G loss: 5.975143]\n",
            "1639 [D loss: 0.012926, acc.: 100.00%] [G loss: 8.686018]\n",
            "1640 [D loss: 0.023011, acc.: 100.00%] [G loss: 7.995190]\n",
            "1641 [D loss: 0.037125, acc.: 100.00%] [G loss: 5.075998]\n",
            "1642 [D loss: 0.154218, acc.: 95.31%] [G loss: 7.072692]\n",
            "1643 [D loss: 0.296727, acc.: 85.94%] [G loss: 4.400066]\n",
            "1644 [D loss: 0.165440, acc.: 92.19%] [G loss: 6.354296]\n",
            "1645 [D loss: 0.023773, acc.: 100.00%] [G loss: 8.525531]\n",
            "1646 [D loss: 0.113621, acc.: 95.31%] [G loss: 3.737113]\n",
            "1647 [D loss: 0.060183, acc.: 100.00%] [G loss: 5.017463]\n",
            "1648 [D loss: 0.041983, acc.: 100.00%] [G loss: 5.228665]\n",
            "1649 [D loss: 0.058992, acc.: 98.44%] [G loss: 5.457267]\n",
            "1650 [D loss: 0.015556, acc.: 100.00%] [G loss: 6.671855]\n",
            "1651 [D loss: 0.215527, acc.: 92.19%] [G loss: 5.910432]\n",
            "1652 [D loss: 0.014002, acc.: 100.00%] [G loss: 5.577249]\n",
            "1653 [D loss: 0.041152, acc.: 100.00%] [G loss: 4.816895]\n",
            "1654 [D loss: 0.018981, acc.: 100.00%] [G loss: 3.496132]\n",
            "1655 [D loss: 0.173841, acc.: 95.31%] [G loss: 8.020475]\n",
            "1656 [D loss: 0.190779, acc.: 92.19%] [G loss: 7.133287]\n",
            "1657 [D loss: 0.031796, acc.: 100.00%] [G loss: 4.894452]\n",
            "1658 [D loss: 0.062504, acc.: 98.44%] [G loss: 6.158034]\n",
            "1659 [D loss: 0.058647, acc.: 98.44%] [G loss: 6.024005]\n",
            "1660 [D loss: 0.108681, acc.: 96.88%] [G loss: 5.327425]\n",
            "1661 [D loss: 0.046251, acc.: 100.00%] [G loss: 8.082388]\n",
            "1662 [D loss: 0.030963, acc.: 100.00%] [G loss: 6.339377]\n",
            "1663 [D loss: 0.093201, acc.: 96.88%] [G loss: 5.325073]\n",
            "1664 [D loss: 0.076598, acc.: 98.44%] [G loss: 7.227588]\n",
            "1665 [D loss: 0.091048, acc.: 96.88%] [G loss: 5.903065]\n",
            "1666 [D loss: 0.019898, acc.: 100.00%] [G loss: 4.636444]\n",
            "1667 [D loss: 0.029122, acc.: 100.00%] [G loss: 4.251277]\n",
            "1668 [D loss: 0.011761, acc.: 100.00%] [G loss: 4.800111]\n",
            "1669 [D loss: 0.148381, acc.: 93.75%] [G loss: 6.947075]\n",
            "1670 [D loss: 0.134084, acc.: 95.31%] [G loss: 7.037243]\n",
            "1671 [D loss: 0.091625, acc.: 95.31%] [G loss: 4.993817]\n",
            "1672 [D loss: 0.078226, acc.: 98.44%] [G loss: 4.546268]\n",
            "1673 [D loss: 0.028164, acc.: 100.00%] [G loss: 2.407551]\n",
            "1674 [D loss: 0.059932, acc.: 98.44%] [G loss: 6.546815]\n",
            "1675 [D loss: 0.084140, acc.: 98.44%] [G loss: 4.585545]\n",
            "1676 [D loss: 0.187060, acc.: 93.75%] [G loss: 6.985126]\n",
            "1677 [D loss: 0.026194, acc.: 100.00%] [G loss: 8.117329]\n",
            "1678 [D loss: 1.066839, acc.: 42.19%] [G loss: 9.270534]\n",
            "1679 [D loss: 0.222679, acc.: 87.50%] [G loss: 6.545462]\n",
            "1680 [D loss: 0.131119, acc.: 98.44%] [G loss: 4.377944]\n",
            "1681 [D loss: 0.020442, acc.: 100.00%] [G loss: 5.730804]\n",
            "1682 [D loss: 0.115397, acc.: 96.88%] [G loss: 5.858672]\n",
            "1683 [D loss: 0.021392, acc.: 100.00%] [G loss: 6.824149]\n",
            "1684 [D loss: 0.060918, acc.: 98.44%] [G loss: 6.093694]\n",
            "1685 [D loss: 0.076301, acc.: 96.88%] [G loss: 5.358946]\n",
            "1686 [D loss: 0.125797, acc.: 96.88%] [G loss: 6.445695]\n",
            "1687 [D loss: 0.040232, acc.: 100.00%] [G loss: 7.634644]\n",
            "1688 [D loss: 0.321595, acc.: 87.50%] [G loss: 4.302876]\n",
            "1689 [D loss: 0.027647, acc.: 98.44%] [G loss: 4.793957]\n",
            "1690 [D loss: 0.028092, acc.: 100.00%] [G loss: 4.961989]\n",
            "1691 [D loss: 0.064501, acc.: 98.44%] [G loss: 5.614103]\n",
            "1692 [D loss: 0.019341, acc.: 100.00%] [G loss: 5.785399]\n",
            "1693 [D loss: 0.053869, acc.: 100.00%] [G loss: 5.181183]\n",
            "1694 [D loss: 0.185163, acc.: 93.75%] [G loss: 9.821606]\n",
            "1695 [D loss: 0.141730, acc.: 95.31%] [G loss: 8.373243]\n",
            "1696 [D loss: 0.037553, acc.: 98.44%] [G loss: 6.708071]\n",
            "1697 [D loss: 0.014018, acc.: 100.00%] [G loss: 4.568600]\n",
            "1698 [D loss: 0.022279, acc.: 100.00%] [G loss: 3.970148]\n",
            "1699 [D loss: 0.073524, acc.: 100.00%] [G loss: 5.413033]\n",
            "1700 [D loss: 0.008056, acc.: 100.00%] [G loss: 5.700736]\n",
            "1701 [D loss: 0.027949, acc.: 100.00%] [G loss: 5.031142]\n",
            "1702 [D loss: 0.051270, acc.: 100.00%] [G loss: 3.700730]\n",
            "1703 [D loss: 0.082584, acc.: 96.88%] [G loss: 5.116545]\n",
            "1704 [D loss: 0.015942, acc.: 100.00%] [G loss: 4.525772]\n",
            "1705 [D loss: 0.050841, acc.: 100.00%] [G loss: 5.897510]\n",
            "1706 [D loss: 0.052210, acc.: 100.00%] [G loss: 6.040432]\n",
            "1707 [D loss: 0.043076, acc.: 100.00%] [G loss: 5.511453]\n",
            "1708 [D loss: 0.604364, acc.: 68.75%] [G loss: 7.674393]\n",
            "1709 [D loss: 0.022567, acc.: 100.00%] [G loss: 9.763796]\n",
            "1710 [D loss: 0.055649, acc.: 96.88%] [G loss: 8.379541]\n",
            "1711 [D loss: 0.100973, acc.: 98.44%] [G loss: 6.555387]\n",
            "1712 [D loss: 0.007432, acc.: 100.00%] [G loss: 5.453217]\n",
            "1713 [D loss: 0.094260, acc.: 100.00%] [G loss: 5.415951]\n",
            "1714 [D loss: 0.004289, acc.: 100.00%] [G loss: 6.796618]\n",
            "1715 [D loss: 0.024239, acc.: 100.00%] [G loss: 5.519942]\n",
            "1716 [D loss: 0.081093, acc.: 96.88%] [G loss: 4.811940]\n",
            "1717 [D loss: 0.431456, acc.: 73.44%] [G loss: 4.669456]\n",
            "1718 [D loss: 0.010557, acc.: 100.00%] [G loss: 4.724855]\n",
            "1719 [D loss: 0.197571, acc.: 92.19%] [G loss: 7.486446]\n",
            "1720 [D loss: 0.037897, acc.: 100.00%] [G loss: 7.268525]\n",
            "1721 [D loss: 0.107517, acc.: 95.31%] [G loss: 4.242507]\n",
            "1722 [D loss: 0.039606, acc.: 100.00%] [G loss: 4.627195]\n",
            "1723 [D loss: 0.018331, acc.: 100.00%] [G loss: 6.269598]\n",
            "1724 [D loss: 0.022714, acc.: 100.00%] [G loss: 5.015313]\n",
            "1725 [D loss: 0.059899, acc.: 98.44%] [G loss: 4.677717]\n",
            "1726 [D loss: 0.381661, acc.: 85.94%] [G loss: 10.888620]\n",
            "1727 [D loss: 0.534303, acc.: 73.44%] [G loss: 4.498477]\n",
            "1728 [D loss: 0.127985, acc.: 96.88%] [G loss: 6.366731]\n",
            "1729 [D loss: 0.009560, acc.: 100.00%] [G loss: 6.440053]\n",
            "1730 [D loss: 0.071843, acc.: 98.44%] [G loss: 6.265577]\n",
            "1731 [D loss: 0.027639, acc.: 100.00%] [G loss: 6.121540]\n",
            "1732 [D loss: 0.122215, acc.: 98.44%] [G loss: 6.145969]\n",
            "1733 [D loss: 0.055830, acc.: 98.44%] [G loss: 5.155342]\n",
            "1734 [D loss: 0.013152, acc.: 100.00%] [G loss: 6.094523]\n",
            "1735 [D loss: 0.028780, acc.: 98.44%] [G loss: 4.325100]\n",
            "1736 [D loss: 0.070808, acc.: 98.44%] [G loss: 5.907395]\n",
            "1737 [D loss: 0.119170, acc.: 96.88%] [G loss: 2.985929]\n",
            "1738 [D loss: 0.022161, acc.: 100.00%] [G loss: 5.077888]\n",
            "1739 [D loss: 0.048642, acc.: 100.00%] [G loss: 5.796916]\n",
            "1740 [D loss: 0.244008, acc.: 90.62%] [G loss: 5.564408]\n",
            "1741 [D loss: 0.034647, acc.: 98.44%] [G loss: 6.576035]\n",
            "1742 [D loss: 0.055526, acc.: 100.00%] [G loss: 3.735405]\n",
            "1743 [D loss: 0.052300, acc.: 100.00%] [G loss: 5.778269]\n",
            "1744 [D loss: 0.049619, acc.: 98.44%] [G loss: 5.305030]\n",
            "1745 [D loss: 0.049788, acc.: 98.44%] [G loss: 6.088699]\n",
            "1746 [D loss: 0.056463, acc.: 98.44%] [G loss: 6.606667]\n",
            "1747 [D loss: 0.026097, acc.: 100.00%] [G loss: 5.451212]\n",
            "1748 [D loss: 0.351212, acc.: 87.50%] [G loss: 9.065489]\n",
            "1749 [D loss: 0.127778, acc.: 93.75%] [G loss: 8.118166]\n",
            "1750 [D loss: 0.071523, acc.: 98.44%] [G loss: 5.124010]\n",
            "1751 [D loss: 0.033953, acc.: 100.00%] [G loss: 6.863751]\n",
            "1752 [D loss: 0.244440, acc.: 89.06%] [G loss: 10.738863]\n",
            "1753 [D loss: 0.063688, acc.: 98.44%] [G loss: 13.006063]\n",
            "1754 [D loss: 0.285627, acc.: 87.50%] [G loss: 5.628666]\n",
            "1755 [D loss: 0.593277, acc.: 75.00%] [G loss: 12.163832]\n",
            "1756 [D loss: 0.369946, acc.: 85.94%] [G loss: 10.114522]\n",
            "1757 [D loss: 0.042953, acc.: 100.00%] [G loss: 7.381860]\n",
            "1758 [D loss: 0.063220, acc.: 98.44%] [G loss: 5.749111]\n",
            "1759 [D loss: 0.009753, acc.: 100.00%] [G loss: 7.341855]\n",
            "1760 [D loss: 0.079254, acc.: 98.44%] [G loss: 5.709520]\n",
            "1761 [D loss: 0.034291, acc.: 100.00%] [G loss: 5.552124]\n",
            "1762 [D loss: 0.418479, acc.: 75.00%] [G loss: 7.353618]\n",
            "1763 [D loss: 0.032182, acc.: 100.00%] [G loss: 6.948734]\n",
            "1764 [D loss: 0.017762, acc.: 100.00%] [G loss: 9.830056]\n",
            "1765 [D loss: 0.091891, acc.: 95.31%] [G loss: 4.293907]\n",
            "1766 [D loss: 0.038029, acc.: 100.00%] [G loss: 5.290833]\n",
            "1767 [D loss: 0.023310, acc.: 100.00%] [G loss: 5.329343]\n",
            "1768 [D loss: 0.064109, acc.: 100.00%] [G loss: 3.405349]\n",
            "1769 [D loss: 0.224155, acc.: 89.06%] [G loss: 7.306564]\n",
            "1770 [D loss: 0.102712, acc.: 98.44%] [G loss: 7.735755]\n",
            "1771 [D loss: 0.167889, acc.: 95.31%] [G loss: 4.158274]\n",
            "1772 [D loss: 0.028088, acc.: 100.00%] [G loss: 3.789369]\n",
            "1773 [D loss: 0.073419, acc.: 98.44%] [G loss: 4.931875]\n",
            "1774 [D loss: 0.029066, acc.: 100.00%] [G loss: 6.741553]\n",
            "1775 [D loss: 0.930313, acc.: 53.12%] [G loss: 12.599552]\n",
            "1776 [D loss: 0.138563, acc.: 92.19%] [G loss: 9.494791]\n",
            "1777 [D loss: 0.094834, acc.: 96.88%] [G loss: 7.801432]\n",
            "1778 [D loss: 0.014371, acc.: 100.00%] [G loss: 5.358581]\n",
            "1779 [D loss: 0.041293, acc.: 98.44%] [G loss: 4.536111]\n",
            "1780 [D loss: 0.036171, acc.: 100.00%] [G loss: 5.860172]\n",
            "1781 [D loss: 0.039809, acc.: 100.00%] [G loss: 5.697358]\n",
            "1782 [D loss: 0.028732, acc.: 100.00%] [G loss: 5.770280]\n",
            "1783 [D loss: 0.054965, acc.: 98.44%] [G loss: 4.710932]\n",
            "1784 [D loss: 0.151994, acc.: 93.75%] [G loss: 6.147599]\n",
            "1785 [D loss: 0.295789, acc.: 87.50%] [G loss: 4.310373]\n",
            "1786 [D loss: 0.025293, acc.: 100.00%] [G loss: 4.499279]\n",
            "1787 [D loss: 0.073643, acc.: 98.44%] [G loss: 5.290565]\n",
            "1788 [D loss: 0.017875, acc.: 100.00%] [G loss: 5.708072]\n",
            "1789 [D loss: 0.056842, acc.: 100.00%] [G loss: 4.948724]\n",
            "1790 [D loss: 0.294794, acc.: 90.62%] [G loss: 6.070025]\n",
            "1791 [D loss: 0.052406, acc.: 100.00%] [G loss: 6.588161]\n",
            "1792 [D loss: 0.035336, acc.: 100.00%] [G loss: 6.646234]\n",
            "1793 [D loss: 0.055359, acc.: 100.00%] [G loss: 5.426030]\n",
            "1794 [D loss: 0.046153, acc.: 98.44%] [G loss: 6.707389]\n",
            "1795 [D loss: 0.165048, acc.: 95.31%] [G loss: 7.093350]\n",
            "1796 [D loss: 0.066781, acc.: 100.00%] [G loss: 6.225366]\n",
            "1797 [D loss: 0.019280, acc.: 100.00%] [G loss: 8.363599]\n",
            "1798 [D loss: 0.012468, acc.: 100.00%] [G loss: 4.605163]\n",
            "1799 [D loss: 0.214681, acc.: 90.62%] [G loss: 8.359770]\n",
            "1800 [D loss: 0.041827, acc.: 100.00%] [G loss: 8.186157]\n",
            "1801 [D loss: 0.095461, acc.: 95.31%] [G loss: 6.104065]\n",
            "1802 [D loss: 0.086045, acc.: 98.44%] [G loss: 5.942760]\n",
            "1803 [D loss: 0.009996, acc.: 100.00%] [G loss: 6.769999]\n",
            "1804 [D loss: 0.057584, acc.: 100.00%] [G loss: 5.374812]\n",
            "1805 [D loss: 0.195327, acc.: 93.75%] [G loss: 4.101226]\n",
            "1806 [D loss: 0.061855, acc.: 98.44%] [G loss: 6.772579]\n",
            "1807 [D loss: 0.032057, acc.: 100.00%] [G loss: 5.141613]\n",
            "1808 [D loss: 0.076683, acc.: 100.00%] [G loss: 5.141797]\n",
            "1809 [D loss: 0.227115, acc.: 90.62%] [G loss: 7.689653]\n",
            "1810 [D loss: 0.126186, acc.: 93.75%] [G loss: 6.192278]\n",
            "1811 [D loss: 0.020297, acc.: 100.00%] [G loss: 4.170281]\n",
            "1812 [D loss: 0.055905, acc.: 98.44%] [G loss: 5.160845]\n",
            "1813 [D loss: 0.037164, acc.: 100.00%] [G loss: 5.616518]\n",
            "1814 [D loss: 0.018663, acc.: 100.00%] [G loss: 5.410463]\n",
            "1815 [D loss: 0.201402, acc.: 90.62%] [G loss: 6.541973]\n",
            "1816 [D loss: 0.026785, acc.: 100.00%] [G loss: 6.486412]\n",
            "1817 [D loss: 0.372315, acc.: 85.94%] [G loss: 6.889267]\n",
            "1818 [D loss: 0.018024, acc.: 100.00%] [G loss: 7.542787]\n",
            "1819 [D loss: 0.044481, acc.: 100.00%] [G loss: 7.021911]\n",
            "1820 [D loss: 0.015711, acc.: 100.00%] [G loss: 5.464413]\n",
            "1821 [D loss: 0.072257, acc.: 100.00%] [G loss: 5.075902]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2487950a6a54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_epoch_no\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-ae7f0a4953ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, epochs, batch_size, save_interval, start_epoch_no)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m# Sample noise and generate a batch of new images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# Train the discriminator (real classified as ones and generated as zeros)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "de5a99a2-1d0f-4acb-b9f7-1032173175be",
        "id": "zAa0ZCGtwNj7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dcgan = DCGAN()\n",
        "dcgan.load_model()\n",
        "\n",
        "dcgan.train(X_train, epochs=4000, batch_size=32, save_interval=50,start_epoch_no=1821)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_23 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 8193      \n",
            "=================================================================\n",
            "Total params: 2,171,713\n",
            "Trainable params: 2,169,281\n",
            "Non-trainable params: 2,432\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 32768)             3309568   \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 8, 8, 256)         1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2 (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_8 (UpSampling2 (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 32, 32, 128)       295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_9 (UpSampling2 (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 64, 64, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 64, 64, 3)         1731      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 64, 64, 3)         0         \n",
            "=================================================================\n",
            "Total params: 5,452,931\n",
            "Trainable params: 5,451,523\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1821 [D loss: 0.116415, acc.: 98.44%] [G loss: 4.777758]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1822 [D loss: 0.016272, acc.: 100.00%] [G loss: 6.340481]\n",
            "1823 [D loss: 0.032756, acc.: 100.00%] [G loss: 8.268696]\n",
            "1824 [D loss: 0.099414, acc.: 98.44%] [G loss: 7.938467]\n",
            "1825 [D loss: 0.006904, acc.: 100.00%] [G loss: 9.570461]\n",
            "1826 [D loss: 0.019819, acc.: 100.00%] [G loss: 5.924885]\n",
            "1827 [D loss: 0.030115, acc.: 100.00%] [G loss: 6.697103]\n",
            "1828 [D loss: 0.025521, acc.: 100.00%] [G loss: 5.422039]\n",
            "1829 [D loss: 0.046987, acc.: 100.00%] [G loss: 6.880138]\n",
            "1830 [D loss: 0.008784, acc.: 100.00%] [G loss: 6.579162]\n",
            "1831 [D loss: 0.045588, acc.: 100.00%] [G loss: 6.929455]\n",
            "1832 [D loss: 0.164393, acc.: 96.88%] [G loss: 8.725731]\n",
            "1833 [D loss: 0.047802, acc.: 98.44%] [G loss: 5.370621]\n",
            "1834 [D loss: 0.020450, acc.: 100.00%] [G loss: 6.649355]\n",
            "1835 [D loss: 0.016555, acc.: 100.00%] [G loss: 7.024532]\n",
            "1836 [D loss: 0.032759, acc.: 98.44%] [G loss: 6.623753]\n",
            "1837 [D loss: 0.033202, acc.: 100.00%] [G loss: 7.434024]\n",
            "1838 [D loss: 0.010775, acc.: 100.00%] [G loss: 6.296946]\n",
            "1839 [D loss: 0.198521, acc.: 96.88%] [G loss: 12.419113]\n",
            "1840 [D loss: 0.844076, acc.: 62.50%] [G loss: 10.184294]\n",
            "1841 [D loss: 0.001127, acc.: 100.00%] [G loss: 10.619766]\n",
            "1842 [D loss: 0.108444, acc.: 95.31%] [G loss: 5.300692]\n",
            "1843 [D loss: 0.098769, acc.: 93.75%] [G loss: 9.074663]\n",
            "1844 [D loss: 0.013484, acc.: 100.00%] [G loss: 6.564000]\n",
            "1845 [D loss: 0.224610, acc.: 93.75%] [G loss: 7.909081]\n",
            "1846 [D loss: 0.105459, acc.: 100.00%] [G loss: 9.457775]\n",
            "1847 [D loss: 0.092442, acc.: 98.44%] [G loss: 7.189140]\n",
            "1848 [D loss: 0.051715, acc.: 100.00%] [G loss: 6.445784]\n",
            "1849 [D loss: 0.079064, acc.: 98.44%] [G loss: 4.857483]\n",
            "1850 [D loss: 0.127039, acc.: 95.31%] [G loss: 9.474999]\n",
            "1851 [D loss: 0.754135, acc.: 57.81%] [G loss: 11.996286]\n",
            "1852 [D loss: 0.028345, acc.: 100.00%] [G loss: 10.273262]\n",
            "1853 [D loss: 0.430725, acc.: 85.94%] [G loss: 5.544186]\n",
            "1854 [D loss: 0.285553, acc.: 90.62%] [G loss: 11.203991]\n",
            "1855 [D loss: 0.003153, acc.: 100.00%] [G loss: 12.428400]\n",
            "1856 [D loss: 0.398016, acc.: 85.94%] [G loss: 5.006439]\n",
            "1857 [D loss: 0.132849, acc.: 95.31%] [G loss: 6.903038]\n",
            "1858 [D loss: 0.008363, acc.: 100.00%] [G loss: 6.593822]\n",
            "1859 [D loss: 0.034269, acc.: 100.00%] [G loss: 4.843882]\n",
            "1860 [D loss: 0.045152, acc.: 98.44%] [G loss: 5.387450]\n",
            "1861 [D loss: 0.117737, acc.: 95.31%] [G loss: 5.549633]\n",
            "1862 [D loss: 0.856571, acc.: 64.06%] [G loss: 13.363341]\n",
            "1863 [D loss: 0.137799, acc.: 92.19%] [G loss: 13.837103]\n",
            "1864 [D loss: 0.384937, acc.: 79.69%] [G loss: 3.447218]\n",
            "1865 [D loss: 0.199932, acc.: 92.19%] [G loss: 6.243739]\n",
            "1866 [D loss: 0.047986, acc.: 98.44%] [G loss: 7.509251]\n",
            "1867 [D loss: 0.023501, acc.: 100.00%] [G loss: 7.329150]\n",
            "1868 [D loss: 0.132355, acc.: 93.75%] [G loss: 3.763023]\n",
            "1869 [D loss: 0.043737, acc.: 96.88%] [G loss: 2.719902]\n",
            "1870 [D loss: 0.166925, acc.: 90.62%] [G loss: 8.043194]\n",
            "1871 [D loss: 0.020538, acc.: 100.00%] [G loss: 9.046352]\n",
            "1872 [D loss: 0.101723, acc.: 96.88%] [G loss: 5.217845]\n",
            "1873 [D loss: 0.028362, acc.: 100.00%] [G loss: 3.366111]\n",
            "1874 [D loss: 0.378392, acc.: 81.25%] [G loss: 8.481332]\n",
            "1875 [D loss: 0.149005, acc.: 93.75%] [G loss: 4.536851]\n",
            "1876 [D loss: 0.059657, acc.: 98.44%] [G loss: 4.659201]\n",
            "1877 [D loss: 0.016669, acc.: 100.00%] [G loss: 5.818472]\n",
            "1878 [D loss: 0.069989, acc.: 100.00%] [G loss: 5.196990]\n",
            "1879 [D loss: 0.230558, acc.: 93.75%] [G loss: 7.419432]\n",
            "1880 [D loss: 0.034656, acc.: 100.00%] [G loss: 6.816350]\n",
            "1881 [D loss: 0.266059, acc.: 89.06%] [G loss: 4.747083]\n",
            "1882 [D loss: 0.110013, acc.: 96.88%] [G loss: 8.017471]\n",
            "1883 [D loss: 0.070868, acc.: 96.88%] [G loss: 5.896574]\n",
            "1884 [D loss: 0.017212, acc.: 100.00%] [G loss: 5.962634]\n",
            "1885 [D loss: 0.058896, acc.: 98.44%] [G loss: 6.167663]\n",
            "1886 [D loss: 0.071323, acc.: 98.44%] [G loss: 5.662725]\n",
            "1887 [D loss: 0.033047, acc.: 100.00%] [G loss: 7.294528]\n",
            "1888 [D loss: 0.079999, acc.: 100.00%] [G loss: 8.041666]\n",
            "1889 [D loss: 0.029761, acc.: 100.00%] [G loss: 7.706447]\n",
            "1890 [D loss: 0.256502, acc.: 82.81%] [G loss: 8.129314]\n",
            "1891 [D loss: 0.021950, acc.: 100.00%] [G loss: 6.525980]\n",
            "1892 [D loss: 0.086101, acc.: 98.44%] [G loss: 5.201941]\n",
            "1893 [D loss: 0.026345, acc.: 100.00%] [G loss: 4.699739]\n",
            "1894 [D loss: 0.168888, acc.: 92.19%] [G loss: 9.760162]\n",
            "1895 [D loss: 0.074447, acc.: 96.88%] [G loss: 8.690969]\n",
            "1896 [D loss: 0.077460, acc.: 96.88%] [G loss: 3.605771]\n",
            "1897 [D loss: 0.026474, acc.: 100.00%] [G loss: 4.134809]\n",
            "1898 [D loss: 0.176074, acc.: 93.75%] [G loss: 9.251290]\n",
            "1899 [D loss: 0.131804, acc.: 96.88%] [G loss: 5.214798]\n",
            "1900 [D loss: 0.049053, acc.: 100.00%] [G loss: 3.817331]\n",
            "1901 [D loss: 0.009445, acc.: 100.00%] [G loss: 4.974946]\n",
            "1902 [D loss: 0.011087, acc.: 100.00%] [G loss: 5.278209]\n",
            "1903 [D loss: 0.045017, acc.: 100.00%] [G loss: 4.126412]\n",
            "1904 [D loss: 0.112444, acc.: 96.88%] [G loss: 9.128632]\n",
            "1905 [D loss: 0.122308, acc.: 96.88%] [G loss: 7.428720]\n",
            "1906 [D loss: 0.048988, acc.: 98.44%] [G loss: 4.685270]\n",
            "1907 [D loss: 0.138021, acc.: 96.88%] [G loss: 5.767259]\n",
            "1908 [D loss: 0.013248, acc.: 100.00%] [G loss: 6.876602]\n",
            "1909 [D loss: 0.005852, acc.: 100.00%] [G loss: 5.560654]\n",
            "1910 [D loss: 0.035367, acc.: 100.00%] [G loss: 5.290391]\n",
            "1911 [D loss: 0.171416, acc.: 93.75%] [G loss: 10.685489]\n",
            "1912 [D loss: 0.405758, acc.: 76.56%] [G loss: 5.037123]\n",
            "1913 [D loss: 0.002983, acc.: 100.00%] [G loss: 5.283127]\n",
            "1914 [D loss: 0.007466, acc.: 100.00%] [G loss: 5.345467]\n",
            "1915 [D loss: 0.114520, acc.: 96.88%] [G loss: 6.389320]\n",
            "1916 [D loss: 0.078767, acc.: 98.44%] [G loss: 4.978913]\n",
            "1917 [D loss: 0.037530, acc.: 100.00%] [G loss: 6.030830]\n",
            "1918 [D loss: 0.072090, acc.: 100.00%] [G loss: 4.902215]\n",
            "1919 [D loss: 0.007060, acc.: 100.00%] [G loss: 5.263775]\n",
            "1920 [D loss: 0.034672, acc.: 100.00%] [G loss: 5.891751]\n",
            "1921 [D loss: 0.044629, acc.: 100.00%] [G loss: 5.857875]\n",
            "1922 [D loss: 0.037164, acc.: 98.44%] [G loss: 5.473819]\n",
            "1923 [D loss: 0.049908, acc.: 100.00%] [G loss: 4.809732]\n",
            "1924 [D loss: 0.106108, acc.: 95.31%] [G loss: 9.012600]\n",
            "1925 [D loss: 1.090631, acc.: 37.50%] [G loss: 13.890379]\n",
            "1926 [D loss: 0.006609, acc.: 100.00%] [G loss: 14.106393]\n",
            "1927 [D loss: 0.155992, acc.: 93.75%] [G loss: 11.600539]\n",
            "1928 [D loss: 0.141902, acc.: 89.06%] [G loss: 7.493175]\n",
            "1929 [D loss: 0.024894, acc.: 100.00%] [G loss: 6.422794]\n",
            "1930 [D loss: 0.012398, acc.: 100.00%] [G loss: 5.590018]\n",
            "1931 [D loss: 0.012635, acc.: 100.00%] [G loss: 4.922898]\n",
            "1932 [D loss: 0.091735, acc.: 96.88%] [G loss: 6.994978]\n",
            "1933 [D loss: 0.016029, acc.: 98.44%] [G loss: 7.186908]\n",
            "1934 [D loss: 0.052696, acc.: 98.44%] [G loss: 3.637046]\n",
            "1935 [D loss: 0.161365, acc.: 96.88%] [G loss: 6.726604]\n",
            "1936 [D loss: 0.205593, acc.: 89.06%] [G loss: 4.184342]\n",
            "1937 [D loss: 0.014035, acc.: 100.00%] [G loss: 3.603261]\n",
            "1938 [D loss: 0.083892, acc.: 98.44%] [G loss: 7.347882]\n",
            "1939 [D loss: 0.004238, acc.: 100.00%] [G loss: 8.573081]\n",
            "1940 [D loss: 0.057259, acc.: 98.44%] [G loss: 4.342944]\n",
            "1941 [D loss: 0.059460, acc.: 96.88%] [G loss: 5.419539]\n",
            "1942 [D loss: 0.008248, acc.: 100.00%] [G loss: 7.416510]\n",
            "1943 [D loss: 0.027111, acc.: 98.44%] [G loss: 7.326242]\n",
            "1944 [D loss: 0.151408, acc.: 96.88%] [G loss: 7.525823]\n",
            "1945 [D loss: 0.010393, acc.: 100.00%] [G loss: 7.803119]\n",
            "1946 [D loss: 0.099170, acc.: 96.88%] [G loss: 5.334326]\n",
            "1947 [D loss: 0.046436, acc.: 100.00%] [G loss: 5.476378]\n",
            "1948 [D loss: 0.025496, acc.: 100.00%] [G loss: 4.620687]\n",
            "1949 [D loss: 0.076286, acc.: 100.00%] [G loss: 4.458983]\n",
            "1950 [D loss: 0.011721, acc.: 100.00%] [G loss: 3.203340]\n",
            "1951 [D loss: 0.013829, acc.: 100.00%] [G loss: 6.434307]\n",
            "1952 [D loss: 0.031629, acc.: 98.44%] [G loss: 5.222029]\n",
            "1953 [D loss: 0.033564, acc.: 100.00%] [G loss: 4.434643]\n",
            "1954 [D loss: 0.078969, acc.: 100.00%] [G loss: 7.639506]\n",
            "1955 [D loss: 0.060777, acc.: 100.00%] [G loss: 6.640492]\n",
            "1956 [D loss: 0.105533, acc.: 96.88%] [G loss: 4.422469]\n",
            "1957 [D loss: 0.080314, acc.: 98.44%] [G loss: 9.256248]\n",
            "1958 [D loss: 0.023321, acc.: 100.00%] [G loss: 10.235134]\n",
            "1959 [D loss: 0.014665, acc.: 100.00%] [G loss: 8.971249]\n",
            "1960 [D loss: 0.010150, acc.: 100.00%] [G loss: 7.295469]\n",
            "1961 [D loss: 0.014233, acc.: 100.00%] [G loss: 6.356909]\n",
            "1962 [D loss: 0.032567, acc.: 100.00%] [G loss: 6.736279]\n",
            "1963 [D loss: 0.077670, acc.: 100.00%] [G loss: 6.733602]\n",
            "1964 [D loss: 0.015513, acc.: 100.00%] [G loss: 7.300680]\n",
            "1965 [D loss: 0.008455, acc.: 100.00%] [G loss: 6.451373]\n",
            "1966 [D loss: 0.011427, acc.: 100.00%] [G loss: 5.774909]\n",
            "1967 [D loss: 0.040287, acc.: 100.00%] [G loss: 6.096821]\n",
            "1968 [D loss: 0.037003, acc.: 98.44%] [G loss: 5.382401]\n",
            "1969 [D loss: 0.044489, acc.: 100.00%] [G loss: 5.597763]\n",
            "1970 [D loss: 0.088136, acc.: 100.00%] [G loss: 7.362394]\n",
            "1971 [D loss: 0.010808, acc.: 100.00%] [G loss: 7.441708]\n",
            "1972 [D loss: 0.067871, acc.: 98.44%] [G loss: 6.153610]\n",
            "1973 [D loss: 0.031265, acc.: 100.00%] [G loss: 5.343533]\n",
            "1974 [D loss: 0.021277, acc.: 100.00%] [G loss: 7.974302]\n",
            "1975 [D loss: 0.007331, acc.: 100.00%] [G loss: 7.684836]\n",
            "1976 [D loss: 0.006430, acc.: 100.00%] [G loss: 8.189423]\n",
            "1977 [D loss: 0.034697, acc.: 100.00%] [G loss: 6.347131]\n",
            "1978 [D loss: 0.020124, acc.: 100.00%] [G loss: 6.955122]\n",
            "1979 [D loss: 0.411094, acc.: 84.38%] [G loss: 11.327489]\n",
            "1980 [D loss: 0.035986, acc.: 98.44%] [G loss: 12.376263]\n",
            "1981 [D loss: 0.240488, acc.: 89.06%] [G loss: 2.869719]\n",
            "1982 [D loss: 0.265861, acc.: 85.94%] [G loss: 14.106688]\n",
            "1983 [D loss: 0.003807, acc.: 100.00%] [G loss: 15.312194]\n",
            "1984 [D loss: 0.110179, acc.: 93.75%] [G loss: 12.197069]\n",
            "1985 [D loss: 0.057811, acc.: 98.44%] [G loss: 9.543717]\n",
            "1986 [D loss: 0.015032, acc.: 100.00%] [G loss: 5.253731]\n",
            "1987 [D loss: 0.117376, acc.: 98.44%] [G loss: 8.685905]\n",
            "1988 [D loss: 0.090783, acc.: 96.88%] [G loss: 7.394153]\n",
            "1989 [D loss: 0.034140, acc.: 98.44%] [G loss: 5.719907]\n",
            "1990 [D loss: 0.047533, acc.: 100.00%] [G loss: 8.562904]\n",
            "1991 [D loss: 0.029430, acc.: 98.44%] [G loss: 5.949447]\n",
            "1992 [D loss: 0.051903, acc.: 100.00%] [G loss: 5.509816]\n",
            "1993 [D loss: 0.023089, acc.: 100.00%] [G loss: 4.042671]\n",
            "1994 [D loss: 0.114857, acc.: 96.88%] [G loss: 9.145999]\n",
            "1995 [D loss: 0.032659, acc.: 100.00%] [G loss: 7.383736]\n",
            "1996 [D loss: 0.033597, acc.: 100.00%] [G loss: 5.108009]\n",
            "1997 [D loss: 0.421919, acc.: 81.25%] [G loss: 15.292749]\n",
            "1998 [D loss: 0.898235, acc.: 60.94%] [G loss: 3.048954]\n",
            "1999 [D loss: 0.457193, acc.: 81.25%] [G loss: 11.641971]\n",
            "2000 [D loss: 0.010238, acc.: 100.00%] [G loss: 13.130467]\n",
            "2001 [D loss: 0.777163, acc.: 70.31%] [G loss: 2.973226]\n",
            "2002 [D loss: 1.027776, acc.: 64.06%] [G loss: 15.322943]\n",
            "2003 [D loss: 0.777588, acc.: 65.62%] [G loss: 9.399345]\n",
            "2004 [D loss: 0.123927, acc.: 93.75%] [G loss: 8.051769]\n",
            "2005 [D loss: 0.002500, acc.: 100.00%] [G loss: 6.689183]\n",
            "2006 [D loss: 0.032452, acc.: 98.44%] [G loss: 5.786084]\n",
            "2007 [D loss: 0.089205, acc.: 96.88%] [G loss: 4.977132]\n",
            "2008 [D loss: 0.418735, acc.: 82.81%] [G loss: 7.844862]\n",
            "2009 [D loss: 0.309296, acc.: 82.81%] [G loss: 6.632964]\n",
            "2010 [D loss: 0.087304, acc.: 98.44%] [G loss: 3.944133]\n",
            "2011 [D loss: 0.054422, acc.: 98.44%] [G loss: 5.350593]\n",
            "2012 [D loss: 0.188202, acc.: 95.31%] [G loss: 6.556644]\n",
            "2013 [D loss: 0.052406, acc.: 100.00%] [G loss: 6.351234]\n",
            "2014 [D loss: 0.202342, acc.: 93.75%] [G loss: 7.400540]\n",
            "2015 [D loss: 0.024741, acc.: 100.00%] [G loss: 7.626667]\n",
            "2016 [D loss: 0.025937, acc.: 100.00%] [G loss: 6.229317]\n",
            "2017 [D loss: 0.157662, acc.: 92.19%] [G loss: 10.509941]\n",
            "2018 [D loss: 0.166265, acc.: 92.19%] [G loss: 8.381775]\n",
            "2019 [D loss: 0.058734, acc.: 98.44%] [G loss: 4.587513]\n",
            "2020 [D loss: 0.011371, acc.: 100.00%] [G loss: 5.610653]\n",
            "2021 [D loss: 0.135261, acc.: 90.62%] [G loss: 8.136810]\n",
            "2022 [D loss: 0.147851, acc.: 93.75%] [G loss: 6.310338]\n",
            "2023 [D loss: 0.152222, acc.: 96.88%] [G loss: 6.749004]\n",
            "2024 [D loss: 0.020884, acc.: 100.00%] [G loss: 7.531667]\n",
            "2025 [D loss: 0.020005, acc.: 100.00%] [G loss: 5.266644]\n",
            "2026 [D loss: 0.079390, acc.: 96.88%] [G loss: 7.913039]\n",
            "2027 [D loss: 0.067458, acc.: 100.00%] [G loss: 5.915107]\n",
            "2028 [D loss: 0.060218, acc.: 98.44%] [G loss: 6.409406]\n",
            "2029 [D loss: 0.199455, acc.: 93.75%] [G loss: 9.149689]\n",
            "2030 [D loss: 0.124874, acc.: 93.75%] [G loss: 7.716067]\n",
            "2031 [D loss: 0.116493, acc.: 96.88%] [G loss: 7.040446]\n",
            "2032 [D loss: 0.027828, acc.: 98.44%] [G loss: 5.159348]\n",
            "2033 [D loss: 0.012775, acc.: 100.00%] [G loss: 4.739517]\n",
            "2034 [D loss: 0.076983, acc.: 96.88%] [G loss: 7.540168]\n",
            "2035 [D loss: 0.015543, acc.: 100.00%] [G loss: 6.763828]\n",
            "2036 [D loss: 0.135665, acc.: 95.31%] [G loss: 4.608085]\n",
            "2037 [D loss: 0.106938, acc.: 96.88%] [G loss: 8.044024]\n",
            "2038 [D loss: 0.106526, acc.: 98.44%] [G loss: 8.183632]\n",
            "2039 [D loss: 0.021772, acc.: 100.00%] [G loss: 4.639024]\n",
            "2040 [D loss: 0.075641, acc.: 98.44%] [G loss: 8.647296]\n",
            "2041 [D loss: 0.045216, acc.: 98.44%] [G loss: 7.574156]\n",
            "2042 [D loss: 0.007728, acc.: 100.00%] [G loss: 5.212600]\n",
            "2043 [D loss: 0.012590, acc.: 100.00%] [G loss: 4.813801]\n",
            "2044 [D loss: 0.105501, acc.: 96.88%] [G loss: 5.939792]\n",
            "2045 [D loss: 0.057013, acc.: 98.44%] [G loss: 6.279443]\n",
            "2046 [D loss: 0.062197, acc.: 98.44%] [G loss: 6.996937]\n",
            "2047 [D loss: 0.104602, acc.: 96.88%] [G loss: 6.717586]\n",
            "2048 [D loss: 0.102110, acc.: 98.44%] [G loss: 5.058208]\n",
            "2049 [D loss: 0.060495, acc.: 100.00%] [G loss: 5.667802]\n",
            "2050 [D loss: 0.013827, acc.: 100.00%] [G loss: 6.384365]\n",
            "2051 [D loss: 0.020416, acc.: 100.00%] [G loss: 5.760043]\n",
            "2052 [D loss: 0.069230, acc.: 98.44%] [G loss: 7.241034]\n",
            "2053 [D loss: 0.108710, acc.: 96.88%] [G loss: 6.319898]\n",
            "2054 [D loss: 0.035321, acc.: 100.00%] [G loss: 3.701978]\n",
            "2055 [D loss: 0.039880, acc.: 98.44%] [G loss: 4.768944]\n",
            "2056 [D loss: 0.047548, acc.: 100.00%] [G loss: 6.993498]\n",
            "2057 [D loss: 0.075380, acc.: 98.44%] [G loss: 5.869533]\n",
            "2058 [D loss: 0.059191, acc.: 98.44%] [G loss: 7.460413]\n",
            "2059 [D loss: 0.039862, acc.: 100.00%] [G loss: 6.502038]\n",
            "2060 [D loss: 0.005333, acc.: 100.00%] [G loss: 6.319400]\n",
            "2061 [D loss: 0.014585, acc.: 100.00%] [G loss: 5.556361]\n",
            "2062 [D loss: 0.015483, acc.: 100.00%] [G loss: 5.496561]\n",
            "2063 [D loss: 0.202270, acc.: 92.19%] [G loss: 10.919242]\n",
            "2064 [D loss: 0.058623, acc.: 98.44%] [G loss: 10.282341]\n",
            "2065 [D loss: 0.025756, acc.: 98.44%] [G loss: 7.569448]\n",
            "2066 [D loss: 0.042783, acc.: 100.00%] [G loss: 5.301389]\n",
            "2067 [D loss: 0.011502, acc.: 100.00%] [G loss: 5.967909]\n",
            "2068 [D loss: 0.019124, acc.: 100.00%] [G loss: 6.314373]\n",
            "2069 [D loss: 0.011134, acc.: 100.00%] [G loss: 4.656693]\n",
            "2070 [D loss: 0.155704, acc.: 96.88%] [G loss: 5.622701]\n",
            "2071 [D loss: 0.020679, acc.: 100.00%] [G loss: 5.385674]\n",
            "2072 [D loss: 0.003944, acc.: 100.00%] [G loss: 7.927182]\n",
            "2073 [D loss: 0.028505, acc.: 100.00%] [G loss: 7.787849]\n",
            "2074 [D loss: 0.032786, acc.: 100.00%] [G loss: 5.380937]\n",
            "2075 [D loss: 0.041148, acc.: 100.00%] [G loss: 7.706379]\n",
            "2076 [D loss: 0.039116, acc.: 100.00%] [G loss: 5.510423]\n",
            "2077 [D loss: 0.028202, acc.: 100.00%] [G loss: 6.229628]\n",
            "2078 [D loss: 0.056107, acc.: 98.44%] [G loss: 5.960191]\n",
            "2079 [D loss: 0.055251, acc.: 100.00%] [G loss: 5.412144]\n",
            "2080 [D loss: 0.018265, acc.: 100.00%] [G loss: 5.345709]\n",
            "2081 [D loss: 0.025950, acc.: 100.00%] [G loss: 5.903101]\n",
            "2082 [D loss: 0.021763, acc.: 100.00%] [G loss: 5.701287]\n",
            "2083 [D loss: 0.324073, acc.: 85.94%] [G loss: 5.088959]\n",
            "2084 [D loss: 0.004757, acc.: 100.00%] [G loss: 7.286785]\n",
            "2085 [D loss: 0.026648, acc.: 100.00%] [G loss: 7.082116]\n",
            "2086 [D loss: 0.036609, acc.: 98.44%] [G loss: 6.254607]\n",
            "2087 [D loss: 0.002555, acc.: 100.00%] [G loss: 7.783201]\n",
            "2088 [D loss: 0.029660, acc.: 100.00%] [G loss: 4.932030]\n",
            "2089 [D loss: 0.071287, acc.: 100.00%] [G loss: 7.570827]\n",
            "2090 [D loss: 0.008557, acc.: 100.00%] [G loss: 9.562583]\n",
            "2091 [D loss: 0.029478, acc.: 100.00%] [G loss: 6.049443]\n",
            "2092 [D loss: 0.033382, acc.: 100.00%] [G loss: 3.612418]\n",
            "2093 [D loss: 0.074786, acc.: 100.00%] [G loss: 7.902404]\n",
            "2094 [D loss: 0.047575, acc.: 100.00%] [G loss: 7.057082]\n",
            "2095 [D loss: 0.035545, acc.: 100.00%] [G loss: 5.468476]\n",
            "2096 [D loss: 0.034056, acc.: 100.00%] [G loss: 6.691600]\n",
            "2097 [D loss: 0.046762, acc.: 98.44%] [G loss: 6.654207]\n",
            "2098 [D loss: 0.013545, acc.: 100.00%] [G loss: 6.158608]\n",
            "2099 [D loss: 0.012867, acc.: 100.00%] [G loss: 5.942022]\n",
            "2100 [D loss: 0.011199, acc.: 100.00%] [G loss: 7.728973]\n",
            "2101 [D loss: 0.053534, acc.: 100.00%] [G loss: 6.839555]\n",
            "2102 [D loss: 0.084422, acc.: 100.00%] [G loss: 7.697507]\n",
            "2103 [D loss: 0.024724, acc.: 100.00%] [G loss: 7.588106]\n",
            "2104 [D loss: 0.038142, acc.: 100.00%] [G loss: 6.826746]\n",
            "2105 [D loss: 0.025770, acc.: 100.00%] [G loss: 6.298808]\n",
            "2106 [D loss: 0.305648, acc.: 84.38%] [G loss: 8.522787]\n",
            "2107 [D loss: 0.006909, acc.: 100.00%] [G loss: 7.614699]\n",
            "2108 [D loss: 0.001468, acc.: 100.00%] [G loss: 6.532846]\n",
            "2109 [D loss: 0.007533, acc.: 100.00%] [G loss: 8.266628]\n",
            "2110 [D loss: 0.011947, acc.: 100.00%] [G loss: 5.832501]\n",
            "2111 [D loss: 0.195263, acc.: 92.19%] [G loss: 12.767376]\n",
            "2112 [D loss: 0.631711, acc.: 71.88%] [G loss: 5.036227]\n",
            "2113 [D loss: 0.760648, acc.: 67.19%] [G loss: 16.043036]\n",
            "2114 [D loss: 4.892334, acc.: 50.00%] [G loss: 10.214518]\n",
            "2115 [D loss: 0.002096, acc.: 100.00%] [G loss: 6.723439]\n",
            "2116 [D loss: 0.118638, acc.: 96.88%] [G loss: 5.521059]\n",
            "2117 [D loss: 0.024218, acc.: 100.00%] [G loss: 5.500543]\n",
            "2118 [D loss: 0.024388, acc.: 100.00%] [G loss: 5.270058]\n",
            "2119 [D loss: 0.550707, acc.: 71.88%] [G loss: 11.644710]\n",
            "2120 [D loss: 0.051696, acc.: 96.88%] [G loss: 13.122322]\n",
            "2121 [D loss: 1.131140, acc.: 60.94%] [G loss: 2.548810]\n",
            "2122 [D loss: 1.216999, acc.: 57.81%] [G loss: 13.878167]\n",
            "2123 [D loss: 0.414716, acc.: 85.94%] [G loss: 12.762082]\n",
            "2124 [D loss: 0.002538, acc.: 100.00%] [G loss: 11.737961]\n",
            "2125 [D loss: 0.001912, acc.: 100.00%] [G loss: 8.830837]\n",
            "2126 [D loss: 0.019448, acc.: 100.00%] [G loss: 6.486366]\n",
            "2127 [D loss: 0.017917, acc.: 100.00%] [G loss: 3.651875]\n",
            "2128 [D loss: 0.221708, acc.: 89.06%] [G loss: 7.048340]\n",
            "2129 [D loss: 0.013334, acc.: 100.00%] [G loss: 6.695474]\n",
            "2130 [D loss: 0.183541, acc.: 98.44%] [G loss: 5.545358]\n",
            "2131 [D loss: 0.096144, acc.: 96.88%] [G loss: 4.541730]\n",
            "2132 [D loss: 0.029363, acc.: 100.00%] [G loss: 5.966333]\n",
            "2133 [D loss: 0.161395, acc.: 96.88%] [G loss: 3.749106]\n",
            "2134 [D loss: 0.193249, acc.: 89.06%] [G loss: 8.511726]\n",
            "2135 [D loss: 0.629262, acc.: 67.19%] [G loss: 7.363387]\n",
            "2136 [D loss: 0.012069, acc.: 100.00%] [G loss: 5.780462]\n",
            "2137 [D loss: 0.070241, acc.: 98.44%] [G loss: 6.336572]\n",
            "2138 [D loss: 0.011722, acc.: 100.00%] [G loss: 4.657622]\n",
            "2139 [D loss: 0.143741, acc.: 95.31%] [G loss: 8.101670]\n",
            "2140 [D loss: 0.264206, acc.: 85.94%] [G loss: 4.832113]\n",
            "2141 [D loss: 0.135479, acc.: 98.44%] [G loss: 7.545485]\n",
            "2142 [D loss: 0.082390, acc.: 98.44%] [G loss: 6.308444]\n",
            "2143 [D loss: 0.147681, acc.: 96.88%] [G loss: 4.313240]\n",
            "2144 [D loss: 0.148392, acc.: 93.75%] [G loss: 5.451002]\n",
            "2145 [D loss: 0.044549, acc.: 98.44%] [G loss: 7.232593]\n",
            "2146 [D loss: 0.146547, acc.: 95.31%] [G loss: 3.817665]\n",
            "2147 [D loss: 0.129948, acc.: 96.88%] [G loss: 5.941471]\n",
            "2148 [D loss: 0.167643, acc.: 96.88%] [G loss: 5.846490]\n",
            "2149 [D loss: 0.034497, acc.: 100.00%] [G loss: 5.735631]\n",
            "2150 [D loss: 0.017538, acc.: 100.00%] [G loss: 4.247743]\n",
            "2151 [D loss: 0.143176, acc.: 95.31%] [G loss: 5.056259]\n",
            "2152 [D loss: 0.039852, acc.: 100.00%] [G loss: 6.579537]\n",
            "2153 [D loss: 0.153327, acc.: 95.31%] [G loss: 3.820652]\n",
            "2154 [D loss: 0.330111, acc.: 82.81%] [G loss: 10.901035]\n",
            "2155 [D loss: 0.468440, acc.: 78.12%] [G loss: 7.319274]\n",
            "2156 [D loss: 0.058174, acc.: 98.44%] [G loss: 6.765219]\n",
            "2157 [D loss: 0.215172, acc.: 89.06%] [G loss: 10.204642]\n",
            "2158 [D loss: 0.006138, acc.: 100.00%] [G loss: 11.452737]\n",
            "2159 [D loss: 0.151079, acc.: 92.19%] [G loss: 8.756502]\n",
            "2160 [D loss: 0.006929, acc.: 100.00%] [G loss: 7.028025]\n",
            "2161 [D loss: 0.006278, acc.: 100.00%] [G loss: 5.724743]\n",
            "2162 [D loss: 0.090764, acc.: 96.88%] [G loss: 6.156359]\n",
            "2163 [D loss: 0.019219, acc.: 100.00%] [G loss: 4.682787]\n",
            "2164 [D loss: 0.025054, acc.: 100.00%] [G loss: 5.260704]\n",
            "2165 [D loss: 0.051035, acc.: 100.00%] [G loss: 5.702492]\n",
            "2166 [D loss: 0.162784, acc.: 93.75%] [G loss: 7.284294]\n",
            "2167 [D loss: 0.105936, acc.: 95.31%] [G loss: 5.394543]\n",
            "2168 [D loss: 0.360081, acc.: 90.62%] [G loss: 7.120864]\n",
            "2169 [D loss: 0.061894, acc.: 98.44%] [G loss: 6.128219]\n",
            "2170 [D loss: 0.042221, acc.: 98.44%] [G loss: 6.505004]\n",
            "2171 [D loss: 0.014664, acc.: 100.00%] [G loss: 5.702744]\n",
            "2172 [D loss: 0.009906, acc.: 100.00%] [G loss: 4.822740]\n",
            "2173 [D loss: 0.071404, acc.: 98.44%] [G loss: 5.777783]\n",
            "2174 [D loss: 0.109154, acc.: 96.88%] [G loss: 3.829967]\n",
            "2175 [D loss: 0.025716, acc.: 100.00%] [G loss: 4.496096]\n",
            "2176 [D loss: 0.093587, acc.: 96.88%] [G loss: 7.269559]\n",
            "2177 [D loss: 0.096085, acc.: 95.31%] [G loss: 5.512538]\n",
            "2178 [D loss: 0.052187, acc.: 100.00%] [G loss: 5.764827]\n",
            "2179 [D loss: 0.042946, acc.: 100.00%] [G loss: 4.569401]\n",
            "2180 [D loss: 0.023154, acc.: 100.00%] [G loss: 4.042433]\n",
            "2181 [D loss: 0.018312, acc.: 100.00%] [G loss: 4.862869]\n",
            "2182 [D loss: 0.034643, acc.: 100.00%] [G loss: 5.989116]\n",
            "2183 [D loss: 0.380978, acc.: 82.81%] [G loss: 7.327278]\n",
            "2184 [D loss: 0.001234, acc.: 100.00%] [G loss: 8.201483]\n",
            "2185 [D loss: 0.256140, acc.: 89.06%] [G loss: 2.026569]\n",
            "2186 [D loss: 0.863301, acc.: 62.50%] [G loss: 15.747440]\n",
            "2187 [D loss: 0.195975, acc.: 92.19%] [G loss: 15.948359]\n",
            "2188 [D loss: 0.284483, acc.: 87.50%] [G loss: 13.510018]\n",
            "2189 [D loss: 0.048444, acc.: 100.00%] [G loss: 6.689512]\n",
            "2190 [D loss: 0.007687, acc.: 100.00%] [G loss: 7.343639]\n",
            "2191 [D loss: 0.124033, acc.: 93.75%] [G loss: 5.286818]\n",
            "2192 [D loss: 0.003278, acc.: 100.00%] [G loss: 5.681434]\n",
            "2193 [D loss: 0.003160, acc.: 100.00%] [G loss: 7.768323]\n",
            "2194 [D loss: 0.003871, acc.: 100.00%] [G loss: 5.205770]\n",
            "2195 [D loss: 0.111958, acc.: 96.88%] [G loss: 5.860090]\n",
            "2196 [D loss: 0.153574, acc.: 95.31%] [G loss: 7.557620]\n",
            "2197 [D loss: 0.074128, acc.: 98.44%] [G loss: 8.016957]\n",
            "2198 [D loss: 0.038486, acc.: 100.00%] [G loss: 4.607392]\n",
            "2199 [D loss: 0.031356, acc.: 100.00%] [G loss: 5.213174]\n",
            "2200 [D loss: 0.095617, acc.: 96.88%] [G loss: 4.837712]\n",
            "2201 [D loss: 0.131731, acc.: 98.44%] [G loss: 7.146042]\n",
            "2202 [D loss: 0.028490, acc.: 100.00%] [G loss: 8.168434]\n",
            "2203 [D loss: 0.123140, acc.: 98.44%] [G loss: 4.532682]\n",
            "2204 [D loss: 0.027795, acc.: 100.00%] [G loss: 4.488488]\n",
            "2205 [D loss: 0.028590, acc.: 98.44%] [G loss: 5.670350]\n",
            "2206 [D loss: 0.097755, acc.: 98.44%] [G loss: 6.747114]\n",
            "2207 [D loss: 0.039462, acc.: 100.00%] [G loss: 6.299992]\n",
            "2208 [D loss: 0.303947, acc.: 92.19%] [G loss: 9.236814]\n",
            "2209 [D loss: 0.030210, acc.: 100.00%] [G loss: 12.076159]\n",
            "2210 [D loss: 0.055348, acc.: 98.44%] [G loss: 10.426914]\n",
            "2211 [D loss: 0.078948, acc.: 96.88%] [G loss: 5.863930]\n",
            "2212 [D loss: 0.017446, acc.: 100.00%] [G loss: 4.042045]\n",
            "2213 [D loss: 0.091903, acc.: 96.88%] [G loss: 7.730301]\n",
            "2214 [D loss: 0.019656, acc.: 100.00%] [G loss: 8.694901]\n",
            "2215 [D loss: 0.007233, acc.: 100.00%] [G loss: 7.784627]\n",
            "2216 [D loss: 0.013482, acc.: 100.00%] [G loss: 6.368402]\n",
            "2217 [D loss: 0.014511, acc.: 100.00%] [G loss: 4.449978]\n",
            "2218 [D loss: 0.645348, acc.: 67.19%] [G loss: 13.612645]\n",
            "2219 [D loss: 0.104851, acc.: 95.31%] [G loss: 15.015203]\n",
            "2220 [D loss: 0.159752, acc.: 95.31%] [G loss: 10.930199]\n",
            "2221 [D loss: 0.004588, acc.: 100.00%] [G loss: 8.502049]\n",
            "2222 [D loss: 0.009828, acc.: 100.00%] [G loss: 5.236152]\n",
            "2223 [D loss: 0.010105, acc.: 100.00%] [G loss: 6.303495]\n",
            "2224 [D loss: 0.016069, acc.: 100.00%] [G loss: 6.435477]\n",
            "2225 [D loss: 0.146827, acc.: 95.31%] [G loss: 5.218410]\n",
            "2226 [D loss: 0.004048, acc.: 100.00%] [G loss: 7.849629]\n",
            "2227 [D loss: 0.033780, acc.: 100.00%] [G loss: 5.445305]\n",
            "2228 [D loss: 0.016316, acc.: 100.00%] [G loss: 5.291930]\n",
            "2229 [D loss: 0.024965, acc.: 100.00%] [G loss: 7.141388]\n",
            "2230 [D loss: 0.019208, acc.: 100.00%] [G loss: 5.680549]\n",
            "2231 [D loss: 0.093223, acc.: 95.31%] [G loss: 8.941618]\n",
            "2232 [D loss: 0.037204, acc.: 100.00%] [G loss: 7.815792]\n",
            "2233 [D loss: 0.098824, acc.: 98.44%] [G loss: 3.634037]\n",
            "2234 [D loss: 0.223450, acc.: 93.75%] [G loss: 13.573133]\n",
            "2235 [D loss: 0.296094, acc.: 79.69%] [G loss: 6.485411]\n",
            "2236 [D loss: 0.009934, acc.: 100.00%] [G loss: 6.583622]\n",
            "2237 [D loss: 0.167340, acc.: 90.62%] [G loss: 9.821602]\n",
            "2238 [D loss: 0.001826, acc.: 100.00%] [G loss: 11.899104]\n",
            "2239 [D loss: 0.035610, acc.: 98.44%] [G loss: 8.450019]\n",
            "2240 [D loss: 0.010732, acc.: 100.00%] [G loss: 6.768312]\n",
            "2241 [D loss: 0.004679, acc.: 100.00%] [G loss: 6.516908]\n",
            "2242 [D loss: 0.196189, acc.: 92.19%] [G loss: 8.572644]\n",
            "2243 [D loss: 0.037103, acc.: 100.00%] [G loss: 10.026909]\n",
            "2244 [D loss: 1.796407, acc.: 20.31%] [G loss: 12.531570]\n",
            "2245 [D loss: 0.023717, acc.: 100.00%] [G loss: 15.777798]\n",
            "2246 [D loss: 0.697316, acc.: 71.88%] [G loss: 8.824079]\n",
            "2247 [D loss: 0.030983, acc.: 98.44%] [G loss: 5.406313]\n",
            "2248 [D loss: 0.085865, acc.: 96.88%] [G loss: 5.635436]\n",
            "2249 [D loss: 0.040854, acc.: 98.44%] [G loss: 6.441307]\n",
            "2250 [D loss: 0.033291, acc.: 100.00%] [G loss: 5.323518]\n",
            "2251 [D loss: 0.474997, acc.: 76.56%] [G loss: 11.575376]\n",
            "2252 [D loss: 1.472463, acc.: 53.12%] [G loss: 1.756858]\n",
            "2253 [D loss: 1.368950, acc.: 59.38%] [G loss: 14.666567]\n",
            "2254 [D loss: 0.009576, acc.: 100.00%] [G loss: 16.112604]\n",
            "2255 [D loss: 2.237976, acc.: 51.56%] [G loss: 12.703877]\n",
            "2256 [D loss: 0.004218, acc.: 100.00%] [G loss: 7.848518]\n",
            "2257 [D loss: 0.037058, acc.: 98.44%] [G loss: 5.919509]\n",
            "2258 [D loss: 0.149680, acc.: 92.19%] [G loss: 6.487775]\n",
            "2259 [D loss: 0.045547, acc.: 98.44%] [G loss: 7.423051]\n",
            "2260 [D loss: 0.039189, acc.: 100.00%] [G loss: 6.532169]\n",
            "2261 [D loss: 0.017140, acc.: 100.00%] [G loss: 5.857310]\n",
            "2262 [D loss: 0.133778, acc.: 95.31%] [G loss: 7.288275]\n",
            "2263 [D loss: 0.368656, acc.: 82.81%] [G loss: 6.904105]\n",
            "2264 [D loss: 0.019451, acc.: 100.00%] [G loss: 5.533212]\n",
            "2265 [D loss: 0.114039, acc.: 93.75%] [G loss: 7.433437]\n",
            "2266 [D loss: 0.033945, acc.: 100.00%] [G loss: 7.002888]\n",
            "2267 [D loss: 0.054240, acc.: 100.00%] [G loss: 5.423711]\n",
            "2268 [D loss: 0.228861, acc.: 92.19%] [G loss: 5.589703]\n",
            "2269 [D loss: 0.139056, acc.: 96.88%] [G loss: 5.610034]\n",
            "2270 [D loss: 0.132840, acc.: 96.88%] [G loss: 4.619274]\n",
            "2271 [D loss: 0.074022, acc.: 96.88%] [G loss: 6.509459]\n",
            "2272 [D loss: 0.084068, acc.: 98.44%] [G loss: 5.523770]\n",
            "2273 [D loss: 0.138251, acc.: 98.44%] [G loss: 3.408109]\n",
            "2274 [D loss: 0.051810, acc.: 98.44%] [G loss: 6.225392]\n",
            "2275 [D loss: 0.086211, acc.: 96.88%] [G loss: 4.185050]\n",
            "2276 [D loss: 0.014086, acc.: 100.00%] [G loss: 6.361568]\n",
            "2277 [D loss: 0.035283, acc.: 100.00%] [G loss: 5.898814]\n",
            "2278 [D loss: 0.042716, acc.: 98.44%] [G loss: 7.280152]\n",
            "2279 [D loss: 0.070937, acc.: 100.00%] [G loss: 8.369379]\n",
            "2280 [D loss: 0.054020, acc.: 100.00%] [G loss: 7.890303]\n",
            "2281 [D loss: 0.160748, acc.: 93.75%] [G loss: 7.118113]\n",
            "2282 [D loss: 0.034287, acc.: 98.44%] [G loss: 7.838681]\n",
            "2283 [D loss: 0.264310, acc.: 87.50%] [G loss: 4.778613]\n",
            "2284 [D loss: 0.016928, acc.: 100.00%] [G loss: 4.348986]\n",
            "2285 [D loss: 0.069385, acc.: 96.88%] [G loss: 6.194942]\n",
            "2286 [D loss: 0.007366, acc.: 100.00%] [G loss: 5.308983]\n",
            "2287 [D loss: 0.062522, acc.: 98.44%] [G loss: 6.046453]\n",
            "2288 [D loss: 0.017180, acc.: 100.00%] [G loss: 5.012965]\n",
            "2289 [D loss: 0.032326, acc.: 100.00%] [G loss: 3.822520]\n",
            "2290 [D loss: 0.134383, acc.: 95.31%] [G loss: 5.473510]\n",
            "2291 [D loss: 0.021474, acc.: 100.00%] [G loss: 5.964071]\n",
            "2292 [D loss: 0.022587, acc.: 100.00%] [G loss: 5.162919]\n",
            "2293 [D loss: 0.119536, acc.: 93.75%] [G loss: 8.155095]\n",
            "2294 [D loss: 0.123261, acc.: 98.44%] [G loss: 5.812786]\n",
            "2295 [D loss: 0.016083, acc.: 100.00%] [G loss: 6.454543]\n",
            "2296 [D loss: 0.118848, acc.: 96.88%] [G loss: 5.175700]\n",
            "2297 [D loss: 0.015188, acc.: 100.00%] [G loss: 6.346920]\n",
            "2298 [D loss: 0.014731, acc.: 100.00%] [G loss: 6.751152]\n",
            "2299 [D loss: 0.018431, acc.: 100.00%] [G loss: 4.754648]\n",
            "2300 [D loss: 0.066068, acc.: 98.44%] [G loss: 5.133358]\n",
            "2301 [D loss: 0.101912, acc.: 95.31%] [G loss: 8.168779]\n",
            "2302 [D loss: 0.078213, acc.: 98.44%] [G loss: 7.382151]\n",
            "2303 [D loss: 0.038655, acc.: 100.00%] [G loss: 5.467299]\n",
            "2304 [D loss: 0.050212, acc.: 100.00%] [G loss: 6.139745]\n",
            "2305 [D loss: 0.019296, acc.: 100.00%] [G loss: 6.300771]\n",
            "2306 [D loss: 0.051773, acc.: 100.00%] [G loss: 5.786746]\n",
            "2307 [D loss: 0.087308, acc.: 96.88%] [G loss: 4.390896]\n",
            "2308 [D loss: 0.024239, acc.: 100.00%] [G loss: 4.480694]\n",
            "2309 [D loss: 0.113618, acc.: 98.44%] [G loss: 6.981570]\n",
            "2310 [D loss: 0.021734, acc.: 100.00%] [G loss: 5.746556]\n",
            "2311 [D loss: 0.009324, acc.: 100.00%] [G loss: 4.438416]\n",
            "2312 [D loss: 0.024246, acc.: 100.00%] [G loss: 6.072078]\n",
            "2313 [D loss: 0.046176, acc.: 100.00%] [G loss: 6.411070]\n",
            "2314 [D loss: 0.045527, acc.: 100.00%] [G loss: 4.926689]\n",
            "2315 [D loss: 0.020166, acc.: 100.00%] [G loss: 3.730525]\n",
            "2316 [D loss: 0.032304, acc.: 100.00%] [G loss: 6.056239]\n",
            "2317 [D loss: 0.139182, acc.: 95.31%] [G loss: 7.505771]\n",
            "2318 [D loss: 0.212465, acc.: 89.06%] [G loss: 6.717035]\n",
            "2319 [D loss: 0.067497, acc.: 100.00%] [G loss: 6.975967]\n",
            "2320 [D loss: 0.006298, acc.: 100.00%] [G loss: 8.455955]\n",
            "2321 [D loss: 0.007051, acc.: 100.00%] [G loss: 6.878430]\n",
            "2322 [D loss: 0.020313, acc.: 100.00%] [G loss: 7.566175]\n",
            "2323 [D loss: 0.045268, acc.: 100.00%] [G loss: 6.006700]\n",
            "2324 [D loss: 0.008037, acc.: 100.00%] [G loss: 5.086436]\n",
            "2325 [D loss: 0.010998, acc.: 100.00%] [G loss: 5.284475]\n",
            "2326 [D loss: 0.013438, acc.: 100.00%] [G loss: 6.042214]\n",
            "2327 [D loss: 0.029099, acc.: 100.00%] [G loss: 6.588292]\n",
            "2328 [D loss: 0.132702, acc.: 96.88%] [G loss: 7.112136]\n",
            "2329 [D loss: 0.013312, acc.: 100.00%] [G loss: 7.560275]\n",
            "2330 [D loss: 0.045939, acc.: 100.00%] [G loss: 6.021523]\n",
            "2331 [D loss: 0.040556, acc.: 100.00%] [G loss: 6.442292]\n",
            "2332 [D loss: 0.025353, acc.: 100.00%] [G loss: 5.384115]\n",
            "2333 [D loss: 0.029177, acc.: 100.00%] [G loss: 7.128645]\n",
            "2334 [D loss: 0.045670, acc.: 100.00%] [G loss: 5.158716]\n",
            "2335 [D loss: 0.143089, acc.: 95.31%] [G loss: 10.469681]\n",
            "2336 [D loss: 0.133147, acc.: 95.31%] [G loss: 7.192096]\n",
            "2337 [D loss: 0.020828, acc.: 100.00%] [G loss: 6.421038]\n",
            "2338 [D loss: 0.008948, acc.: 100.00%] [G loss: 4.519955]\n",
            "2339 [D loss: 0.020092, acc.: 100.00%] [G loss: 4.951005]\n",
            "2340 [D loss: 0.053474, acc.: 98.44%] [G loss: 7.527706]\n",
            "2341 [D loss: 0.004160, acc.: 100.00%] [G loss: 7.041343]\n",
            "2342 [D loss: 0.095495, acc.: 96.88%] [G loss: 4.150137]\n",
            "2343 [D loss: 0.015187, acc.: 100.00%] [G loss: 4.148008]\n",
            "2344 [D loss: 0.012024, acc.: 100.00%] [G loss: 4.290057]\n",
            "2345 [D loss: 0.108361, acc.: 96.88%] [G loss: 8.093311]\n",
            "2346 [D loss: 0.021072, acc.: 100.00%] [G loss: 9.066887]\n",
            "2347 [D loss: 0.012376, acc.: 100.00%] [G loss: 6.095819]\n",
            "2348 [D loss: 0.047314, acc.: 100.00%] [G loss: 3.270355]\n",
            "2349 [D loss: 0.177485, acc.: 93.75%] [G loss: 8.482189]\n",
            "2350 [D loss: 0.035922, acc.: 100.00%] [G loss: 9.451567]\n",
            "2351 [D loss: 0.045888, acc.: 98.44%] [G loss: 7.673089]\n",
            "2352 [D loss: 0.011256, acc.: 100.00%] [G loss: 6.544366]\n",
            "2353 [D loss: 0.051246, acc.: 98.44%] [G loss: 6.529531]\n",
            "2354 [D loss: 0.013412, acc.: 100.00%] [G loss: 6.467081]\n",
            "2355 [D loss: 0.026766, acc.: 100.00%] [G loss: 6.508840]\n",
            "2356 [D loss: 0.027264, acc.: 100.00%] [G loss: 5.267701]\n",
            "2357 [D loss: 0.004758, acc.: 100.00%] [G loss: 5.277059]\n",
            "2358 [D loss: 0.014963, acc.: 100.00%] [G loss: 5.442458]\n",
            "2359 [D loss: 0.163136, acc.: 95.31%] [G loss: 8.854450]\n",
            "2360 [D loss: 0.031097, acc.: 100.00%] [G loss: 7.472116]\n",
            "2361 [D loss: 0.027611, acc.: 100.00%] [G loss: 7.593324]\n",
            "2362 [D loss: 0.015447, acc.: 100.00%] [G loss: 7.241851]\n",
            "2363 [D loss: 0.041281, acc.: 98.44%] [G loss: 5.112679]\n",
            "2364 [D loss: 0.015061, acc.: 100.00%] [G loss: 4.610283]\n",
            "2365 [D loss: 0.015679, acc.: 100.00%] [G loss: 6.048497]\n",
            "2366 [D loss: 0.118928, acc.: 96.88%] [G loss: 6.696531]\n",
            "2367 [D loss: 0.064044, acc.: 100.00%] [G loss: 7.049814]\n",
            "2368 [D loss: 0.021966, acc.: 100.00%] [G loss: 5.443946]\n",
            "2369 [D loss: 0.018163, acc.: 100.00%] [G loss: 4.553911]\n",
            "2370 [D loss: 0.017114, acc.: 100.00%] [G loss: 4.995743]\n",
            "2371 [D loss: 0.019319, acc.: 100.00%] [G loss: 5.060809]\n",
            "2372 [D loss: 0.043818, acc.: 100.00%] [G loss: 6.512568]\n",
            "2373 [D loss: 0.009904, acc.: 100.00%] [G loss: 7.773744]\n",
            "2374 [D loss: 0.096499, acc.: 96.88%] [G loss: 4.502013]\n",
            "2375 [D loss: 0.422454, acc.: 75.00%] [G loss: 15.926999]\n",
            "2376 [D loss: 2.215482, acc.: 48.44%] [G loss: 1.229027]\n",
            "2377 [D loss: 0.857350, acc.: 68.75%] [G loss: 15.165350]\n",
            "2378 [D loss: 0.323555, acc.: 84.38%] [G loss: 13.786959]\n",
            "2379 [D loss: 0.022173, acc.: 98.44%] [G loss: 11.350044]\n",
            "2380 [D loss: 0.003910, acc.: 100.00%] [G loss: 8.835972]\n",
            "2381 [D loss: 0.015371, acc.: 100.00%] [G loss: 5.763651]\n",
            "2382 [D loss: 0.169397, acc.: 93.75%] [G loss: 6.100710]\n",
            "2383 [D loss: 0.007349, acc.: 100.00%] [G loss: 6.947413]\n",
            "2384 [D loss: 0.253721, acc.: 90.62%] [G loss: 3.578274]\n",
            "2385 [D loss: 0.333385, acc.: 82.81%] [G loss: 11.039337]\n",
            "2386 [D loss: 0.182912, acc.: 89.06%] [G loss: 8.573507]\n",
            "2387 [D loss: 0.141283, acc.: 93.75%] [G loss: 4.243747]\n",
            "2388 [D loss: 0.180701, acc.: 92.19%] [G loss: 9.611366]\n",
            "2389 [D loss: 0.046294, acc.: 100.00%] [G loss: 8.989495]\n",
            "2390 [D loss: 0.018885, acc.: 100.00%] [G loss: 7.136586]\n",
            "2391 [D loss: 0.120984, acc.: 95.31%] [G loss: 7.135972]\n",
            "2392 [D loss: 0.029608, acc.: 100.00%] [G loss: 6.497931]\n",
            "2393 [D loss: 0.330757, acc.: 89.06%] [G loss: 5.762671]\n",
            "2394 [D loss: 0.010820, acc.: 100.00%] [G loss: 7.514316]\n",
            "2395 [D loss: 0.241713, acc.: 93.75%] [G loss: 7.846057]\n",
            "2396 [D loss: 0.064409, acc.: 98.44%] [G loss: 6.135667]\n",
            "2397 [D loss: 0.051367, acc.: 100.00%] [G loss: 6.385181]\n",
            "2398 [D loss: 0.132801, acc.: 93.75%] [G loss: 7.146709]\n",
            "2399 [D loss: 0.026946, acc.: 100.00%] [G loss: 7.734597]\n",
            "2400 [D loss: 0.039107, acc.: 100.00%] [G loss: 6.597038]\n",
            "2401 [D loss: 0.118431, acc.: 96.88%] [G loss: 6.373814]\n",
            "2402 [D loss: 0.016627, acc.: 100.00%] [G loss: 5.098870]\n",
            "2403 [D loss: 0.209388, acc.: 95.31%] [G loss: 10.379150]\n",
            "2404 [D loss: 0.007228, acc.: 100.00%] [G loss: 11.554423]\n",
            "2405 [D loss: 0.119488, acc.: 95.31%] [G loss: 8.992990]\n",
            "2406 [D loss: 0.014270, acc.: 100.00%] [G loss: 4.696836]\n",
            "2407 [D loss: 0.075699, acc.: 98.44%] [G loss: 5.891381]\n",
            "2408 [D loss: 0.013475, acc.: 100.00%] [G loss: 7.233823]\n",
            "2409 [D loss: 0.025982, acc.: 100.00%] [G loss: 5.629522]\n",
            "2410 [D loss: 0.022259, acc.: 100.00%] [G loss: 4.057113]\n",
            "2411 [D loss: 0.106020, acc.: 95.31%] [G loss: 6.363655]\n",
            "2412 [D loss: 0.055757, acc.: 100.00%] [G loss: 9.196896]\n",
            "2413 [D loss: 0.097121, acc.: 98.44%] [G loss: 7.049994]\n",
            "2414 [D loss: 0.028178, acc.: 100.00%] [G loss: 7.457167]\n",
            "2415 [D loss: 0.054307, acc.: 98.44%] [G loss: 6.434554]\n",
            "2416 [D loss: 0.031334, acc.: 100.00%] [G loss: 5.219610]\n",
            "2417 [D loss: 0.113969, acc.: 96.88%] [G loss: 6.684068]\n",
            "2418 [D loss: 0.032638, acc.: 100.00%] [G loss: 5.763438]\n",
            "2419 [D loss: 0.083280, acc.: 96.88%] [G loss: 5.114535]\n",
            "2420 [D loss: 0.009666, acc.: 100.00%] [G loss: 7.357547]\n",
            "2421 [D loss: 0.004602, acc.: 100.00%] [G loss: 7.025276]\n",
            "2422 [D loss: 0.074319, acc.: 98.44%] [G loss: 5.936547]\n",
            "2423 [D loss: 0.036432, acc.: 100.00%] [G loss: 5.053215]\n",
            "2424 [D loss: 0.073149, acc.: 100.00%] [G loss: 6.016805]\n",
            "2425 [D loss: 0.033316, acc.: 100.00%] [G loss: 6.353302]\n",
            "2426 [D loss: 0.034927, acc.: 100.00%] [G loss: 4.272685]\n",
            "2427 [D loss: 0.063578, acc.: 98.44%] [G loss: 4.803233]\n",
            "2428 [D loss: 0.039224, acc.: 100.00%] [G loss: 3.788133]\n",
            "2429 [D loss: 0.025908, acc.: 100.00%] [G loss: 5.972504]\n",
            "2430 [D loss: 0.045132, acc.: 100.00%] [G loss: 5.932212]\n",
            "2431 [D loss: 0.040262, acc.: 98.44%] [G loss: 4.729924]\n",
            "2432 [D loss: 0.056457, acc.: 100.00%] [G loss: 5.366369]\n",
            "2433 [D loss: 0.040897, acc.: 100.00%] [G loss: 5.730760]\n",
            "2434 [D loss: 0.036087, acc.: 98.44%] [G loss: 6.227267]\n",
            "2435 [D loss: 0.042711, acc.: 100.00%] [G loss: 4.517029]\n",
            "2436 [D loss: 0.043512, acc.: 98.44%] [G loss: 6.527194]\n",
            "2437 [D loss: 0.032143, acc.: 100.00%] [G loss: 5.445293]\n",
            "2438 [D loss: 0.024240, acc.: 100.00%] [G loss: 5.421048]\n",
            "2439 [D loss: 0.068483, acc.: 98.44%] [G loss: 6.581474]\n",
            "2440 [D loss: 0.138245, acc.: 96.88%] [G loss: 5.060076]\n",
            "2441 [D loss: 0.005872, acc.: 100.00%] [G loss: 5.450038]\n",
            "2442 [D loss: 0.006513, acc.: 100.00%] [G loss: 5.731231]\n",
            "2443 [D loss: 0.004578, acc.: 100.00%] [G loss: 5.793322]\n",
            "2444 [D loss: 0.041699, acc.: 98.44%] [G loss: 5.688009]\n",
            "2445 [D loss: 0.003943, acc.: 100.00%] [G loss: 6.283906]\n",
            "2446 [D loss: 0.031914, acc.: 100.00%] [G loss: 5.054192]\n",
            "2447 [D loss: 0.033804, acc.: 100.00%] [G loss: 4.202600]\n",
            "2448 [D loss: 0.035984, acc.: 100.00%] [G loss: 6.054573]\n",
            "2449 [D loss: 0.034802, acc.: 100.00%] [G loss: 7.823620]\n",
            "2450 [D loss: 0.085950, acc.: 96.88%] [G loss: 7.394909]\n",
            "2451 [D loss: 0.020407, acc.: 100.00%] [G loss: 4.538019]\n",
            "2452 [D loss: 0.108811, acc.: 96.88%] [G loss: 10.416101]\n",
            "2453 [D loss: 1.276340, acc.: 42.19%] [G loss: 16.112604]\n",
            "2454 [D loss: 1.468860, acc.: 54.69%] [G loss: 8.409891]\n",
            "2455 [D loss: 0.008651, acc.: 100.00%] [G loss: 7.237577]\n",
            "2456 [D loss: 0.073905, acc.: 95.31%] [G loss: 5.388652]\n",
            "2457 [D loss: 0.056740, acc.: 98.44%] [G loss: 7.031133]\n",
            "2458 [D loss: 0.010099, acc.: 100.00%] [G loss: 5.964936]\n",
            "2459 [D loss: 0.026849, acc.: 100.00%] [G loss: 5.286688]\n",
            "2460 [D loss: 0.078658, acc.: 96.88%] [G loss: 6.318753]\n",
            "2461 [D loss: 0.006335, acc.: 100.00%] [G loss: 6.680863]\n",
            "2462 [D loss: 0.046542, acc.: 98.44%] [G loss: 5.520807]\n",
            "2463 [D loss: 0.242926, acc.: 92.19%] [G loss: 6.189152]\n",
            "2464 [D loss: 0.012469, acc.: 100.00%] [G loss: 5.481665]\n",
            "2465 [D loss: 0.033639, acc.: 98.44%] [G loss: 4.679977]\n",
            "2466 [D loss: 0.067932, acc.: 96.88%] [G loss: 6.895980]\n",
            "2467 [D loss: 0.034787, acc.: 100.00%] [G loss: 6.228284]\n",
            "2468 [D loss: 0.108895, acc.: 98.44%] [G loss: 4.908523]\n",
            "2469 [D loss: 0.033955, acc.: 100.00%] [G loss: 6.392825]\n",
            "2470 [D loss: 0.054989, acc.: 100.00%] [G loss: 3.954262]\n",
            "2471 [D loss: 0.394501, acc.: 78.12%] [G loss: 13.029629]\n",
            "2472 [D loss: 0.724226, acc.: 60.94%] [G loss: 2.278964]\n",
            "2473 [D loss: 0.326432, acc.: 84.38%] [G loss: 9.663414]\n",
            "2474 [D loss: 0.002004, acc.: 100.00%] [G loss: 11.016281]\n",
            "2475 [D loss: 0.201080, acc.: 96.88%] [G loss: 8.905010]\n",
            "2476 [D loss: 0.002927, acc.: 100.00%] [G loss: 6.963185]\n",
            "2477 [D loss: 0.014641, acc.: 100.00%] [G loss: 4.851900]\n",
            "2478 [D loss: 0.099449, acc.: 95.31%] [G loss: 7.984744]\n",
            "2479 [D loss: 0.010138, acc.: 100.00%] [G loss: 9.071871]\n",
            "2480 [D loss: 0.039698, acc.: 100.00%] [G loss: 6.043552]\n",
            "2481 [D loss: 0.004618, acc.: 100.00%] [G loss: 4.009660]\n",
            "2482 [D loss: 0.019366, acc.: 100.00%] [G loss: 6.069710]\n",
            "2483 [D loss: 0.043542, acc.: 98.44%] [G loss: 5.122882]\n",
            "2484 [D loss: 0.025599, acc.: 100.00%] [G loss: 5.074727]\n",
            "2485 [D loss: 0.038846, acc.: 100.00%] [G loss: 5.757726]\n",
            "2486 [D loss: 0.044665, acc.: 100.00%] [G loss: 4.529775]\n",
            "2487 [D loss: 0.079715, acc.: 100.00%] [G loss: 5.206687]\n",
            "2488 [D loss: 0.048304, acc.: 98.44%] [G loss: 7.201480]\n",
            "2489 [D loss: 0.025086, acc.: 100.00%] [G loss: 6.064735]\n",
            "2490 [D loss: 0.156943, acc.: 95.31%] [G loss: 6.458894]\n",
            "2491 [D loss: 0.083287, acc.: 98.44%] [G loss: 4.927750]\n",
            "2492 [D loss: 0.114356, acc.: 98.44%] [G loss: 4.608953]\n",
            "2493 [D loss: 0.052331, acc.: 98.44%] [G loss: 6.271328]\n",
            "2494 [D loss: 0.062507, acc.: 100.00%] [G loss: 6.120045]\n",
            "2495 [D loss: 0.094761, acc.: 98.44%] [G loss: 5.603116]\n",
            "2496 [D loss: 0.023545, acc.: 100.00%] [G loss: 6.736659]\n",
            "2497 [D loss: 0.018906, acc.: 100.00%] [G loss: 6.666029]\n",
            "2498 [D loss: 0.015182, acc.: 100.00%] [G loss: 7.554363]\n",
            "2499 [D loss: 0.037825, acc.: 100.00%] [G loss: 5.101879]\n",
            "2500 [D loss: 0.022751, acc.: 100.00%] [G loss: 3.429737]\n",
            "2501 [D loss: 0.480305, acc.: 78.12%] [G loss: 15.921371]\n",
            "2502 [D loss: 1.771460, acc.: 51.56%] [G loss: 6.582893]\n",
            "2503 [D loss: 0.064220, acc.: 98.44%] [G loss: 5.358614]\n",
            "2504 [D loss: 0.105507, acc.: 93.75%] [G loss: 6.930889]\n",
            "2505 [D loss: 0.015104, acc.: 100.00%] [G loss: 5.384503]\n",
            "2506 [D loss: 0.001192, acc.: 100.00%] [G loss: 7.542593]\n",
            "2507 [D loss: 0.161493, acc.: 96.88%] [G loss: 7.418995]\n",
            "2508 [D loss: 0.002449, acc.: 100.00%] [G loss: 9.423653]\n",
            "2509 [D loss: 0.169036, acc.: 93.75%] [G loss: 5.986557]\n",
            "2510 [D loss: 0.067074, acc.: 96.88%] [G loss: 4.242815]\n",
            "2511 [D loss: 0.133720, acc.: 90.62%] [G loss: 5.732717]\n",
            "2512 [D loss: 0.002502, acc.: 100.00%] [G loss: 8.769337]\n",
            "2513 [D loss: 0.023310, acc.: 100.00%] [G loss: 7.582832]\n",
            "2514 [D loss: 0.064478, acc.: 98.44%] [G loss: 6.107332]\n",
            "2515 [D loss: 0.039953, acc.: 100.00%] [G loss: 5.093519]\n",
            "2516 [D loss: 0.007119, acc.: 100.00%] [G loss: 3.990049]\n",
            "2517 [D loss: 0.216728, acc.: 92.19%] [G loss: 9.991548]\n",
            "2518 [D loss: 0.034412, acc.: 100.00%] [G loss: 11.794895]\n",
            "2519 [D loss: 0.183635, acc.: 90.62%] [G loss: 5.757977]\n",
            "2520 [D loss: 0.004794, acc.: 100.00%] [G loss: 3.201012]\n",
            "2521 [D loss: 0.077258, acc.: 98.44%] [G loss: 5.115737]\n",
            "2522 [D loss: 0.013015, acc.: 100.00%] [G loss: 8.205817]\n",
            "2523 [D loss: 0.016915, acc.: 100.00%] [G loss: 6.541261]\n",
            "2524 [D loss: 0.053783, acc.: 98.44%] [G loss: 5.372561]\n",
            "2525 [D loss: 0.016007, acc.: 100.00%] [G loss: 7.104924]\n",
            "2526 [D loss: 0.546988, acc.: 68.75%] [G loss: 11.323405]\n",
            "2527 [D loss: 0.031178, acc.: 100.00%] [G loss: 13.373779]\n",
            "2528 [D loss: 0.109198, acc.: 95.31%] [G loss: 10.695457]\n",
            "2529 [D loss: 0.032954, acc.: 100.00%] [G loss: 7.181076]\n",
            "2530 [D loss: 0.020221, acc.: 100.00%] [G loss: 4.825613]\n",
            "2531 [D loss: 0.047105, acc.: 98.44%] [G loss: 5.281719]\n",
            "2532 [D loss: 0.010680, acc.: 100.00%] [G loss: 7.912110]\n",
            "2533 [D loss: 0.083156, acc.: 98.44%] [G loss: 5.743015]\n",
            "2534 [D loss: 0.001226, acc.: 100.00%] [G loss: 8.970554]\n",
            "2535 [D loss: 0.155979, acc.: 96.88%] [G loss: 3.659106]\n",
            "2536 [D loss: 0.083773, acc.: 95.31%] [G loss: 4.965067]\n",
            "2537 [D loss: 0.016939, acc.: 100.00%] [G loss: 5.012568]\n",
            "2538 [D loss: 0.024618, acc.: 100.00%] [G loss: 5.947108]\n",
            "2539 [D loss: 0.038078, acc.: 100.00%] [G loss: 5.617449]\n",
            "2540 [D loss: 0.041925, acc.: 100.00%] [G loss: 4.999577]\n",
            "2541 [D loss: 0.016848, acc.: 100.00%] [G loss: 4.917074]\n",
            "2542 [D loss: 0.087609, acc.: 100.00%] [G loss: 3.833038]\n",
            "2543 [D loss: 0.146547, acc.: 96.88%] [G loss: 9.286178]\n",
            "2544 [D loss: 0.043125, acc.: 100.00%] [G loss: 8.398421]\n",
            "2545 [D loss: 0.247283, acc.: 89.06%] [G loss: 5.853250]\n",
            "2546 [D loss: 0.005746, acc.: 100.00%] [G loss: 5.932200]\n",
            "2547 [D loss: 0.008767, acc.: 100.00%] [G loss: 6.114347]\n",
            "2548 [D loss: 0.016851, acc.: 100.00%] [G loss: 5.697955]\n",
            "2549 [D loss: 0.070895, acc.: 96.88%] [G loss: 8.524228]\n",
            "2550 [D loss: 0.023885, acc.: 100.00%] [G loss: 9.855932]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2551 [D loss: 0.020001, acc.: 100.00%] [G loss: 5.998769]\n",
            "2552 [D loss: 0.004118, acc.: 100.00%] [G loss: 6.427567]\n",
            "2553 [D loss: 0.053704, acc.: 100.00%] [G loss: 5.134659]\n",
            "2554 [D loss: 0.036714, acc.: 100.00%] [G loss: 4.794304]\n",
            "2555 [D loss: 0.080617, acc.: 98.44%] [G loss: 7.699808]\n",
            "2556 [D loss: 0.040425, acc.: 100.00%] [G loss: 8.240269]\n",
            "2557 [D loss: 0.017532, acc.: 100.00%] [G loss: 5.417544]\n",
            "2558 [D loss: 0.014942, acc.: 100.00%] [G loss: 6.165301]\n",
            "2559 [D loss: 0.133164, acc.: 95.31%] [G loss: 5.514303]\n",
            "2560 [D loss: 0.029868, acc.: 100.00%] [G loss: 6.147516]\n",
            "2561 [D loss: 0.016840, acc.: 100.00%] [G loss: 6.537527]\n",
            "2562 [D loss: 0.158094, acc.: 96.88%] [G loss: 4.717266]\n",
            "2563 [D loss: 0.007538, acc.: 100.00%] [G loss: 7.916245]\n",
            "2564 [D loss: 0.002524, acc.: 100.00%] [G loss: 5.352021]\n",
            "2565 [D loss: 0.024591, acc.: 100.00%] [G loss: 4.253024]\n",
            "2566 [D loss: 0.053250, acc.: 100.00%] [G loss: 4.784410]\n",
            "2567 [D loss: 0.050125, acc.: 100.00%] [G loss: 8.424309]\n",
            "2568 [D loss: 0.026744, acc.: 100.00%] [G loss: 8.795020]\n",
            "2569 [D loss: 0.048201, acc.: 100.00%] [G loss: 4.773699]\n",
            "2570 [D loss: 0.070104, acc.: 98.44%] [G loss: 5.662536]\n",
            "2571 [D loss: 0.014441, acc.: 100.00%] [G loss: 6.080745]\n",
            "2572 [D loss: 0.011752, acc.: 100.00%] [G loss: 7.442869]\n",
            "2573 [D loss: 0.153225, acc.: 96.88%] [G loss: 7.737175]\n",
            "2574 [D loss: 0.046537, acc.: 100.00%] [G loss: 7.345294]\n",
            "2575 [D loss: 0.004544, acc.: 100.00%] [G loss: 5.596485]\n",
            "2576 [D loss: 0.008952, acc.: 100.00%] [G loss: 5.412094]\n",
            "2577 [D loss: 0.109692, acc.: 96.88%] [G loss: 6.633986]\n",
            "2578 [D loss: 0.079609, acc.: 96.88%] [G loss: 3.347077]\n",
            "2579 [D loss: 0.020327, acc.: 100.00%] [G loss: 3.998085]\n",
            "2580 [D loss: 0.005579, acc.: 100.00%] [G loss: 6.799756]\n",
            "2581 [D loss: 0.025800, acc.: 100.00%] [G loss: 6.874545]\n",
            "2582 [D loss: 0.009949, acc.: 100.00%] [G loss: 5.874644]\n",
            "2583 [D loss: 0.228094, acc.: 89.06%] [G loss: 8.909467]\n",
            "2584 [D loss: 0.086306, acc.: 96.88%] [G loss: 8.419585]\n",
            "2585 [D loss: 0.075247, acc.: 98.44%] [G loss: 4.176004]\n",
            "2586 [D loss: 0.103665, acc.: 95.31%] [G loss: 9.131238]\n",
            "2587 [D loss: 0.013050, acc.: 100.00%] [G loss: 7.717227]\n",
            "2588 [D loss: 0.018272, acc.: 100.00%] [G loss: 9.375589]\n",
            "2589 [D loss: 0.005430, acc.: 100.00%] [G loss: 7.753431]\n",
            "2590 [D loss: 0.015009, acc.: 100.00%] [G loss: 5.966581]\n",
            "2591 [D loss: 0.017350, acc.: 100.00%] [G loss: 4.968089]\n",
            "2592 [D loss: 0.002920, acc.: 100.00%] [G loss: 4.260053]\n",
            "2593 [D loss: 0.004241, acc.: 100.00%] [G loss: 4.620058]\n",
            "2594 [D loss: 0.020535, acc.: 100.00%] [G loss: 4.804021]\n",
            "2595 [D loss: 0.050034, acc.: 98.44%] [G loss: 6.193186]\n",
            "2596 [D loss: 0.045170, acc.: 98.44%] [G loss: 5.446467]\n",
            "2597 [D loss: 0.006162, acc.: 100.00%] [G loss: 5.488549]\n",
            "2598 [D loss: 0.028927, acc.: 100.00%] [G loss: 4.014819]\n",
            "2599 [D loss: 0.174587, acc.: 95.31%] [G loss: 8.838832]\n",
            "2600 [D loss: 0.063048, acc.: 100.00%] [G loss: 9.762272]\n",
            "2601 [D loss: 0.009250, acc.: 100.00%] [G loss: 8.325943]\n",
            "2602 [D loss: 0.001575, acc.: 100.00%] [G loss: 5.480853]\n",
            "2603 [D loss: 0.025255, acc.: 98.44%] [G loss: 6.415845]\n",
            "2604 [D loss: 0.032227, acc.: 100.00%] [G loss: 7.406186]\n",
            "2605 [D loss: 0.033089, acc.: 98.44%] [G loss: 7.964499]\n",
            "2606 [D loss: 0.013002, acc.: 100.00%] [G loss: 9.693069]\n",
            "2607 [D loss: 0.013221, acc.: 100.00%] [G loss: 5.982239]\n",
            "2608 [D loss: 0.030336, acc.: 100.00%] [G loss: 5.578661]\n",
            "2609 [D loss: 0.020434, acc.: 100.00%] [G loss: 5.540511]\n",
            "2610 [D loss: 0.105039, acc.: 100.00%] [G loss: 4.226773]\n",
            "2611 [D loss: 0.035757, acc.: 100.00%] [G loss: 5.927385]\n",
            "2612 [D loss: 0.003905, acc.: 100.00%] [G loss: 5.773211]\n",
            "2613 [D loss: 0.082652, acc.: 96.88%] [G loss: 7.756079]\n",
            "2614 [D loss: 0.014701, acc.: 100.00%] [G loss: 7.964807]\n",
            "2615 [D loss: 0.007667, acc.: 100.00%] [G loss: 6.453960]\n",
            "2616 [D loss: 0.002409, acc.: 100.00%] [G loss: 6.541013]\n",
            "2617 [D loss: 0.002535, acc.: 100.00%] [G loss: 7.616509]\n",
            "2618 [D loss: 0.028158, acc.: 100.00%] [G loss: 5.953050]\n",
            "2619 [D loss: 0.021420, acc.: 100.00%] [G loss: 7.844387]\n",
            "2620 [D loss: 0.032104, acc.: 98.44%] [G loss: 6.337072]\n",
            "2621 [D loss: 0.031681, acc.: 100.00%] [G loss: 4.979206]\n",
            "2622 [D loss: 0.006997, acc.: 100.00%] [G loss: 8.510845]\n",
            "2623 [D loss: 0.093693, acc.: 98.44%] [G loss: 5.155394]\n",
            "2624 [D loss: 0.067975, acc.: 98.44%] [G loss: 9.291795]\n",
            "2625 [D loss: 0.028969, acc.: 100.00%] [G loss: 8.200107]\n",
            "2626 [D loss: 0.066921, acc.: 98.44%] [G loss: 5.192471]\n",
            "2627 [D loss: 0.012534, acc.: 100.00%] [G loss: 6.825284]\n",
            "2628 [D loss: 0.015027, acc.: 100.00%] [G loss: 6.626004]\n",
            "2629 [D loss: 0.004423, acc.: 100.00%] [G loss: 5.554270]\n",
            "2630 [D loss: 0.013323, acc.: 100.00%] [G loss: 5.664753]\n",
            "2631 [D loss: 0.033323, acc.: 98.44%] [G loss: 6.451226]\n",
            "2632 [D loss: 0.162662, acc.: 96.88%] [G loss: 6.568501]\n",
            "2633 [D loss: 0.025262, acc.: 100.00%] [G loss: 7.739023]\n",
            "2634 [D loss: 0.049170, acc.: 98.44%] [G loss: 8.225340]\n",
            "2635 [D loss: 0.000883, acc.: 100.00%] [G loss: 9.989142]\n",
            "2636 [D loss: 0.010177, acc.: 100.00%] [G loss: 7.594987]\n",
            "2637 [D loss: 0.032324, acc.: 98.44%] [G loss: 6.316088]\n",
            "2638 [D loss: 0.010869, acc.: 100.00%] [G loss: 6.258458]\n",
            "2639 [D loss: 0.031215, acc.: 100.00%] [G loss: 4.975468]\n",
            "2640 [D loss: 0.032277, acc.: 100.00%] [G loss: 7.583026]\n",
            "2641 [D loss: 0.001467, acc.: 100.00%] [G loss: 5.705756]\n",
            "2642 [D loss: 0.120678, acc.: 96.88%] [G loss: 7.841015]\n",
            "2643 [D loss: 0.020023, acc.: 100.00%] [G loss: 11.716743]\n",
            "2644 [D loss: 0.020666, acc.: 100.00%] [G loss: 10.969069]\n",
            "2645 [D loss: 0.015503, acc.: 100.00%] [G loss: 7.578783]\n",
            "2646 [D loss: 0.016003, acc.: 100.00%] [G loss: 6.316608]\n",
            "2647 [D loss: 0.013376, acc.: 100.00%] [G loss: 4.581044]\n",
            "2648 [D loss: 0.010235, acc.: 100.00%] [G loss: 7.157735]\n",
            "2649 [D loss: 0.022955, acc.: 100.00%] [G loss: 5.752736]\n",
            "2650 [D loss: 0.046333, acc.: 100.00%] [G loss: 6.811110]\n",
            "2651 [D loss: 0.138809, acc.: 95.31%] [G loss: 8.440010]\n",
            "2652 [D loss: 0.006672, acc.: 100.00%] [G loss: 9.713655]\n",
            "2653 [D loss: 0.022578, acc.: 100.00%] [G loss: 6.269429]\n",
            "2654 [D loss: 0.005724, acc.: 100.00%] [G loss: 5.033531]\n",
            "2655 [D loss: 0.004867, acc.: 100.00%] [G loss: 6.661809]\n",
            "2656 [D loss: 0.005075, acc.: 100.00%] [G loss: 7.025500]\n",
            "2657 [D loss: 0.181356, acc.: 92.19%] [G loss: 10.085812]\n",
            "2658 [D loss: 0.003120, acc.: 100.00%] [G loss: 10.416016]\n",
            "2659 [D loss: 0.140913, acc.: 95.31%] [G loss: 6.124319]\n",
            "2660 [D loss: 0.021279, acc.: 100.00%] [G loss: 6.529582]\n",
            "2661 [D loss: 0.043615, acc.: 100.00%] [G loss: 8.864182]\n",
            "2662 [D loss: 0.000968, acc.: 100.00%] [G loss: 8.476945]\n",
            "2663 [D loss: 0.015209, acc.: 100.00%] [G loss: 6.365409]\n",
            "2664 [D loss: 0.014434, acc.: 100.00%] [G loss: 7.957624]\n",
            "2665 [D loss: 0.051173, acc.: 100.00%] [G loss: 5.181231]\n",
            "2666 [D loss: 0.003953, acc.: 100.00%] [G loss: 6.764425]\n",
            "2667 [D loss: 0.142765, acc.: 95.31%] [G loss: 5.588889]\n",
            "2668 [D loss: 0.175004, acc.: 96.88%] [G loss: 5.327993]\n",
            "2669 [D loss: 0.032524, acc.: 100.00%] [G loss: 6.268135]\n",
            "2670 [D loss: 0.001241, acc.: 100.00%] [G loss: 7.604838]\n",
            "2671 [D loss: 0.004832, acc.: 100.00%] [G loss: 5.763474]\n",
            "2672 [D loss: 0.001188, acc.: 100.00%] [G loss: 7.532592]\n",
            "2673 [D loss: 0.012954, acc.: 100.00%] [G loss: 6.900893]\n",
            "2674 [D loss: 0.005321, acc.: 100.00%] [G loss: 4.163442]\n",
            "2675 [D loss: 0.027309, acc.: 100.00%] [G loss: 5.757805]\n",
            "2676 [D loss: 0.092586, acc.: 95.31%] [G loss: 3.125942]\n",
            "2677 [D loss: 0.011652, acc.: 100.00%] [G loss: 3.593831]\n",
            "2678 [D loss: 0.012252, acc.: 100.00%] [G loss: 3.653387]\n",
            "2679 [D loss: 0.052738, acc.: 98.44%] [G loss: 5.400142]\n",
            "2680 [D loss: 0.037706, acc.: 100.00%] [G loss: 6.251228]\n",
            "2681 [D loss: 0.003380, acc.: 100.00%] [G loss: 8.343733]\n",
            "2682 [D loss: 0.043785, acc.: 98.44%] [G loss: 4.457646]\n",
            "2683 [D loss: 0.012909, acc.: 100.00%] [G loss: 6.619308]\n",
            "2684 [D loss: 0.016797, acc.: 100.00%] [G loss: 7.192523]\n",
            "2685 [D loss: 0.193560, acc.: 93.75%] [G loss: 9.491158]\n",
            "2686 [D loss: 0.012720, acc.: 100.00%] [G loss: 10.895496]\n",
            "2687 [D loss: 0.017339, acc.: 100.00%] [G loss: 10.176378]\n",
            "2688 [D loss: 0.002916, acc.: 100.00%] [G loss: 9.374800]\n",
            "2689 [D loss: 0.006854, acc.: 100.00%] [G loss: 9.018783]\n",
            "2690 [D loss: 0.007871, acc.: 100.00%] [G loss: 6.676429]\n",
            "2691 [D loss: 0.005039, acc.: 100.00%] [G loss: 7.562818]\n",
            "2692 [D loss: 0.004250, acc.: 100.00%] [G loss: 5.288940]\n",
            "2693 [D loss: 0.022831, acc.: 100.00%] [G loss: 4.909261]\n",
            "2694 [D loss: 0.006401, acc.: 100.00%] [G loss: 7.943197]\n",
            "2695 [D loss: 0.025395, acc.: 100.00%] [G loss: 5.371339]\n",
            "2696 [D loss: 0.003425, acc.: 100.00%] [G loss: 6.304192]\n",
            "2697 [D loss: 0.021841, acc.: 100.00%] [G loss: 6.131615]\n",
            "2698 [D loss: 0.023989, acc.: 100.00%] [G loss: 5.755744]\n",
            "2699 [D loss: 0.025033, acc.: 100.00%] [G loss: 3.578257]\n",
            "2700 [D loss: 0.034982, acc.: 100.00%] [G loss: 5.072725]\n",
            "2701 [D loss: 0.005444, acc.: 100.00%] [G loss: 6.933939]\n",
            "2702 [D loss: 0.010825, acc.: 100.00%] [G loss: 6.861207]\n",
            "2703 [D loss: 0.057718, acc.: 100.00%] [G loss: 6.394085]\n",
            "2704 [D loss: 0.026607, acc.: 100.00%] [G loss: 4.823175]\n",
            "2705 [D loss: 0.011008, acc.: 100.00%] [G loss: 6.509095]\n",
            "2706 [D loss: 0.067818, acc.: 100.00%] [G loss: 6.839605]\n",
            "2707 [D loss: 0.014380, acc.: 100.00%] [G loss: 6.559087]\n",
            "2708 [D loss: 0.140246, acc.: 93.75%] [G loss: 8.351908]\n",
            "2709 [D loss: 0.006931, acc.: 100.00%] [G loss: 10.486738]\n",
            "2710 [D loss: 0.083806, acc.: 98.44%] [G loss: 6.717131]\n",
            "2711 [D loss: 0.038739, acc.: 100.00%] [G loss: 5.489433]\n",
            "2712 [D loss: 0.005059, acc.: 100.00%] [G loss: 6.878275]\n",
            "2713 [D loss: 0.005073, acc.: 100.00%] [G loss: 8.752912]\n",
            "2714 [D loss: 0.046657, acc.: 98.44%] [G loss: 4.429092]\n",
            "2715 [D loss: 0.099984, acc.: 96.88%] [G loss: 9.333188]\n",
            "2716 [D loss: 0.004073, acc.: 100.00%] [G loss: 13.111313]\n",
            "2717 [D loss: 0.241991, acc.: 89.06%] [G loss: 4.324767]\n",
            "2718 [D loss: 0.410685, acc.: 84.38%] [G loss: 14.718370]\n",
            "2719 [D loss: 0.030850, acc.: 100.00%] [G loss: 15.908186]\n",
            "2720 [D loss: 0.854259, acc.: 64.06%] [G loss: 2.821971]\n",
            "2721 [D loss: 1.551374, acc.: 54.69%] [G loss: 15.933652]\n",
            "2722 [D loss: 0.183005, acc.: 89.06%] [G loss: 16.118095]\n",
            "2723 [D loss: 0.792465, acc.: 76.56%] [G loss: 14.120235]\n",
            "2724 [D loss: 0.000052, acc.: 100.00%] [G loss: 9.675379]\n",
            "2725 [D loss: 0.024425, acc.: 100.00%] [G loss: 5.067030]\n",
            "2726 [D loss: 0.345013, acc.: 89.06%] [G loss: 13.332041]\n",
            "2727 [D loss: 0.000020, acc.: 100.00%] [G loss: 14.591532]\n",
            "2728 [D loss: 0.004722, acc.: 100.00%] [G loss: 13.284197]\n",
            "2729 [D loss: 0.004616, acc.: 100.00%] [G loss: 10.568714]\n",
            "2730 [D loss: 0.006194, acc.: 100.00%] [G loss: 8.314417]\n",
            "2731 [D loss: 0.034311, acc.: 100.00%] [G loss: 4.569335]\n",
            "2732 [D loss: 0.073103, acc.: 98.44%] [G loss: 5.267390]\n",
            "2733 [D loss: 0.076316, acc.: 98.44%] [G loss: 3.285599]\n",
            "2734 [D loss: 0.020795, acc.: 100.00%] [G loss: 5.078034]\n",
            "2735 [D loss: 0.270240, acc.: 89.06%] [G loss: 10.616239]\n",
            "2736 [D loss: 0.476200, acc.: 75.00%] [G loss: 5.771449]\n",
            "2737 [D loss: 0.234019, acc.: 90.62%] [G loss: 10.516739]\n",
            "2738 [D loss: 0.009814, acc.: 100.00%] [G loss: 12.157032]\n",
            "2739 [D loss: 0.572546, acc.: 75.00%] [G loss: 2.441866]\n",
            "2740 [D loss: 1.520251, acc.: 57.81%] [G loss: 15.575046]\n",
            "2741 [D loss: 0.229286, acc.: 87.50%] [G loss: 15.958692]\n",
            "2742 [D loss: 0.429257, acc.: 78.12%] [G loss: 11.246627]\n",
            "2743 [D loss: 0.002190, acc.: 100.00%] [G loss: 7.895093]\n",
            "2744 [D loss: 0.094938, acc.: 95.31%] [G loss: 6.684862]\n",
            "2745 [D loss: 0.006613, acc.: 100.00%] [G loss: 4.415740]\n",
            "2746 [D loss: 0.130900, acc.: 95.31%] [G loss: 7.513767]\n",
            "2747 [D loss: 0.364372, acc.: 84.38%] [G loss: 7.077442]\n",
            "2748 [D loss: 0.025083, acc.: 100.00%] [G loss: 8.657333]\n",
            "2749 [D loss: 0.001621, acc.: 100.00%] [G loss: 7.356201]\n",
            "2750 [D loss: 0.038085, acc.: 98.44%] [G loss: 6.969750]\n",
            "2751 [D loss: 0.022743, acc.: 100.00%] [G loss: 6.274643]\n",
            "2752 [D loss: 0.164452, acc.: 96.88%] [G loss: 7.054351]\n",
            "2753 [D loss: 0.029830, acc.: 100.00%] [G loss: 7.460631]\n",
            "2754 [D loss: 0.010670, acc.: 100.00%] [G loss: 8.288573]\n",
            "2755 [D loss: 0.446171, acc.: 78.12%] [G loss: 13.490064]\n",
            "2756 [D loss: 0.062247, acc.: 98.44%] [G loss: 14.977497]\n",
            "2757 [D loss: 0.711954, acc.: 71.88%] [G loss: 3.823486]\n",
            "2758 [D loss: 0.311141, acc.: 82.81%] [G loss: 12.379049]\n",
            "2759 [D loss: 0.000758, acc.: 100.00%] [G loss: 14.138054]\n",
            "2760 [D loss: 0.003479, acc.: 100.00%] [G loss: 11.599182]\n",
            "2761 [D loss: 0.001995, acc.: 100.00%] [G loss: 9.346938]\n",
            "2762 [D loss: 0.004022, acc.: 100.00%] [G loss: 8.343230]\n",
            "2763 [D loss: 0.019730, acc.: 100.00%] [G loss: 6.983235]\n",
            "2764 [D loss: 0.082915, acc.: 96.88%] [G loss: 4.339318]\n",
            "2765 [D loss: 0.061914, acc.: 98.44%] [G loss: 8.371635]\n",
            "2766 [D loss: 0.011795, acc.: 100.00%] [G loss: 8.844707]\n",
            "2767 [D loss: 0.175157, acc.: 93.75%] [G loss: 3.877341]\n",
            "2768 [D loss: 0.795115, acc.: 62.50%] [G loss: 15.306206]\n",
            "2769 [D loss: 1.514131, acc.: 53.12%] [G loss: 9.837628]\n",
            "2770 [D loss: 0.001270, acc.: 100.00%] [G loss: 5.395730]\n",
            "2771 [D loss: 0.014807, acc.: 100.00%] [G loss: 5.681322]\n",
            "2772 [D loss: 0.280785, acc.: 87.50%] [G loss: 9.496680]\n",
            "2773 [D loss: 0.002080, acc.: 100.00%] [G loss: 11.056355]\n",
            "2774 [D loss: 0.219481, acc.: 90.62%] [G loss: 5.169136]\n",
            "2775 [D loss: 0.068326, acc.: 98.44%] [G loss: 4.160288]\n",
            "2776 [D loss: 0.016714, acc.: 100.00%] [G loss: 5.371122]\n",
            "2777 [D loss: 0.261740, acc.: 93.75%] [G loss: 8.166172]\n",
            "2778 [D loss: 0.021176, acc.: 100.00%] [G loss: 9.832497]\n",
            "2779 [D loss: 0.179891, acc.: 89.06%] [G loss: 3.855976]\n",
            "2780 [D loss: 0.382251, acc.: 81.25%] [G loss: 12.270297]\n",
            "2781 [D loss: 0.237165, acc.: 89.06%] [G loss: 12.486364]\n",
            "2782 [D loss: 0.072844, acc.: 96.88%] [G loss: 7.820522]\n",
            "2783 [D loss: 0.014326, acc.: 100.00%] [G loss: 5.236618]\n",
            "2784 [D loss: 0.301391, acc.: 82.81%] [G loss: 11.147636]\n",
            "2785 [D loss: 0.608289, acc.: 76.56%] [G loss: 7.288035]\n",
            "2786 [D loss: 0.034226, acc.: 100.00%] [G loss: 7.244736]\n",
            "2787 [D loss: 0.010726, acc.: 100.00%] [G loss: 5.631882]\n",
            "2788 [D loss: 0.078867, acc.: 96.88%] [G loss: 6.403168]\n",
            "2789 [D loss: 0.022570, acc.: 100.00%] [G loss: 6.907032]\n",
            "2790 [D loss: 0.108147, acc.: 96.88%] [G loss: 4.154846]\n",
            "2791 [D loss: 0.055884, acc.: 98.44%] [G loss: 6.639334]\n",
            "2792 [D loss: 0.146575, acc.: 95.31%] [G loss: 7.587864]\n",
            "2793 [D loss: 0.038847, acc.: 98.44%] [G loss: 9.158752]\n",
            "2794 [D loss: 0.058820, acc.: 98.44%] [G loss: 6.181842]\n",
            "2795 [D loss: 0.057853, acc.: 98.44%] [G loss: 6.818526]\n",
            "2796 [D loss: 0.034436, acc.: 98.44%] [G loss: 7.886432]\n",
            "2797 [D loss: 0.194136, acc.: 93.75%] [G loss: 8.042656]\n",
            "2798 [D loss: 0.013305, acc.: 100.00%] [G loss: 8.558608]\n",
            "2799 [D loss: 0.007048, acc.: 100.00%] [G loss: 6.247601]\n",
            "2800 [D loss: 0.084814, acc.: 98.44%] [G loss: 4.768310]\n",
            "2801 [D loss: 0.030455, acc.: 100.00%] [G loss: 6.534989]\n",
            "2802 [D loss: 0.052638, acc.: 100.00%] [G loss: 7.313836]\n",
            "2803 [D loss: 0.053125, acc.: 100.00%] [G loss: 7.689057]\n",
            "2804 [D loss: 0.165132, acc.: 90.62%] [G loss: 4.442846]\n",
            "2805 [D loss: 0.192847, acc.: 92.19%] [G loss: 11.423172]\n",
            "2806 [D loss: 0.067319, acc.: 96.88%] [G loss: 13.364847]\n",
            "2807 [D loss: 0.029739, acc.: 100.00%] [G loss: 11.867811]\n",
            "2808 [D loss: 0.017060, acc.: 100.00%] [G loss: 10.333704]\n",
            "2809 [D loss: 0.048512, acc.: 100.00%] [G loss: 6.474058]\n",
            "2810 [D loss: 0.137449, acc.: 92.19%] [G loss: 9.044394]\n",
            "2811 [D loss: 0.010450, acc.: 100.00%] [G loss: 10.995365]\n",
            "2812 [D loss: 0.004168, acc.: 100.00%] [G loss: 9.007010]\n",
            "2813 [D loss: 0.017277, acc.: 100.00%] [G loss: 7.222166]\n",
            "2814 [D loss: 0.075147, acc.: 96.88%] [G loss: 7.576149]\n",
            "2815 [D loss: 0.080733, acc.: 96.88%] [G loss: 5.008327]\n",
            "2816 [D loss: 0.009695, acc.: 100.00%] [G loss: 4.527357]\n",
            "2817 [D loss: 0.017368, acc.: 100.00%] [G loss: 5.697037]\n",
            "2818 [D loss: 0.067646, acc.: 96.88%] [G loss: 5.915662]\n",
            "2819 [D loss: 0.028181, acc.: 100.00%] [G loss: 7.377665]\n",
            "2820 [D loss: 0.013452, acc.: 100.00%] [G loss: 6.404828]\n",
            "2821 [D loss: 0.379137, acc.: 79.69%] [G loss: 11.892977]\n",
            "2822 [D loss: 0.080801, acc.: 95.31%] [G loss: 14.293838]\n",
            "2823 [D loss: 0.058836, acc.: 100.00%] [G loss: 11.622765]\n",
            "2824 [D loss: 0.007797, acc.: 100.00%] [G loss: 9.259933]\n",
            "2825 [D loss: 0.017464, acc.: 100.00%] [G loss: 7.791550]\n",
            "2826 [D loss: 0.009680, acc.: 100.00%] [G loss: 6.139236]\n",
            "2827 [D loss: 0.004974, acc.: 100.00%] [G loss: 5.213923]\n",
            "2828 [D loss: 0.099053, acc.: 96.88%] [G loss: 7.721357]\n",
            "2829 [D loss: 0.010401, acc.: 100.00%] [G loss: 8.416664]\n",
            "2830 [D loss: 0.043617, acc.: 98.44%] [G loss: 7.913645]\n",
            "2831 [D loss: 0.050448, acc.: 100.00%] [G loss: 4.537902]\n",
            "2832 [D loss: 0.031892, acc.: 100.00%] [G loss: 6.252289]\n",
            "2833 [D loss: 0.027444, acc.: 100.00%] [G loss: 7.116372]\n",
            "2834 [D loss: 0.214407, acc.: 92.19%] [G loss: 4.742311]\n",
            "2835 [D loss: 0.027523, acc.: 100.00%] [G loss: 5.019407]\n",
            "2836 [D loss: 0.004390, acc.: 100.00%] [G loss: 7.767870]\n",
            "2837 [D loss: 0.045888, acc.: 98.44%] [G loss: 5.780280]\n",
            "2838 [D loss: 0.240600, acc.: 87.50%] [G loss: 13.237103]\n",
            "2839 [D loss: 0.142515, acc.: 95.31%] [G loss: 13.462310]\n",
            "2840 [D loss: 0.275545, acc.: 85.94%] [G loss: 5.613386]\n",
            "2841 [D loss: 0.088855, acc.: 96.88%] [G loss: 7.463650]\n",
            "2842 [D loss: 0.003961, acc.: 100.00%] [G loss: 6.592908]\n",
            "2843 [D loss: 0.010086, acc.: 100.00%] [G loss: 6.285454]\n",
            "2844 [D loss: 0.010809, acc.: 100.00%] [G loss: 5.862037]\n",
            "2845 [D loss: 0.054679, acc.: 98.44%] [G loss: 7.530803]\n",
            "2846 [D loss: 0.003118, acc.: 100.00%] [G loss: 8.562577]\n",
            "2847 [D loss: 0.007821, acc.: 100.00%] [G loss: 6.900953]\n",
            "2848 [D loss: 0.108808, acc.: 98.44%] [G loss: 8.330590]\n",
            "2849 [D loss: 0.013274, acc.: 100.00%] [G loss: 7.219187]\n",
            "2850 [D loss: 0.030034, acc.: 100.00%] [G loss: 5.953396]\n",
            "2851 [D loss: 0.029338, acc.: 100.00%] [G loss: 5.471745]\n",
            "2852 [D loss: 0.049702, acc.: 96.88%] [G loss: 7.386289]\n",
            "2853 [D loss: 0.030573, acc.: 98.44%] [G loss: 6.669188]\n",
            "2854 [D loss: 0.087888, acc.: 98.44%] [G loss: 5.383477]\n",
            "2855 [D loss: 0.106452, acc.: 98.44%] [G loss: 6.338739]\n",
            "2856 [D loss: 0.002167, acc.: 100.00%] [G loss: 8.412691]\n",
            "2857 [D loss: 0.047436, acc.: 100.00%] [G loss: 5.354135]\n",
            "2858 [D loss: 0.018870, acc.: 100.00%] [G loss: 4.286807]\n",
            "2859 [D loss: 0.030766, acc.: 100.00%] [G loss: 4.617324]\n",
            "2860 [D loss: 0.101544, acc.: 96.88%] [G loss: 9.421476]\n",
            "2861 [D loss: 0.019602, acc.: 100.00%] [G loss: 11.226505]\n",
            "2862 [D loss: 0.277524, acc.: 85.94%] [G loss: 4.111382]\n",
            "2863 [D loss: 0.594489, acc.: 68.75%] [G loss: 15.642854]\n",
            "2864 [D loss: 0.104082, acc.: 93.75%] [G loss: 16.105631]\n",
            "2865 [D loss: 1.737941, acc.: 50.00%] [G loss: 4.087422]\n",
            "2866 [D loss: 1.526866, acc.: 56.25%] [G loss: 15.176059]\n",
            "2867 [D loss: 0.062293, acc.: 98.44%] [G loss: 16.118095]\n",
            "2868 [D loss: 2.451790, acc.: 51.56%] [G loss: 5.029181]\n",
            "2869 [D loss: 0.251431, acc.: 90.62%] [G loss: 7.344370]\n",
            "2870 [D loss: 0.079359, acc.: 96.88%] [G loss: 6.215137]\n",
            "2871 [D loss: 0.010574, acc.: 100.00%] [G loss: 5.593597]\n",
            "2872 [D loss: 0.101217, acc.: 96.88%] [G loss: 6.801457]\n",
            "2873 [D loss: 0.023935, acc.: 100.00%] [G loss: 6.031928]\n",
            "2874 [D loss: 0.022781, acc.: 100.00%] [G loss: 4.561210]\n",
            "2875 [D loss: 0.304711, acc.: 87.50%] [G loss: 8.468231]\n",
            "2876 [D loss: 0.169788, acc.: 92.19%] [G loss: 6.395933]\n",
            "2877 [D loss: 3.093442, acc.: 15.62%] [G loss: 14.481038]\n",
            "2878 [D loss: 0.023572, acc.: 98.44%] [G loss: 16.118095]\n",
            "2879 [D loss: 0.875222, acc.: 65.62%] [G loss: 11.159449]\n",
            "2880 [D loss: 0.000662, acc.: 100.00%] [G loss: 6.712051]\n",
            "2881 [D loss: 0.002640, acc.: 100.00%] [G loss: 5.379617]\n",
            "2882 [D loss: 0.026667, acc.: 100.00%] [G loss: 5.468552]\n",
            "2883 [D loss: 0.053361, acc.: 100.00%] [G loss: 6.197529]\n",
            "2884 [D loss: 0.034994, acc.: 100.00%] [G loss: 4.240155]\n",
            "2885 [D loss: 0.096065, acc.: 96.88%] [G loss: 4.788751]\n",
            "2886 [D loss: 0.017659, acc.: 100.00%] [G loss: 5.231707]\n",
            "2887 [D loss: 0.061179, acc.: 100.00%] [G loss: 4.389032]\n",
            "2888 [D loss: 0.015392, acc.: 100.00%] [G loss: 4.623449]\n",
            "2889 [D loss: 0.018229, acc.: 100.00%] [G loss: 4.399493]\n",
            "2890 [D loss: 0.076400, acc.: 98.44%] [G loss: 3.800543]\n",
            "2891 [D loss: 0.163983, acc.: 96.88%] [G loss: 6.369096]\n",
            "2892 [D loss: 0.372536, acc.: 82.81%] [G loss: 5.557962]\n",
            "2893 [D loss: 0.019052, acc.: 100.00%] [G loss: 6.198420]\n",
            "2894 [D loss: 0.085986, acc.: 96.88%] [G loss: 3.520427]\n",
            "2895 [D loss: 0.021643, acc.: 100.00%] [G loss: 4.484361]\n",
            "2896 [D loss: 0.249503, acc.: 89.06%] [G loss: 8.487328]\n",
            "2897 [D loss: 0.026460, acc.: 100.00%] [G loss: 9.208403]\n",
            "2898 [D loss: 0.353790, acc.: 82.81%] [G loss: 2.921163]\n",
            "2899 [D loss: 0.161808, acc.: 89.06%] [G loss: 7.113048]\n",
            "2900 [D loss: 0.047744, acc.: 98.44%] [G loss: 6.833935]\n",
            "2901 [D loss: 0.021182, acc.: 100.00%] [G loss: 5.233016]\n",
            "2902 [D loss: 0.069352, acc.: 98.44%] [G loss: 5.661125]\n",
            "2903 [D loss: 0.223280, acc.: 89.06%] [G loss: 6.828331]\n",
            "2904 [D loss: 0.024815, acc.: 100.00%] [G loss: 9.818310]\n",
            "2905 [D loss: 0.221423, acc.: 89.06%] [G loss: 5.002015]\n",
            "2906 [D loss: 0.127044, acc.: 93.75%] [G loss: 7.699620]\n",
            "2907 [D loss: 0.013796, acc.: 100.00%] [G loss: 6.473649]\n",
            "2908 [D loss: 0.008472, acc.: 100.00%] [G loss: 5.228907]\n",
            "2909 [D loss: 0.103612, acc.: 96.88%] [G loss: 4.225853]\n",
            "2910 [D loss: 0.040954, acc.: 98.44%] [G loss: 4.795670]\n",
            "2911 [D loss: 0.081999, acc.: 96.88%] [G loss: 3.536856]\n",
            "2912 [D loss: 0.036474, acc.: 100.00%] [G loss: 4.390356]\n",
            "2913 [D loss: 0.105836, acc.: 96.88%] [G loss: 5.907037]\n",
            "2914 [D loss: 0.292947, acc.: 85.94%] [G loss: 3.918937]\n",
            "2915 [D loss: 0.087055, acc.: 96.88%] [G loss: 6.641319]\n",
            "2916 [D loss: 0.046350, acc.: 98.44%] [G loss: 5.381181]\n",
            "2917 [D loss: 0.103992, acc.: 98.44%] [G loss: 4.529476]\n",
            "2918 [D loss: 0.008787, acc.: 100.00%] [G loss: 5.889093]\n",
            "2919 [D loss: 0.017667, acc.: 100.00%] [G loss: 5.635219]\n",
            "2920 [D loss: 0.082406, acc.: 96.88%] [G loss: 6.110575]\n",
            "2921 [D loss: 0.175531, acc.: 92.19%] [G loss: 6.042357]\n",
            "2922 [D loss: 0.033052, acc.: 100.00%] [G loss: 5.335412]\n",
            "2923 [D loss: 0.052035, acc.: 100.00%] [G loss: 5.038486]\n",
            "2924 [D loss: 0.032890, acc.: 100.00%] [G loss: 4.965006]\n",
            "2925 [D loss: 0.092885, acc.: 98.44%] [G loss: 7.252296]\n",
            "2926 [D loss: 0.009687, acc.: 100.00%] [G loss: 8.782534]\n",
            "2927 [D loss: 0.048179, acc.: 98.44%] [G loss: 3.565815]\n",
            "2928 [D loss: 0.118349, acc.: 96.88%] [G loss: 7.551558]\n",
            "2929 [D loss: 0.160671, acc.: 93.75%] [G loss: 5.005699]\n",
            "2930 [D loss: 0.021628, acc.: 100.00%] [G loss: 5.950219]\n",
            "2931 [D loss: 0.065930, acc.: 98.44%] [G loss: 7.663053]\n",
            "2932 [D loss: 0.024740, acc.: 100.00%] [G loss: 8.243519]\n",
            "2933 [D loss: 0.042776, acc.: 100.00%] [G loss: 5.313156]\n",
            "2934 [D loss: 0.037278, acc.: 100.00%] [G loss: 4.848525]\n",
            "2935 [D loss: 0.217355, acc.: 89.06%] [G loss: 7.974478]\n",
            "2936 [D loss: 0.368863, acc.: 85.94%] [G loss: 3.257648]\n",
            "2937 [D loss: 0.129332, acc.: 93.75%] [G loss: 7.453731]\n",
            "2938 [D loss: 0.004716, acc.: 100.00%] [G loss: 8.573796]\n",
            "2939 [D loss: 0.019629, acc.: 100.00%] [G loss: 8.328601]\n",
            "2940 [D loss: 0.008428, acc.: 100.00%] [G loss: 5.632169]\n",
            "2941 [D loss: 0.046116, acc.: 98.44%] [G loss: 6.203943]\n",
            "2942 [D loss: 0.030365, acc.: 100.00%] [G loss: 5.097268]\n",
            "2943 [D loss: 0.025169, acc.: 100.00%] [G loss: 4.738546]\n",
            "2944 [D loss: 0.037989, acc.: 100.00%] [G loss: 4.358206]\n",
            "2945 [D loss: 0.040382, acc.: 98.44%] [G loss: 7.156462]\n",
            "2946 [D loss: 0.024449, acc.: 100.00%] [G loss: 5.699598]\n",
            "2947 [D loss: 0.133610, acc.: 95.31%] [G loss: 3.569880]\n",
            "2948 [D loss: 0.103542, acc.: 98.44%] [G loss: 8.114355]\n",
            "2949 [D loss: 0.011931, acc.: 100.00%] [G loss: 8.139490]\n",
            "2950 [D loss: 0.004395, acc.: 100.00%] [G loss: 8.440905]\n",
            "2951 [D loss: 0.012568, acc.: 100.00%] [G loss: 8.063855]\n",
            "2952 [D loss: 0.016387, acc.: 100.00%] [G loss: 5.264775]\n",
            "2953 [D loss: 0.018276, acc.: 100.00%] [G loss: 5.049720]\n",
            "2954 [D loss: 0.232703, acc.: 93.75%] [G loss: 3.150854]\n",
            "2955 [D loss: 0.067855, acc.: 100.00%] [G loss: 6.986897]\n",
            "2956 [D loss: 0.011408, acc.: 100.00%] [G loss: 7.062533]\n",
            "2957 [D loss: 0.011459, acc.: 100.00%] [G loss: 6.176045]\n",
            "2958 [D loss: 0.015397, acc.: 100.00%] [G loss: 4.228321]\n",
            "2959 [D loss: 0.079891, acc.: 98.44%] [G loss: 5.893265]\n",
            "2960 [D loss: 0.070697, acc.: 98.44%] [G loss: 5.816887]\n",
            "2961 [D loss: 0.013344, acc.: 100.00%] [G loss: 5.931062]\n",
            "2962 [D loss: 0.209271, acc.: 93.75%] [G loss: 9.568459]\n",
            "2963 [D loss: 0.064927, acc.: 98.44%] [G loss: 7.525342]\n",
            "2964 [D loss: 0.004661, acc.: 100.00%] [G loss: 7.584407]\n",
            "2965 [D loss: 0.095553, acc.: 96.88%] [G loss: 4.075281]\n",
            "2966 [D loss: 0.060379, acc.: 96.88%] [G loss: 6.904442]\n",
            "2967 [D loss: 0.009591, acc.: 100.00%] [G loss: 6.338032]\n",
            "2968 [D loss: 0.005678, acc.: 100.00%] [G loss: 4.387506]\n",
            "2969 [D loss: 0.134457, acc.: 96.88%] [G loss: 4.820735]\n",
            "2970 [D loss: 0.008167, acc.: 100.00%] [G loss: 8.479971]\n",
            "2971 [D loss: 0.013695, acc.: 100.00%] [G loss: 5.165005]\n",
            "2972 [D loss: 0.049188, acc.: 98.44%] [G loss: 6.993358]\n",
            "2973 [D loss: 0.013163, acc.: 100.00%] [G loss: 5.805986]\n",
            "2974 [D loss: 0.019199, acc.: 100.00%] [G loss: 6.716020]\n",
            "2975 [D loss: 0.043444, acc.: 100.00%] [G loss: 7.212983]\n",
            "2976 [D loss: 0.102983, acc.: 100.00%] [G loss: 6.699198]\n",
            "2977 [D loss: 0.012701, acc.: 100.00%] [G loss: 6.477807]\n",
            "2978 [D loss: 0.034646, acc.: 100.00%] [G loss: 7.438046]\n",
            "2979 [D loss: 0.029451, acc.: 100.00%] [G loss: 4.801677]\n",
            "2980 [D loss: 0.019653, acc.: 100.00%] [G loss: 5.325867]\n",
            "2981 [D loss: 0.026219, acc.: 100.00%] [G loss: 7.328898]\n",
            "2982 [D loss: 0.014654, acc.: 100.00%] [G loss: 4.491578]\n",
            "2983 [D loss: 0.315640, acc.: 85.94%] [G loss: 15.250902]\n",
            "2984 [D loss: 0.229273, acc.: 93.75%] [G loss: 15.895280]\n",
            "2985 [D loss: 0.450982, acc.: 76.56%] [G loss: 3.899541]\n",
            "2986 [D loss: 0.220770, acc.: 89.06%] [G loss: 5.955142]\n",
            "2987 [D loss: 0.000136, acc.: 100.00%] [G loss: 10.394251]\n",
            "2988 [D loss: 0.000192, acc.: 100.00%] [G loss: 9.900811]\n",
            "2989 [D loss: 0.006129, acc.: 100.00%] [G loss: 8.557114]\n",
            "2990 [D loss: 0.061642, acc.: 98.44%] [G loss: 6.654298]\n",
            "2991 [D loss: 0.043773, acc.: 98.44%] [G loss: 5.388513]\n",
            "2992 [D loss: 0.012632, acc.: 100.00%] [G loss: 6.610331]\n",
            "2993 [D loss: 0.005195, acc.: 100.00%] [G loss: 4.490502]\n",
            "2994 [D loss: 0.031146, acc.: 100.00%] [G loss: 4.874179]\n",
            "2995 [D loss: 0.002100, acc.: 100.00%] [G loss: 6.524799]\n",
            "2996 [D loss: 0.058221, acc.: 98.44%] [G loss: 6.139033]\n",
            "2997 [D loss: 0.009355, acc.: 100.00%] [G loss: 7.286580]\n",
            "2998 [D loss: 0.007533, acc.: 100.00%] [G loss: 5.851329]\n",
            "2999 [D loss: 0.044497, acc.: 98.44%] [G loss: 6.433759]\n",
            "3000 [D loss: 0.019131, acc.: 100.00%] [G loss: 5.843293]\n",
            "3001 [D loss: 0.012740, acc.: 100.00%] [G loss: 6.715476]\n",
            "3002 [D loss: 0.050784, acc.: 98.44%] [G loss: 5.147617]\n",
            "3003 [D loss: 0.201568, acc.: 92.19%] [G loss: 8.723079]\n",
            "3004 [D loss: 0.012743, acc.: 100.00%] [G loss: 14.157060]\n",
            "3005 [D loss: 0.335310, acc.: 84.38%] [G loss: 6.529670]\n",
            "3006 [D loss: 0.047044, acc.: 98.44%] [G loss: 5.060819]\n",
            "3007 [D loss: 0.079231, acc.: 98.44%] [G loss: 7.530305]\n",
            "3008 [D loss: 0.016049, acc.: 100.00%] [G loss: 7.949849]\n",
            "3009 [D loss: 0.004314, acc.: 100.00%] [G loss: 9.897345]\n",
            "3010 [D loss: 0.129588, acc.: 92.19%] [G loss: 5.937814]\n",
            "3011 [D loss: 0.008181, acc.: 100.00%] [G loss: 4.837255]\n",
            "3012 [D loss: 0.031793, acc.: 100.00%] [G loss: 3.420380]\n",
            "3013 [D loss: 0.020854, acc.: 100.00%] [G loss: 5.175032]\n",
            "3014 [D loss: 0.017774, acc.: 100.00%] [G loss: 5.567190]\n",
            "3015 [D loss: 0.005874, acc.: 100.00%] [G loss: 6.910463]\n",
            "3016 [D loss: 0.034578, acc.: 100.00%] [G loss: 6.422304]\n",
            "3017 [D loss: 0.049754, acc.: 98.44%] [G loss: 6.723197]\n",
            "3018 [D loss: 0.016504, acc.: 100.00%] [G loss: 6.881293]\n",
            "3019 [D loss: 0.068573, acc.: 100.00%] [G loss: 6.647471]\n",
            "3020 [D loss: 0.009704, acc.: 100.00%] [G loss: 4.750475]\n",
            "3021 [D loss: 0.013613, acc.: 100.00%] [G loss: 3.864341]\n",
            "3022 [D loss: 0.019316, acc.: 100.00%] [G loss: 5.417886]\n",
            "3023 [D loss: 0.205394, acc.: 95.31%] [G loss: 8.920045]\n",
            "3024 [D loss: 0.285005, acc.: 85.94%] [G loss: 4.026598]\n",
            "3025 [D loss: 0.188105, acc.: 92.19%] [G loss: 8.757833]\n",
            "3026 [D loss: 0.003898, acc.: 100.00%] [G loss: 12.178713]\n",
            "3027 [D loss: 0.002030, acc.: 100.00%] [G loss: 12.481361]\n",
            "3028 [D loss: 0.012885, acc.: 100.00%] [G loss: 10.632668]\n",
            "3029 [D loss: 0.005514, acc.: 100.00%] [G loss: 9.492182]\n",
            "3030 [D loss: 0.002427, acc.: 100.00%] [G loss: 8.684361]\n",
            "3031 [D loss: 0.007206, acc.: 100.00%] [G loss: 6.174706]\n",
            "3032 [D loss: 0.003499, acc.: 100.00%] [G loss: 6.067439]\n",
            "3033 [D loss: 0.005700, acc.: 100.00%] [G loss: 7.002855]\n",
            "3034 [D loss: 0.004941, acc.: 100.00%] [G loss: 5.150322]\n",
            "3035 [D loss: 0.015056, acc.: 100.00%] [G loss: 3.500787]\n",
            "3036 [D loss: 0.066397, acc.: 98.44%] [G loss: 7.351405]\n",
            "3037 [D loss: 0.007812, acc.: 100.00%] [G loss: 6.383996]\n",
            "3038 [D loss: 0.016864, acc.: 100.00%] [G loss: 3.479305]\n",
            "3039 [D loss: 0.198615, acc.: 95.31%] [G loss: 5.854907]\n",
            "3040 [D loss: 0.018122, acc.: 100.00%] [G loss: 5.730100]\n",
            "3041 [D loss: 0.084684, acc.: 98.44%] [G loss: 7.187077]\n",
            "3042 [D loss: 0.100559, acc.: 96.88%] [G loss: 6.465255]\n",
            "3043 [D loss: 0.041939, acc.: 98.44%] [G loss: 4.829779]\n",
            "3044 [D loss: 0.010111, acc.: 100.00%] [G loss: 8.218292]\n",
            "3045 [D loss: 0.080334, acc.: 100.00%] [G loss: 4.838558]\n",
            "3046 [D loss: 0.060555, acc.: 96.88%] [G loss: 7.865225]\n",
            "3047 [D loss: 0.008971, acc.: 100.00%] [G loss: 7.870083]\n",
            "3048 [D loss: 0.006765, acc.: 100.00%] [G loss: 6.038888]\n",
            "3049 [D loss: 0.016065, acc.: 100.00%] [G loss: 7.000015]\n",
            "3050 [D loss: 0.023858, acc.: 100.00%] [G loss: 5.574617]\n",
            "3051 [D loss: 0.122521, acc.: 96.88%] [G loss: 9.541952]\n",
            "3052 [D loss: 0.009737, acc.: 100.00%] [G loss: 11.721170]\n",
            "3053 [D loss: 0.002654, acc.: 100.00%] [G loss: 11.607908]\n",
            "3054 [D loss: 0.176448, acc.: 95.31%] [G loss: 3.386248]\n",
            "3055 [D loss: 0.066196, acc.: 96.88%] [G loss: 6.102666]\n",
            "3056 [D loss: 0.007293, acc.: 100.00%] [G loss: 6.884965]\n",
            "3057 [D loss: 0.007213, acc.: 100.00%] [G loss: 6.585800]\n",
            "3058 [D loss: 0.043354, acc.: 100.00%] [G loss: 5.454155]\n",
            "3059 [D loss: 0.006457, acc.: 100.00%] [G loss: 4.451100]\n",
            "3060 [D loss: 0.016266, acc.: 100.00%] [G loss: 6.364731]\n",
            "3061 [D loss: 0.004358, acc.: 100.00%] [G loss: 6.285564]\n",
            "3062 [D loss: 0.011606, acc.: 100.00%] [G loss: 5.278001]\n",
            "3063 [D loss: 0.018511, acc.: 100.00%] [G loss: 6.876140]\n",
            "3064 [D loss: 0.005070, acc.: 100.00%] [G loss: 5.334625]\n",
            "3065 [D loss: 0.005882, acc.: 100.00%] [G loss: 6.224951]\n",
            "3066 [D loss: 0.067199, acc.: 98.44%] [G loss: 3.867277]\n",
            "3067 [D loss: 0.041252, acc.: 98.44%] [G loss: 5.676651]\n",
            "3068 [D loss: 0.024405, acc.: 100.00%] [G loss: 4.579249]\n",
            "3069 [D loss: 0.005301, acc.: 100.00%] [G loss: 4.498180]\n",
            "3070 [D loss: 0.016507, acc.: 100.00%] [G loss: 5.277793]\n",
            "3071 [D loss: 0.186420, acc.: 90.62%] [G loss: 10.687184]\n",
            "3072 [D loss: 0.009175, acc.: 100.00%] [G loss: 10.182881]\n",
            "3073 [D loss: 0.024555, acc.: 100.00%] [G loss: 9.397226]\n",
            "3074 [D loss: 0.012405, acc.: 100.00%] [G loss: 7.768585]\n",
            "3075 [D loss: 0.018168, acc.: 100.00%] [G loss: 6.689990]\n",
            "3076 [D loss: 0.010806, acc.: 100.00%] [G loss: 6.058023]\n",
            "3077 [D loss: 0.101055, acc.: 95.31%] [G loss: 8.579409]\n",
            "3078 [D loss: 0.120955, acc.: 95.31%] [G loss: 6.151317]\n",
            "3079 [D loss: 0.041597, acc.: 98.44%] [G loss: 7.088772]\n",
            "3080 [D loss: 0.003441, acc.: 100.00%] [G loss: 6.601800]\n",
            "3081 [D loss: 0.005760, acc.: 100.00%] [G loss: 6.475040]\n",
            "3082 [D loss: 0.026544, acc.: 98.44%] [G loss: 5.555691]\n",
            "3083 [D loss: 0.014666, acc.: 100.00%] [G loss: 3.996975]\n",
            "3084 [D loss: 0.027893, acc.: 100.00%] [G loss: 6.862438]\n",
            "3085 [D loss: 0.016313, acc.: 100.00%] [G loss: 6.243588]\n",
            "3086 [D loss: 0.004662, acc.: 100.00%] [G loss: 7.550388]\n",
            "3087 [D loss: 0.083426, acc.: 100.00%] [G loss: 6.920856]\n",
            "3088 [D loss: 0.024267, acc.: 100.00%] [G loss: 7.877285]\n",
            "3089 [D loss: 0.016747, acc.: 100.00%] [G loss: 6.532895]\n",
            "3090 [D loss: 0.090233, acc.: 96.88%] [G loss: 6.968876]\n",
            "3091 [D loss: 0.022737, acc.: 98.44%] [G loss: 4.641655]\n",
            "3092 [D loss: 0.033133, acc.: 100.00%] [G loss: 6.523940]\n",
            "3093 [D loss: 0.010333, acc.: 100.00%] [G loss: 7.522882]\n",
            "3094 [D loss: 0.079355, acc.: 98.44%] [G loss: 5.173162]\n",
            "3095 [D loss: 0.008996, acc.: 100.00%] [G loss: 4.388452]\n",
            "3096 [D loss: 0.054585, acc.: 98.44%] [G loss: 9.073519]\n",
            "3097 [D loss: 0.004339, acc.: 100.00%] [G loss: 8.756107]\n",
            "3098 [D loss: 0.005790, acc.: 100.00%] [G loss: 7.304874]\n",
            "3099 [D loss: 0.028689, acc.: 98.44%] [G loss: 5.012171]\n",
            "3100 [D loss: 0.005894, acc.: 100.00%] [G loss: 6.184436]\n",
            "3101 [D loss: 0.023564, acc.: 98.44%] [G loss: 4.853992]\n",
            "3102 [D loss: 0.003026, acc.: 100.00%] [G loss: 5.123211]\n",
            "3103 [D loss: 0.085299, acc.: 98.44%] [G loss: 5.090813]\n",
            "3104 [D loss: 0.036674, acc.: 100.00%] [G loss: 6.014230]\n",
            "3105 [D loss: 0.002522, acc.: 100.00%] [G loss: 6.651665]\n",
            "3106 [D loss: 0.005217, acc.: 100.00%] [G loss: 8.002470]\n",
            "3107 [D loss: 0.015277, acc.: 100.00%] [G loss: 5.866549]\n",
            "3108 [D loss: 0.024158, acc.: 100.00%] [G loss: 5.529115]\n",
            "3109 [D loss: 0.012124, acc.: 100.00%] [G loss: 5.169001]\n",
            "3110 [D loss: 0.017190, acc.: 100.00%] [G loss: 4.832405]\n",
            "3111 [D loss: 0.066822, acc.: 96.88%] [G loss: 6.902143]\n",
            "3112 [D loss: 0.001926, acc.: 100.00%] [G loss: 9.579483]\n",
            "3113 [D loss: 0.076649, acc.: 96.88%] [G loss: 6.286227]\n",
            "3114 [D loss: 0.200142, acc.: 89.06%] [G loss: 13.320573]\n",
            "3115 [D loss: 0.155557, acc.: 95.31%] [G loss: 10.630798]\n",
            "3116 [D loss: 0.004018, acc.: 100.00%] [G loss: 11.483213]\n",
            "3117 [D loss: 0.001669, acc.: 100.00%] [G loss: 8.833139]\n",
            "3118 [D loss: 0.003054, acc.: 100.00%] [G loss: 8.880199]\n",
            "3119 [D loss: 0.002489, acc.: 100.00%] [G loss: 7.672754]\n",
            "3120 [D loss: 0.003742, acc.: 100.00%] [G loss: 7.995381]\n",
            "3121 [D loss: 0.103387, acc.: 96.88%] [G loss: 9.770782]\n",
            "3122 [D loss: 0.026359, acc.: 98.44%] [G loss: 10.827964]\n",
            "3123 [D loss: 0.020988, acc.: 100.00%] [G loss: 9.009315]\n",
            "3124 [D loss: 0.046863, acc.: 98.44%] [G loss: 6.927282]\n",
            "3125 [D loss: 0.019184, acc.: 100.00%] [G loss: 6.284594]\n",
            "3126 [D loss: 0.013879, acc.: 100.00%] [G loss: 6.585209]\n",
            "3127 [D loss: 0.038652, acc.: 100.00%] [G loss: 5.378805]\n",
            "3128 [D loss: 0.064880, acc.: 98.44%] [G loss: 6.598324]\n",
            "3129 [D loss: 0.001087, acc.: 100.00%] [G loss: 9.316883]\n",
            "3130 [D loss: 0.013264, acc.: 100.00%] [G loss: 7.540620]\n",
            "3131 [D loss: 0.009324, acc.: 100.00%] [G loss: 7.533341]\n",
            "3132 [D loss: 0.003888, acc.: 100.00%] [G loss: 7.443379]\n",
            "3133 [D loss: 0.094905, acc.: 96.88%] [G loss: 4.125679]\n",
            "3134 [D loss: 0.029089, acc.: 100.00%] [G loss: 8.301050]\n",
            "3135 [D loss: 0.017127, acc.: 100.00%] [G loss: 6.667744]\n",
            "3136 [D loss: 0.005876, acc.: 100.00%] [G loss: 5.660477]\n",
            "3137 [D loss: 0.011649, acc.: 100.00%] [G loss: 5.735052]\n",
            "3138 [D loss: 0.009417, acc.: 100.00%] [G loss: 5.029463]\n",
            "3139 [D loss: 0.013683, acc.: 100.00%] [G loss: 6.313918]\n",
            "3140 [D loss: 0.081029, acc.: 100.00%] [G loss: 9.434536]\n",
            "3141 [D loss: 0.020166, acc.: 100.00%] [G loss: 11.123499]\n",
            "3142 [D loss: 0.039790, acc.: 98.44%] [G loss: 7.088254]\n",
            "3143 [D loss: 0.049520, acc.: 100.00%] [G loss: 4.417577]\n",
            "3144 [D loss: 0.036192, acc.: 98.44%] [G loss: 6.147927]\n",
            "3145 [D loss: 0.001862, acc.: 100.00%] [G loss: 9.628139]\n",
            "3146 [D loss: 0.009170, acc.: 100.00%] [G loss: 5.562653]\n",
            "3147 [D loss: 0.007907, acc.: 100.00%] [G loss: 6.561726]\n",
            "3148 [D loss: 0.076832, acc.: 98.44%] [G loss: 5.460597]\n",
            "3149 [D loss: 0.016027, acc.: 100.00%] [G loss: 5.621954]\n",
            "3150 [D loss: 0.005224, acc.: 100.00%] [G loss: 8.332619]\n",
            "3151 [D loss: 0.054251, acc.: 98.44%] [G loss: 8.436335]\n",
            "3152 [D loss: 0.027697, acc.: 100.00%] [G loss: 7.063128]\n",
            "3153 [D loss: 0.002814, acc.: 100.00%] [G loss: 5.866848]\n",
            "3154 [D loss: 0.013751, acc.: 100.00%] [G loss: 5.317849]\n",
            "3155 [D loss: 0.019570, acc.: 100.00%] [G loss: 5.655377]\n",
            "3156 [D loss: 0.038044, acc.: 100.00%] [G loss: 7.664535]\n",
            "3157 [D loss: 0.048380, acc.: 96.88%] [G loss: 4.971102]\n",
            "3158 [D loss: 0.021713, acc.: 100.00%] [G loss: 7.202836]\n",
            "3159 [D loss: 0.011514, acc.: 100.00%] [G loss: 6.472963]\n",
            "3160 [D loss: 0.027737, acc.: 100.00%] [G loss: 5.706682]\n",
            "3161 [D loss: 0.012954, acc.: 100.00%] [G loss: 6.819562]\n",
            "3162 [D loss: 0.011611, acc.: 100.00%] [G loss: 6.685617]\n",
            "3163 [D loss: 0.000918, acc.: 100.00%] [G loss: 8.175800]\n",
            "3164 [D loss: 0.068923, acc.: 98.44%] [G loss: 8.001562]\n",
            "3165 [D loss: 0.111868, acc.: 95.31%] [G loss: 4.359181]\n",
            "3166 [D loss: 0.007143, acc.: 100.00%] [G loss: 4.495445]\n",
            "3167 [D loss: 0.012117, acc.: 100.00%] [G loss: 7.050894]\n",
            "3168 [D loss: 0.009838, acc.: 100.00%] [G loss: 5.584967]\n",
            "3169 [D loss: 0.008004, acc.: 100.00%] [G loss: 6.743259]\n",
            "3170 [D loss: 0.008190, acc.: 100.00%] [G loss: 6.415538]\n",
            "3171 [D loss: 0.046731, acc.: 100.00%] [G loss: 7.144804]\n",
            "3172 [D loss: 0.027330, acc.: 100.00%] [G loss: 7.152674]\n",
            "3173 [D loss: 0.008310, acc.: 100.00%] [G loss: 9.191240]\n",
            "3174 [D loss: 0.141208, acc.: 95.31%] [G loss: 3.492927]\n",
            "3175 [D loss: 0.156376, acc.: 92.19%] [G loss: 13.119843]\n",
            "3176 [D loss: 0.006902, acc.: 100.00%] [G loss: 15.363096]\n",
            "3177 [D loss: 0.462893, acc.: 79.69%] [G loss: 2.573912]\n",
            "3178 [D loss: 0.346103, acc.: 82.81%] [G loss: 15.509205]\n",
            "3179 [D loss: 0.000265, acc.: 100.00%] [G loss: 15.027674]\n",
            "3180 [D loss: 0.065738, acc.: 100.00%] [G loss: 12.818790]\n",
            "3181 [D loss: 0.002332, acc.: 100.00%] [G loss: 13.838630]\n",
            "3182 [D loss: 0.002206, acc.: 100.00%] [G loss: 10.432541]\n",
            "3183 [D loss: 0.005134, acc.: 100.00%] [G loss: 9.306316]\n",
            "3184 [D loss: 0.005150, acc.: 100.00%] [G loss: 6.079871]\n",
            "3185 [D loss: 0.009329, acc.: 100.00%] [G loss: 6.625283]\n",
            "3186 [D loss: 0.013798, acc.: 100.00%] [G loss: 5.351724]\n",
            "3187 [D loss: 0.019498, acc.: 100.00%] [G loss: 3.985425]\n",
            "3188 [D loss: 0.005885, acc.: 100.00%] [G loss: 5.495683]\n",
            "3189 [D loss: 0.009348, acc.: 100.00%] [G loss: 5.290623]\n",
            "3190 [D loss: 0.340413, acc.: 82.81%] [G loss: 13.771978]\n",
            "3191 [D loss: 1.123987, acc.: 60.94%] [G loss: 3.521328]\n",
            "3192 [D loss: 0.552907, acc.: 75.00%] [G loss: 15.814231]\n",
            "3193 [D loss: 0.007638, acc.: 100.00%] [G loss: 16.118095]\n",
            "3194 [D loss: 0.449857, acc.: 82.81%] [G loss: 14.472245]\n",
            "3195 [D loss: 0.001219, acc.: 100.00%] [G loss: 12.341641]\n",
            "3196 [D loss: 0.000045, acc.: 100.00%] [G loss: 10.062361]\n",
            "3197 [D loss: 0.001770, acc.: 100.00%] [G loss: 7.510958]\n",
            "3198 [D loss: 0.001967, acc.: 100.00%] [G loss: 6.231060]\n",
            "3199 [D loss: 0.043520, acc.: 100.00%] [G loss: 6.564785]\n",
            "3200 [D loss: 0.004019, acc.: 100.00%] [G loss: 5.169988]\n",
            "3201 [D loss: 0.205791, acc.: 98.44%] [G loss: 4.076151]\n",
            "3202 [D loss: 0.059418, acc.: 98.44%] [G loss: 7.628722]\n",
            "3203 [D loss: 0.002905, acc.: 100.00%] [G loss: 8.666898]\n",
            "3204 [D loss: 0.011775, acc.: 100.00%] [G loss: 5.700059]\n",
            "3205 [D loss: 0.014080, acc.: 100.00%] [G loss: 7.384283]\n",
            "3206 [D loss: 0.070324, acc.: 100.00%] [G loss: 7.091355]\n",
            "3207 [D loss: 0.053249, acc.: 100.00%] [G loss: 4.293841]\n",
            "3208 [D loss: 0.049295, acc.: 100.00%] [G loss: 6.533085]\n",
            "3209 [D loss: 0.244470, acc.: 92.19%] [G loss: 5.761750]\n",
            "3210 [D loss: 0.003404, acc.: 100.00%] [G loss: 7.484370]\n",
            "3211 [D loss: 0.006415, acc.: 100.00%] [G loss: 5.614631]\n",
            "3212 [D loss: 0.123574, acc.: 93.75%] [G loss: 11.051308]\n",
            "3213 [D loss: 0.182601, acc.: 92.19%] [G loss: 8.350859]\n",
            "3214 [D loss: 0.016486, acc.: 100.00%] [G loss: 5.631654]\n",
            "3215 [D loss: 0.008189, acc.: 100.00%] [G loss: 5.724884]\n",
            "3216 [D loss: 0.006588, acc.: 100.00%] [G loss: 6.149529]\n",
            "3217 [D loss: 0.111537, acc.: 96.88%] [G loss: 9.065856]\n",
            "3218 [D loss: 0.011472, acc.: 100.00%] [G loss: 11.025700]\n",
            "3219 [D loss: 0.012133, acc.: 100.00%] [G loss: 8.437376]\n",
            "3220 [D loss: 0.008221, acc.: 100.00%] [G loss: 9.061855]\n",
            "3221 [D loss: 0.030055, acc.: 100.00%] [G loss: 5.802562]\n",
            "3222 [D loss: 0.035785, acc.: 98.44%] [G loss: 5.993760]\n",
            "3223 [D loss: 0.076369, acc.: 96.88%] [G loss: 9.976274]\n",
            "3224 [D loss: 0.007286, acc.: 100.00%] [G loss: 11.970991]\n",
            "3225 [D loss: 0.077008, acc.: 96.88%] [G loss: 8.889883]\n",
            "3226 [D loss: 0.076508, acc.: 98.44%] [G loss: 6.000012]\n",
            "3227 [D loss: 0.110427, acc.: 96.88%] [G loss: 9.711535]\n",
            "3228 [D loss: 0.001166, acc.: 100.00%] [G loss: 9.864412]\n",
            "3229 [D loss: 0.023018, acc.: 100.00%] [G loss: 9.354435]\n",
            "3230 [D loss: 0.065074, acc.: 100.00%] [G loss: 6.318943]\n",
            "3231 [D loss: 0.010264, acc.: 100.00%] [G loss: 6.177939]\n",
            "3232 [D loss: 0.042988, acc.: 100.00%] [G loss: 7.085464]\n",
            "3233 [D loss: 0.084517, acc.: 98.44%] [G loss: 5.083076]\n",
            "3234 [D loss: 0.135646, acc.: 93.75%] [G loss: 9.505452]\n",
            "3235 [D loss: 0.084180, acc.: 95.31%] [G loss: 10.748626]\n",
            "3236 [D loss: 0.002679, acc.: 100.00%] [G loss: 9.181959]\n",
            "3237 [D loss: 0.003549, acc.: 100.00%] [G loss: 6.523148]\n",
            "3238 [D loss: 0.005556, acc.: 100.00%] [G loss: 6.360889]\n",
            "3239 [D loss: 0.008908, acc.: 100.00%] [G loss: 7.295593]\n",
            "3240 [D loss: 0.017669, acc.: 100.00%] [G loss: 7.286234]\n",
            "3241 [D loss: 0.011606, acc.: 100.00%] [G loss: 6.661700]\n",
            "3242 [D loss: 0.002388, acc.: 100.00%] [G loss: 3.650042]\n",
            "3243 [D loss: 0.066574, acc.: 98.44%] [G loss: 4.225769]\n",
            "3244 [D loss: 0.034183, acc.: 100.00%] [G loss: 6.002731]\n",
            "3245 [D loss: 0.056713, acc.: 98.44%] [G loss: 6.332861]\n",
            "3246 [D loss: 0.036430, acc.: 98.44%] [G loss: 6.018258]\n",
            "3247 [D loss: 0.057498, acc.: 100.00%] [G loss: 4.035353]\n",
            "3248 [D loss: 0.067279, acc.: 98.44%] [G loss: 8.465981]\n",
            "3249 [D loss: 0.001820, acc.: 100.00%] [G loss: 7.738287]\n",
            "3250 [D loss: 0.043186, acc.: 100.00%] [G loss: 7.389654]\n",
            "3251 [D loss: 0.007490, acc.: 100.00%] [G loss: 5.714686]\n",
            "3252 [D loss: 0.014081, acc.: 100.00%] [G loss: 5.241034]\n",
            "3253 [D loss: 0.032973, acc.: 100.00%] [G loss: 5.750525]\n",
            "3254 [D loss: 0.030277, acc.: 100.00%] [G loss: 8.466152]\n",
            "3255 [D loss: 0.025683, acc.: 100.00%] [G loss: 7.567276]\n",
            "3256 [D loss: 0.003709, acc.: 100.00%] [G loss: 9.901656]\n",
            "3257 [D loss: 0.006695, acc.: 100.00%] [G loss: 8.578849]\n",
            "3258 [D loss: 0.007405, acc.: 100.00%] [G loss: 5.974355]\n",
            "3259 [D loss: 0.034840, acc.: 98.44%] [G loss: 7.868202]\n",
            "3260 [D loss: 0.008250, acc.: 100.00%] [G loss: 7.193615]\n",
            "3261 [D loss: 0.013581, acc.: 100.00%] [G loss: 6.006536]\n",
            "3262 [D loss: 0.011373, acc.: 100.00%] [G loss: 7.073782]\n",
            "3263 [D loss: 0.016132, acc.: 100.00%] [G loss: 6.734431]\n",
            "3264 [D loss: 0.076407, acc.: 100.00%] [G loss: 5.543395]\n",
            "3265 [D loss: 0.012014, acc.: 100.00%] [G loss: 6.813046]\n",
            "3266 [D loss: 0.004653, acc.: 100.00%] [G loss: 7.515827]\n",
            "3267 [D loss: 0.145071, acc.: 93.75%] [G loss: 8.348070]\n",
            "3268 [D loss: 0.017756, acc.: 100.00%] [G loss: 8.219145]\n",
            "3269 [D loss: 0.020269, acc.: 100.00%] [G loss: 6.314203]\n",
            "3270 [D loss: 0.003478, acc.: 100.00%] [G loss: 6.170103]\n",
            "3271 [D loss: 0.007988, acc.: 100.00%] [G loss: 6.152415]\n",
            "3272 [D loss: 0.009475, acc.: 100.00%] [G loss: 5.916749]\n",
            "3273 [D loss: 0.055059, acc.: 98.44%] [G loss: 5.260274]\n",
            "3274 [D loss: 0.006940, acc.: 100.00%] [G loss: 4.016819]\n",
            "3275 [D loss: 0.083304, acc.: 98.44%] [G loss: 6.659772]\n",
            "3276 [D loss: 0.006096, acc.: 100.00%] [G loss: 8.427155]\n",
            "3277 [D loss: 0.061719, acc.: 100.00%] [G loss: 8.403715]\n",
            "3278 [D loss: 0.009762, acc.: 100.00%] [G loss: 5.997159]\n",
            "3279 [D loss: 0.020614, acc.: 100.00%] [G loss: 7.066165]\n",
            "3280 [D loss: 0.009792, acc.: 100.00%] [G loss: 7.918765]\n",
            "3281 [D loss: 0.004255, acc.: 100.00%] [G loss: 9.080046]\n",
            "3282 [D loss: 0.005097, acc.: 100.00%] [G loss: 7.174238]\n",
            "3283 [D loss: 0.026327, acc.: 100.00%] [G loss: 7.960917]\n",
            "3284 [D loss: 0.013874, acc.: 100.00%] [G loss: 6.490350]\n",
            "3285 [D loss: 0.016477, acc.: 100.00%] [G loss: 6.522026]\n",
            "3286 [D loss: 0.029685, acc.: 100.00%] [G loss: 7.858362]\n",
            "3287 [D loss: 0.062828, acc.: 98.44%] [G loss: 8.369810]\n",
            "3288 [D loss: 0.005862, acc.: 100.00%] [G loss: 10.177755]\n",
            "3289 [D loss: 0.009414, acc.: 100.00%] [G loss: 9.514049]\n",
            "3290 [D loss: 0.058712, acc.: 98.44%] [G loss: 5.498704]\n",
            "3291 [D loss: 0.053584, acc.: 100.00%] [G loss: 7.784190]\n",
            "3292 [D loss: 0.001847, acc.: 100.00%] [G loss: 7.532284]\n",
            "3293 [D loss: 0.007809, acc.: 100.00%] [G loss: 9.302750]\n",
            "3294 [D loss: 0.026476, acc.: 100.00%] [G loss: 8.099079]\n",
            "3295 [D loss: 0.005985, acc.: 100.00%] [G loss: 6.915489]\n",
            "3296 [D loss: 0.011487, acc.: 100.00%] [G loss: 5.872280]\n",
            "3297 [D loss: 0.026051, acc.: 100.00%] [G loss: 6.012217]\n",
            "3298 [D loss: 0.011134, acc.: 100.00%] [G loss: 6.997028]\n",
            "3299 [D loss: 0.001100, acc.: 100.00%] [G loss: 8.284802]\n",
            "3300 [D loss: 0.017386, acc.: 100.00%] [G loss: 6.051689]\n",
            "3301 [D loss: 0.017702, acc.: 100.00%] [G loss: 6.659707]\n",
            "3302 [D loss: 0.004079, acc.: 100.00%] [G loss: 4.876276]\n",
            "3303 [D loss: 0.016489, acc.: 100.00%] [G loss: 6.758268]\n",
            "3304 [D loss: 0.028856, acc.: 98.44%] [G loss: 5.241947]\n",
            "3305 [D loss: 0.037380, acc.: 98.44%] [G loss: 4.346169]\n",
            "3306 [D loss: 0.061772, acc.: 100.00%] [G loss: 8.905905]\n",
            "3307 [D loss: 0.000417, acc.: 100.00%] [G loss: 12.178217]\n",
            "3308 [D loss: 0.466933, acc.: 82.81%] [G loss: 6.312587]\n",
            "3309 [D loss: 0.008675, acc.: 100.00%] [G loss: 5.312726]\n",
            "3310 [D loss: 0.002415, acc.: 100.00%] [G loss: 5.707177]\n",
            "3311 [D loss: 0.001901, acc.: 100.00%] [G loss: 6.219110]\n",
            "3312 [D loss: 0.029843, acc.: 98.44%] [G loss: 7.321893]\n",
            "3313 [D loss: 0.001174, acc.: 100.00%] [G loss: 7.129959]\n",
            "3314 [D loss: 0.007135, acc.: 100.00%] [G loss: 5.729433]\n",
            "3315 [D loss: 0.002226, acc.: 100.00%] [G loss: 6.535600]\n",
            "3316 [D loss: 0.008793, acc.: 100.00%] [G loss: 5.643473]\n",
            "3317 [D loss: 0.019324, acc.: 100.00%] [G loss: 7.286020]\n",
            "3318 [D loss: 0.002120, acc.: 100.00%] [G loss: 7.495469]\n",
            "3319 [D loss: 0.001567, acc.: 100.00%] [G loss: 8.253475]\n",
            "3320 [D loss: 0.019093, acc.: 100.00%] [G loss: 5.302021]\n",
            "3321 [D loss: 0.029483, acc.: 98.44%] [G loss: 8.128925]\n",
            "3322 [D loss: 0.298335, acc.: 89.06%] [G loss: 11.259497]\n",
            "3323 [D loss: 0.006435, acc.: 100.00%] [G loss: 10.460802]\n",
            "3324 [D loss: 0.025980, acc.: 100.00%] [G loss: 8.682959]\n",
            "3325 [D loss: 0.007234, acc.: 100.00%] [G loss: 8.740078]\n",
            "3326 [D loss: 0.017541, acc.: 100.00%] [G loss: 4.999276]\n",
            "3327 [D loss: 0.004503, acc.: 100.00%] [G loss: 7.892106]\n",
            "3328 [D loss: 0.004065, acc.: 100.00%] [G loss: 6.909266]\n",
            "3329 [D loss: 0.001312, acc.: 100.00%] [G loss: 8.540468]\n",
            "3330 [D loss: 0.034349, acc.: 100.00%] [G loss: 4.668447]\n",
            "3331 [D loss: 0.024475, acc.: 100.00%] [G loss: 6.883945]\n",
            "3332 [D loss: 0.004640, acc.: 100.00%] [G loss: 6.313128]\n",
            "3333 [D loss: 0.042914, acc.: 100.00%] [G loss: 8.854477]\n",
            "3334 [D loss: 0.032881, acc.: 100.00%] [G loss: 6.583300]\n",
            "3335 [D loss: 0.003009, acc.: 100.00%] [G loss: 5.860793]\n",
            "3336 [D loss: 0.007136, acc.: 100.00%] [G loss: 6.183974]\n",
            "3337 [D loss: 0.012564, acc.: 100.00%] [G loss: 6.683888]\n",
            "3338 [D loss: 0.188835, acc.: 93.75%] [G loss: 14.941430]\n",
            "3339 [D loss: 0.084564, acc.: 96.88%] [G loss: 14.763391]\n",
            "3340 [D loss: 0.193725, acc.: 92.19%] [G loss: 7.561653]\n",
            "3341 [D loss: 0.062637, acc.: 98.44%] [G loss: 5.350102]\n",
            "3342 [D loss: 0.007331, acc.: 100.00%] [G loss: 6.934357]\n",
            "3343 [D loss: 0.004813, acc.: 100.00%] [G loss: 8.433421]\n",
            "3344 [D loss: 0.004448, acc.: 100.00%] [G loss: 7.627479]\n",
            "3345 [D loss: 0.000324, acc.: 100.00%] [G loss: 8.927944]\n",
            "3346 [D loss: 0.010312, acc.: 100.00%] [G loss: 4.484812]\n",
            "3347 [D loss: 0.013690, acc.: 100.00%] [G loss: 5.763041]\n",
            "3348 [D loss: 0.002236, acc.: 100.00%] [G loss: 7.315077]\n",
            "3349 [D loss: 0.002222, acc.: 100.00%] [G loss: 8.052904]\n",
            "3350 [D loss: 0.000798, acc.: 100.00%] [G loss: 8.086749]\n",
            "3351 [D loss: 0.046169, acc.: 98.44%] [G loss: 6.315669]\n",
            "3352 [D loss: 0.017840, acc.: 100.00%] [G loss: 7.098158]\n",
            "3353 [D loss: 0.001711, acc.: 100.00%] [G loss: 8.363678]\n",
            "3354 [D loss: 0.032467, acc.: 100.00%] [G loss: 7.746680]\n",
            "3355 [D loss: 0.118985, acc.: 96.88%] [G loss: 11.013988]\n",
            "3356 [D loss: 0.029223, acc.: 100.00%] [G loss: 12.992252]\n",
            "3357 [D loss: 0.076551, acc.: 96.88%] [G loss: 7.029194]\n",
            "3358 [D loss: 0.001818, acc.: 100.00%] [G loss: 7.559784]\n",
            "3359 [D loss: 0.008059, acc.: 100.00%] [G loss: 6.148642]\n",
            "3360 [D loss: 0.029068, acc.: 100.00%] [G loss: 4.931141]\n",
            "3361 [D loss: 0.009764, acc.: 100.00%] [G loss: 4.586023]\n",
            "3362 [D loss: 0.031360, acc.: 100.00%] [G loss: 6.686363]\n",
            "3363 [D loss: 0.008365, acc.: 100.00%] [G loss: 8.775047]\n",
            "3364 [D loss: 0.003311, acc.: 100.00%] [G loss: 5.958394]\n",
            "3365 [D loss: 0.006179, acc.: 100.00%] [G loss: 6.681429]\n",
            "3366 [D loss: 0.002178, acc.: 100.00%] [G loss: 8.063237]\n",
            "3367 [D loss: 0.046642, acc.: 100.00%] [G loss: 7.808714]\n",
            "3368 [D loss: 0.008749, acc.: 100.00%] [G loss: 8.940956]\n",
            "3369 [D loss: 0.181920, acc.: 92.19%] [G loss: 3.385252]\n",
            "3370 [D loss: 0.375315, acc.: 85.94%] [G loss: 15.896374]\n",
            "3371 [D loss: 0.633237, acc.: 67.19%] [G loss: 12.189791]\n",
            "3372 [D loss: 0.000209, acc.: 100.00%] [G loss: 8.524525]\n",
            "3373 [D loss: 0.282682, acc.: 87.50%] [G loss: 10.296625]\n",
            "3374 [D loss: 0.002194, acc.: 100.00%] [G loss: 12.269232]\n",
            "3375 [D loss: 0.116754, acc.: 96.88%] [G loss: 11.984865]\n",
            "3376 [D loss: 0.003143, acc.: 100.00%] [G loss: 6.934266]\n",
            "3377 [D loss: 0.000576, acc.: 100.00%] [G loss: 6.928362]\n",
            "3378 [D loss: 0.008426, acc.: 100.00%] [G loss: 4.325013]\n",
            "3379 [D loss: 0.250632, acc.: 85.94%] [G loss: 12.658831]\n",
            "3380 [D loss: 0.039962, acc.: 100.00%] [G loss: 12.902762]\n",
            "3381 [D loss: 0.783736, acc.: 67.19%] [G loss: 5.142433]\n",
            "3382 [D loss: 0.220602, acc.: 90.62%] [G loss: 12.186255]\n",
            "3383 [D loss: 0.001830, acc.: 100.00%] [G loss: 12.997072]\n",
            "3384 [D loss: 0.581157, acc.: 68.75%] [G loss: 6.231526]\n",
            "3385 [D loss: 0.003508, acc.: 100.00%] [G loss: 6.561626]\n",
            "3386 [D loss: 0.579071, acc.: 76.56%] [G loss: 13.671000]\n",
            "3387 [D loss: 0.000125, acc.: 100.00%] [G loss: 15.611195]\n",
            "3388 [D loss: 0.061932, acc.: 98.44%] [G loss: 15.376766]\n",
            "3389 [D loss: 0.022287, acc.: 98.44%] [G loss: 13.436199]\n",
            "3390 [D loss: 0.035999, acc.: 98.44%] [G loss: 11.176596]\n",
            "3391 [D loss: 0.007825, acc.: 100.00%] [G loss: 7.818106]\n",
            "3392 [D loss: 0.013062, acc.: 100.00%] [G loss: 6.578001]\n",
            "3393 [D loss: 0.089023, acc.: 96.88%] [G loss: 8.099319]\n",
            "3394 [D loss: 0.002287, acc.: 100.00%] [G loss: 8.442780]\n",
            "3395 [D loss: 0.015547, acc.: 100.00%] [G loss: 6.737744]\n",
            "3396 [D loss: 0.137505, acc.: 96.88%] [G loss: 7.051598]\n",
            "3397 [D loss: 0.007734, acc.: 100.00%] [G loss: 9.454894]\n",
            "3398 [D loss: 0.027063, acc.: 100.00%] [G loss: 6.332705]\n",
            "3399 [D loss: 0.006852, acc.: 100.00%] [G loss: 5.106835]\n",
            "3400 [D loss: 0.225431, acc.: 89.06%] [G loss: 12.065687]\n",
            "3401 [D loss: 0.186835, acc.: 93.75%] [G loss: 9.898665]\n",
            "3402 [D loss: 0.033433, acc.: 98.44%] [G loss: 7.989170]\n",
            "3403 [D loss: 0.002601, acc.: 100.00%] [G loss: 7.272715]\n",
            "3404 [D loss: 0.042285, acc.: 100.00%] [G loss: 7.022864]\n",
            "3405 [D loss: 0.002971, acc.: 100.00%] [G loss: 5.834935]\n",
            "3406 [D loss: 0.030837, acc.: 100.00%] [G loss: 5.616001]\n",
            "3407 [D loss: 0.034165, acc.: 98.44%] [G loss: 7.693196]\n",
            "3408 [D loss: 0.045593, acc.: 100.00%] [G loss: 6.368752]\n",
            "3409 [D loss: 0.052634, acc.: 98.44%] [G loss: 7.641808]\n",
            "3410 [D loss: 0.009699, acc.: 100.00%] [G loss: 6.584519]\n",
            "3411 [D loss: 0.110098, acc.: 98.44%] [G loss: 6.323776]\n",
            "3412 [D loss: 0.008770, acc.: 100.00%] [G loss: 8.034758]\n",
            "3413 [D loss: 0.003902, acc.: 100.00%] [G loss: 5.753530]\n",
            "3414 [D loss: 0.059396, acc.: 100.00%] [G loss: 5.155292]\n",
            "3415 [D loss: 0.110445, acc.: 98.44%] [G loss: 11.155969]\n",
            "3416 [D loss: 0.038993, acc.: 100.00%] [G loss: 14.853151]\n",
            "3417 [D loss: 0.415405, acc.: 79.69%] [G loss: 4.919081]\n",
            "3418 [D loss: 0.138062, acc.: 95.31%] [G loss: 8.833573]\n",
            "3419 [D loss: 0.002030, acc.: 100.00%] [G loss: 7.931241]\n",
            "3420 [D loss: 0.001939, acc.: 100.00%] [G loss: 8.508667]\n",
            "3421 [D loss: 0.007153, acc.: 100.00%] [G loss: 6.828714]\n",
            "3422 [D loss: 0.011834, acc.: 100.00%] [G loss: 5.797986]\n",
            "3423 [D loss: 0.048478, acc.: 100.00%] [G loss: 7.304885]\n",
            "3424 [D loss: 0.001737, acc.: 100.00%] [G loss: 5.090444]\n",
            "3425 [D loss: 0.029945, acc.: 100.00%] [G loss: 5.706512]\n",
            "3426 [D loss: 0.029515, acc.: 98.44%] [G loss: 4.808956]\n",
            "3427 [D loss: 0.123876, acc.: 95.31%] [G loss: 10.265936]\n",
            "3428 [D loss: 0.007531, acc.: 100.00%] [G loss: 10.376484]\n",
            "3429 [D loss: 0.026908, acc.: 100.00%] [G loss: 9.114301]\n",
            "3430 [D loss: 0.011002, acc.: 100.00%] [G loss: 8.189342]\n",
            "3431 [D loss: 0.023924, acc.: 100.00%] [G loss: 7.779200]\n",
            "3432 [D loss: 0.011058, acc.: 100.00%] [G loss: 5.070351]\n",
            "3433 [D loss: 0.803122, acc.: 68.75%] [G loss: 15.964090]\n",
            "3434 [D loss: 0.455614, acc.: 81.25%] [G loss: 15.239146]\n",
            "3435 [D loss: 0.008033, acc.: 100.00%] [G loss: 12.757755]\n",
            "3436 [D loss: 0.002444, acc.: 100.00%] [G loss: 11.376196]\n",
            "3437 [D loss: 0.003325, acc.: 100.00%] [G loss: 9.822603]\n",
            "3438 [D loss: 0.000519, acc.: 100.00%] [G loss: 9.379801]\n",
            "3439 [D loss: 0.018986, acc.: 98.44%] [G loss: 6.200632]\n",
            "3440 [D loss: 0.022589, acc.: 100.00%] [G loss: 7.432411]\n",
            "3441 [D loss: 0.035067, acc.: 100.00%] [G loss: 6.641134]\n",
            "3442 [D loss: 0.002365, acc.: 100.00%] [G loss: 6.841870]\n",
            "3443 [D loss: 0.058051, acc.: 98.44%] [G loss: 5.962163]\n",
            "3444 [D loss: 0.023936, acc.: 100.00%] [G loss: 7.039003]\n",
            "3445 [D loss: 0.012362, acc.: 100.00%] [G loss: 7.950689]\n",
            "3446 [D loss: 0.015426, acc.: 100.00%] [G loss: 4.028306]\n",
            "3447 [D loss: 0.003801, acc.: 100.00%] [G loss: 7.096688]\n",
            "3448 [D loss: 0.106615, acc.: 95.31%] [G loss: 5.722339]\n",
            "3449 [D loss: 0.025967, acc.: 100.00%] [G loss: 6.043089]\n",
            "3450 [D loss: 0.022742, acc.: 100.00%] [G loss: 7.983708]\n",
            "3451 [D loss: 0.031956, acc.: 100.00%] [G loss: 5.440238]\n",
            "3452 [D loss: 0.032262, acc.: 100.00%] [G loss: 5.418427]\n",
            "3453 [D loss: 0.008610, acc.: 100.00%] [G loss: 8.068779]\n",
            "3454 [D loss: 0.069232, acc.: 98.44%] [G loss: 4.921762]\n",
            "3455 [D loss: 0.020859, acc.: 100.00%] [G loss: 6.509378]\n",
            "3456 [D loss: 0.015995, acc.: 100.00%] [G loss: 6.644338]\n",
            "3457 [D loss: 0.007223, acc.: 100.00%] [G loss: 6.737532]\n",
            "3458 [D loss: 0.083963, acc.: 95.31%] [G loss: 3.603765]\n",
            "3459 [D loss: 0.229337, acc.: 85.94%] [G loss: 13.792247]\n",
            "3460 [D loss: 0.094045, acc.: 98.44%] [G loss: 14.573597]\n",
            "3461 [D loss: 0.167910, acc.: 92.19%] [G loss: 9.445370]\n",
            "3462 [D loss: 0.002521, acc.: 100.00%] [G loss: 6.903362]\n",
            "3463 [D loss: 0.113285, acc.: 95.31%] [G loss: 9.534422]\n",
            "3464 [D loss: 0.000914, acc.: 100.00%] [G loss: 9.136393]\n",
            "3465 [D loss: 0.020184, acc.: 100.00%] [G loss: 8.873782]\n",
            "3466 [D loss: 0.005254, acc.: 100.00%] [G loss: 6.953856]\n",
            "3467 [D loss: 0.007887, acc.: 100.00%] [G loss: 5.195716]\n",
            "3468 [D loss: 0.010365, acc.: 100.00%] [G loss: 6.022307]\n",
            "3469 [D loss: 0.005372, acc.: 100.00%] [G loss: 4.187262]\n",
            "3470 [D loss: 0.186440, acc.: 92.19%] [G loss: 10.791209]\n",
            "3471 [D loss: 0.048289, acc.: 98.44%] [G loss: 12.711685]\n",
            "3472 [D loss: 0.061860, acc.: 98.44%] [G loss: 10.810350]\n",
            "3473 [D loss: 0.003755, acc.: 100.00%] [G loss: 8.679296]\n",
            "3474 [D loss: 0.048578, acc.: 98.44%] [G loss: 6.694689]\n",
            "3475 [D loss: 0.033133, acc.: 100.00%] [G loss: 5.184530]\n",
            "3476 [D loss: 0.004508, acc.: 100.00%] [G loss: 7.885247]\n",
            "3477 [D loss: 0.005930, acc.: 100.00%] [G loss: 7.025196]\n",
            "3478 [D loss: 0.003571, acc.: 100.00%] [G loss: 7.081135]\n",
            "3479 [D loss: 0.026350, acc.: 100.00%] [G loss: 6.726990]\n",
            "3480 [D loss: 0.009534, acc.: 100.00%] [G loss: 6.466666]\n",
            "3481 [D loss: 0.013131, acc.: 100.00%] [G loss: 5.866214]\n",
            "3482 [D loss: 0.073511, acc.: 98.44%] [G loss: 4.152089]\n",
            "3483 [D loss: 0.025199, acc.: 100.00%] [G loss: 7.331139]\n",
            "3484 [D loss: 0.007833, acc.: 100.00%] [G loss: 4.430350]\n",
            "3485 [D loss: 0.077433, acc.: 98.44%] [G loss: 10.311192]\n",
            "3486 [D loss: 0.053310, acc.: 100.00%] [G loss: 7.804391]\n",
            "3487 [D loss: 0.028863, acc.: 100.00%] [G loss: 7.132205]\n",
            "3488 [D loss: 0.032210, acc.: 98.44%] [G loss: 5.175403]\n",
            "3489 [D loss: 0.007678, acc.: 100.00%] [G loss: 7.518230]\n",
            "3490 [D loss: 0.006804, acc.: 100.00%] [G loss: 6.463636]\n",
            "3491 [D loss: 0.022337, acc.: 100.00%] [G loss: 8.468132]\n",
            "3492 [D loss: 0.006049, acc.: 100.00%] [G loss: 8.716909]\n",
            "3493 [D loss: 0.031826, acc.: 100.00%] [G loss: 7.104316]\n",
            "3494 [D loss: 0.011977, acc.: 100.00%] [G loss: 6.876287]\n",
            "3495 [D loss: 0.026657, acc.: 100.00%] [G loss: 6.572115]\n",
            "3496 [D loss: 0.021879, acc.: 100.00%] [G loss: 7.167901]\n",
            "3497 [D loss: 0.012055, acc.: 100.00%] [G loss: 6.370296]\n",
            "3498 [D loss: 0.010245, acc.: 100.00%] [G loss: 6.460361]\n",
            "3499 [D loss: 0.115796, acc.: 96.88%] [G loss: 11.020599]\n",
            "3500 [D loss: 0.007408, acc.: 100.00%] [G loss: 13.324867]\n",
            "3501 [D loss: 0.103954, acc.: 96.88%] [G loss: 10.843090]\n",
            "3502 [D loss: 0.001973, acc.: 100.00%] [G loss: 7.982836]\n",
            "3503 [D loss: 0.011827, acc.: 100.00%] [G loss: 6.367187]\n",
            "3504 [D loss: 0.012475, acc.: 100.00%] [G loss: 5.651175]\n",
            "3505 [D loss: 0.020222, acc.: 100.00%] [G loss: 5.793976]\n",
            "3506 [D loss: 0.013963, acc.: 100.00%] [G loss: 5.805981]\n",
            "3507 [D loss: 0.006589, acc.: 100.00%] [G loss: 4.678255]\n",
            "3508 [D loss: 0.020625, acc.: 100.00%] [G loss: 7.408014]\n",
            "3509 [D loss: 0.001778, acc.: 100.00%] [G loss: 7.901919]\n",
            "3510 [D loss: 0.091037, acc.: 98.44%] [G loss: 8.742262]\n",
            "3511 [D loss: 0.020817, acc.: 100.00%] [G loss: 7.721078]\n",
            "3512 [D loss: 0.018711, acc.: 100.00%] [G loss: 6.800528]\n",
            "3513 [D loss: 0.004106, acc.: 100.00%] [G loss: 7.859698]\n",
            "3514 [D loss: 0.025141, acc.: 100.00%] [G loss: 7.777314]\n",
            "3515 [D loss: 0.035899, acc.: 98.44%] [G loss: 4.883128]\n",
            "3516 [D loss: 0.019983, acc.: 100.00%] [G loss: 4.973619]\n",
            "3517 [D loss: 0.022159, acc.: 100.00%] [G loss: 6.189016]\n",
            "3518 [D loss: 0.047018, acc.: 100.00%] [G loss: 7.310601]\n",
            "3519 [D loss: 0.033689, acc.: 100.00%] [G loss: 6.721682]\n",
            "3520 [D loss: 0.000805, acc.: 100.00%] [G loss: 7.580800]\n",
            "3521 [D loss: 0.041043, acc.: 98.44%] [G loss: 8.694598]\n",
            "3522 [D loss: 0.168815, acc.: 92.19%] [G loss: 4.416823]\n",
            "3523 [D loss: 0.151390, acc.: 93.75%] [G loss: 11.791656]\n",
            "3524 [D loss: 0.007810, acc.: 100.00%] [G loss: 14.038730]\n",
            "3525 [D loss: 0.021142, acc.: 98.44%] [G loss: 12.498369]\n",
            "3526 [D loss: 0.041447, acc.: 100.00%] [G loss: 9.201393]\n",
            "3527 [D loss: 0.000592, acc.: 100.00%] [G loss: 8.501356]\n",
            "3528 [D loss: 0.002063, acc.: 100.00%] [G loss: 6.289334]\n",
            "3529 [D loss: 0.004669, acc.: 100.00%] [G loss: 7.113154]\n",
            "3530 [D loss: 0.012848, acc.: 100.00%] [G loss: 6.142640]\n",
            "3531 [D loss: 0.087299, acc.: 96.88%] [G loss: 8.309420]\n",
            "3532 [D loss: 0.003959, acc.: 100.00%] [G loss: 10.517969]\n",
            "3533 [D loss: 0.050894, acc.: 98.44%] [G loss: 4.684304]\n",
            "3534 [D loss: 0.010416, acc.: 100.00%] [G loss: 6.339045]\n",
            "3535 [D loss: 0.010255, acc.: 100.00%] [G loss: 7.327685]\n",
            "3536 [D loss: 0.017924, acc.: 100.00%] [G loss: 6.703794]\n",
            "3537 [D loss: 0.002845, acc.: 100.00%] [G loss: 8.930977]\n",
            "3538 [D loss: 0.002006, acc.: 100.00%] [G loss: 6.980832]\n",
            "3539 [D loss: 0.017416, acc.: 100.00%] [G loss: 8.279689]\n",
            "3540 [D loss: 0.014982, acc.: 100.00%] [G loss: 5.559968]\n",
            "3541 [D loss: 0.022889, acc.: 100.00%] [G loss: 5.680796]\n",
            "3542 [D loss: 0.060868, acc.: 98.44%] [G loss: 7.911444]\n",
            "3543 [D loss: 0.002281, acc.: 100.00%] [G loss: 9.903833]\n",
            "3544 [D loss: 0.009344, acc.: 100.00%] [G loss: 9.285074]\n",
            "3545 [D loss: 0.053429, acc.: 100.00%] [G loss: 6.631337]\n",
            "3546 [D loss: 0.015353, acc.: 100.00%] [G loss: 5.305037]\n",
            "3547 [D loss: 0.008066, acc.: 100.00%] [G loss: 6.741878]\n",
            "3548 [D loss: 0.006320, acc.: 100.00%] [G loss: 5.829298]\n",
            "3549 [D loss: 0.004724, acc.: 100.00%] [G loss: 6.650160]\n",
            "3550 [D loss: 0.022042, acc.: 100.00%] [G loss: 6.172164]\n",
            "3551 [D loss: 0.008225, acc.: 100.00%] [G loss: 5.940564]\n",
            "3552 [D loss: 0.012902, acc.: 100.00%] [G loss: 5.482676]\n",
            "3553 [D loss: 0.042524, acc.: 98.44%] [G loss: 6.403101]\n",
            "3554 [D loss: 0.022460, acc.: 100.00%] [G loss: 6.716866]\n",
            "3555 [D loss: 0.006765, acc.: 100.00%] [G loss: 5.652078]\n",
            "3556 [D loss: 0.005491, acc.: 100.00%] [G loss: 5.985723]\n",
            "3557 [D loss: 0.012806, acc.: 100.00%] [G loss: 5.382559]\n",
            "3558 [D loss: 0.005898, acc.: 100.00%] [G loss: 3.828847]\n",
            "3559 [D loss: 0.005340, acc.: 100.00%] [G loss: 4.374567]\n",
            "3560 [D loss: 0.220173, acc.: 90.62%] [G loss: 11.602448]\n",
            "3561 [D loss: 0.098408, acc.: 95.31%] [G loss: 10.294288]\n",
            "3562 [D loss: 0.022848, acc.: 98.44%] [G loss: 7.790586]\n",
            "3563 [D loss: 0.000853, acc.: 100.00%] [G loss: 8.304638]\n",
            "3564 [D loss: 0.001612, acc.: 100.00%] [G loss: 8.173396]\n",
            "3565 [D loss: 0.008528, acc.: 100.00%] [G loss: 7.246818]\n",
            "3566 [D loss: 0.005518, acc.: 100.00%] [G loss: 5.633553]\n",
            "3567 [D loss: 0.001340, acc.: 100.00%] [G loss: 5.664107]\n",
            "3568 [D loss: 0.022405, acc.: 100.00%] [G loss: 6.478621]\n",
            "3569 [D loss: 0.003320, acc.: 100.00%] [G loss: 5.382791]\n",
            "3570 [D loss: 0.013399, acc.: 100.00%] [G loss: 4.595567]\n",
            "3571 [D loss: 0.003504, acc.: 100.00%] [G loss: 7.444926]\n",
            "3572 [D loss: 0.001430, acc.: 100.00%] [G loss: 7.075965]\n",
            "3573 [D loss: 0.039080, acc.: 98.44%] [G loss: 7.795031]\n",
            "3574 [D loss: 0.048555, acc.: 98.44%] [G loss: 6.155537]\n",
            "3575 [D loss: 0.020498, acc.: 100.00%] [G loss: 7.002443]\n",
            "3576 [D loss: 0.003804, acc.: 100.00%] [G loss: 7.571419]\n",
            "3577 [D loss: 0.006934, acc.: 100.00%] [G loss: 6.091810]\n",
            "3578 [D loss: 0.029787, acc.: 100.00%] [G loss: 5.917490]\n",
            "3579 [D loss: 0.048318, acc.: 98.44%] [G loss: 7.539227]\n",
            "3580 [D loss: 0.008370, acc.: 100.00%] [G loss: 12.793888]\n",
            "3581 [D loss: 0.014780, acc.: 100.00%] [G loss: 7.697454]\n",
            "3582 [D loss: 0.010562, acc.: 100.00%] [G loss: 6.778418]\n",
            "3583 [D loss: 0.001791, acc.: 100.00%] [G loss: 7.129220]\n",
            "3584 [D loss: 0.010982, acc.: 100.00%] [G loss: 4.340586]\n",
            "3585 [D loss: 0.017076, acc.: 100.00%] [G loss: 7.457729]\n",
            "3586 [D loss: 0.005018, acc.: 100.00%] [G loss: 6.190327]\n",
            "3587 [D loss: 0.004370, acc.: 100.00%] [G loss: 6.230335]\n",
            "3588 [D loss: 0.004435, acc.: 100.00%] [G loss: 6.090322]\n",
            "3589 [D loss: 0.032272, acc.: 100.00%] [G loss: 6.131204]\n",
            "3590 [D loss: 0.031863, acc.: 100.00%] [G loss: 9.629060]\n",
            "3591 [D loss: 0.074407, acc.: 100.00%] [G loss: 6.194558]\n",
            "3592 [D loss: 0.024485, acc.: 100.00%] [G loss: 6.318108]\n",
            "3593 [D loss: 0.003441, acc.: 100.00%] [G loss: 7.209687]\n",
            "3594 [D loss: 0.017389, acc.: 100.00%] [G loss: 6.284313]\n",
            "3595 [D loss: 0.035679, acc.: 100.00%] [G loss: 8.157337]\n",
            "3596 [D loss: 0.002019, acc.: 100.00%] [G loss: 10.263552]\n",
            "3597 [D loss: 0.036501, acc.: 98.44%] [G loss: 7.572412]\n",
            "3598 [D loss: 0.005095, acc.: 100.00%] [G loss: 7.374309]\n",
            "3599 [D loss: 0.000166, acc.: 100.00%] [G loss: 9.161692]\n",
            "3600 [D loss: 0.006106, acc.: 100.00%] [G loss: 6.848739]\n",
            "3601 [D loss: 0.021258, acc.: 100.00%] [G loss: 7.977545]\n",
            "3602 [D loss: 0.003559, acc.: 100.00%] [G loss: 7.433831]\n",
            "3603 [D loss: 0.016120, acc.: 100.00%] [G loss: 6.542489]\n",
            "3604 [D loss: 0.005709, acc.: 100.00%] [G loss: 6.597336]\n",
            "3605 [D loss: 0.071680, acc.: 98.44%] [G loss: 9.100965]\n",
            "3606 [D loss: 0.026483, acc.: 98.44%] [G loss: 10.059696]\n",
            "3607 [D loss: 0.031843, acc.: 100.00%] [G loss: 8.262997]\n",
            "3608 [D loss: 0.008463, acc.: 100.00%] [G loss: 6.767253]\n",
            "3609 [D loss: 0.004943, acc.: 100.00%] [G loss: 8.044333]\n",
            "3610 [D loss: 0.004687, acc.: 100.00%] [G loss: 7.194203]\n",
            "3611 [D loss: 0.003721, acc.: 100.00%] [G loss: 6.626686]\n",
            "3612 [D loss: 0.006269, acc.: 100.00%] [G loss: 6.535937]\n",
            "3613 [D loss: 0.016609, acc.: 100.00%] [G loss: 7.018954]\n",
            "3614 [D loss: 0.001075, acc.: 100.00%] [G loss: 11.331650]\n",
            "3615 [D loss: 0.004623, acc.: 100.00%] [G loss: 6.923425]\n",
            "3616 [D loss: 0.012273, acc.: 100.00%] [G loss: 6.649282]\n",
            "3617 [D loss: 0.014978, acc.: 100.00%] [G loss: 7.427748]\n",
            "3618 [D loss: 0.004990, acc.: 100.00%] [G loss: 7.637439]\n",
            "3619 [D loss: 0.085706, acc.: 96.88%] [G loss: 9.076969]\n",
            "3620 [D loss: 0.066464, acc.: 98.44%] [G loss: 9.916295]\n",
            "3621 [D loss: 0.008257, acc.: 100.00%] [G loss: 7.352673]\n",
            "3622 [D loss: 0.002404, acc.: 100.00%] [G loss: 5.732897]\n",
            "3623 [D loss: 0.004838, acc.: 100.00%] [G loss: 7.324350]\n",
            "3624 [D loss: 0.002160, acc.: 100.00%] [G loss: 5.927295]\n",
            "3625 [D loss: 0.035305, acc.: 98.44%] [G loss: 7.485014]\n",
            "3626 [D loss: 0.003230, acc.: 100.00%] [G loss: 11.137861]\n",
            "3627 [D loss: 0.030800, acc.: 100.00%] [G loss: 8.670101]\n",
            "3628 [D loss: 0.002211, acc.: 100.00%] [G loss: 7.745138]\n",
            "3629 [D loss: 0.001597, acc.: 100.00%] [G loss: 7.423755]\n",
            "3630 [D loss: 0.036092, acc.: 100.00%] [G loss: 8.257096]\n",
            "3631 [D loss: 0.009931, acc.: 100.00%] [G loss: 7.896594]\n",
            "3632 [D loss: 0.002988, acc.: 100.00%] [G loss: 7.227171]\n",
            "3633 [D loss: 0.001385, acc.: 100.00%] [G loss: 9.935394]\n",
            "3634 [D loss: 0.013093, acc.: 100.00%] [G loss: 6.740356]\n",
            "3635 [D loss: 0.009911, acc.: 100.00%] [G loss: 5.614504]\n",
            "3636 [D loss: 0.008450, acc.: 100.00%] [G loss: 7.816984]\n",
            "3637 [D loss: 0.002638, acc.: 100.00%] [G loss: 7.591348]\n",
            "3638 [D loss: 0.008702, acc.: 100.00%] [G loss: 6.447205]\n",
            "3639 [D loss: 0.007460, acc.: 100.00%] [G loss: 4.442913]\n",
            "3640 [D loss: 0.156634, acc.: 92.19%] [G loss: 15.686388]\n",
            "3641 [D loss: 0.870319, acc.: 65.62%] [G loss: 3.393817]\n",
            "3642 [D loss: 1.691290, acc.: 62.50%] [G loss: 16.118095]\n",
            "3643 [D loss: 0.999207, acc.: 65.62%] [G loss: 16.118095]\n",
            "3644 [D loss: 0.000018, acc.: 100.00%] [G loss: 16.118095]\n",
            "3645 [D loss: 0.000047, acc.: 100.00%] [G loss: 16.112604]\n",
            "3646 [D loss: 0.000013, acc.: 100.00%] [G loss: 15.960620]\n",
            "3647 [D loss: 0.000861, acc.: 100.00%] [G loss: 16.076487]\n",
            "3648 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.016996]\n",
            "3649 [D loss: 0.000013, acc.: 100.00%] [G loss: 15.807009]\n",
            "3650 [D loss: 0.000079, acc.: 100.00%] [G loss: 15.911966]\n",
            "3651 [D loss: 0.001199, acc.: 100.00%] [G loss: 15.578700]\n",
            "3652 [D loss: 0.001104, acc.: 100.00%] [G loss: 15.999533]\n",
            "3653 [D loss: 0.000383, acc.: 100.00%] [G loss: 15.212164]\n",
            "3654 [D loss: 0.000794, acc.: 100.00%] [G loss: 14.988964]\n",
            "3655 [D loss: 0.000442, acc.: 100.00%] [G loss: 13.386316]\n",
            "3656 [D loss: 0.015324, acc.: 98.44%] [G loss: 11.350505]\n",
            "3657 [D loss: 0.004509, acc.: 100.00%] [G loss: 8.705019]\n",
            "3658 [D loss: 0.040303, acc.: 98.44%] [G loss: 9.538334]\n",
            "3659 [D loss: 0.002144, acc.: 100.00%] [G loss: 6.520432]\n",
            "3660 [D loss: 0.042610, acc.: 98.44%] [G loss: 5.767486]\n",
            "3661 [D loss: 0.075764, acc.: 98.44%] [G loss: 8.896795]\n",
            "3662 [D loss: 0.005035, acc.: 100.00%] [G loss: 9.170918]\n",
            "3663 [D loss: 0.003479, acc.: 100.00%] [G loss: 8.377550]\n",
            "3664 [D loss: 0.036581, acc.: 100.00%] [G loss: 5.463677]\n",
            "3665 [D loss: 0.312429, acc.: 84.38%] [G loss: 10.605055]\n",
            "3666 [D loss: 0.000425, acc.: 100.00%] [G loss: 13.745076]\n",
            "3667 [D loss: 0.038282, acc.: 98.44%] [G loss: 10.150082]\n",
            "3668 [D loss: 0.000665, acc.: 100.00%] [G loss: 8.563707]\n",
            "3669 [D loss: 0.006802, acc.: 100.00%] [G loss: 6.515261]\n",
            "3670 [D loss: 0.029303, acc.: 100.00%] [G loss: 5.343936]\n",
            "3671 [D loss: 0.046544, acc.: 98.44%] [G loss: 6.080943]\n",
            "3672 [D loss: 0.056181, acc.: 100.00%] [G loss: 5.560815]\n",
            "3673 [D loss: 0.009225, acc.: 100.00%] [G loss: 6.612522]\n",
            "3674 [D loss: 0.001916, acc.: 100.00%] [G loss: 7.340574]\n",
            "3675 [D loss: 0.026136, acc.: 100.00%] [G loss: 5.748685]\n",
            "3676 [D loss: 0.038187, acc.: 100.00%] [G loss: 3.990644]\n",
            "3677 [D loss: 0.157104, acc.: 93.75%] [G loss: 12.196390]\n",
            "3678 [D loss: 0.103029, acc.: 95.31%] [G loss: 10.084214]\n",
            "3679 [D loss: 0.033583, acc.: 98.44%] [G loss: 5.287733]\n",
            "3680 [D loss: 0.007811, acc.: 100.00%] [G loss: 6.359649]\n",
            "3681 [D loss: 0.024745, acc.: 100.00%] [G loss: 6.454948]\n",
            "3682 [D loss: 0.019083, acc.: 100.00%] [G loss: 5.981122]\n",
            "3683 [D loss: 0.052351, acc.: 98.44%] [G loss: 9.048815]\n",
            "3684 [D loss: 0.033128, acc.: 98.44%] [G loss: 6.141500]\n",
            "3685 [D loss: 0.231828, acc.: 95.31%] [G loss: 9.751233]\n",
            "3686 [D loss: 0.014239, acc.: 100.00%] [G loss: 13.385607]\n",
            "3687 [D loss: 0.034675, acc.: 98.44%] [G loss: 10.898531]\n",
            "3688 [D loss: 0.009465, acc.: 100.00%] [G loss: 7.066659]\n",
            "3689 [D loss: 0.016578, acc.: 100.00%] [G loss: 7.242254]\n",
            "3690 [D loss: 0.006768, acc.: 100.00%] [G loss: 7.502763]\n",
            "3691 [D loss: 0.069785, acc.: 96.88%] [G loss: 5.586379]\n",
            "3692 [D loss: 0.360081, acc.: 81.25%] [G loss: 15.817980]\n",
            "3693 [D loss: 0.788915, acc.: 65.62%] [G loss: 7.246983]\n",
            "3694 [D loss: 0.070225, acc.: 96.88%] [G loss: 5.260264]\n",
            "3695 [D loss: 0.053915, acc.: 98.44%] [G loss: 8.171563]\n",
            "3696 [D loss: 0.006179, acc.: 100.00%] [G loss: 8.983923]\n",
            "3697 [D loss: 0.003371, acc.: 100.00%] [G loss: 7.156369]\n",
            "3698 [D loss: 0.011072, acc.: 100.00%] [G loss: 6.207599]\n",
            "3699 [D loss: 0.033965, acc.: 100.00%] [G loss: 3.268207]\n",
            "3700 [D loss: 0.021776, acc.: 100.00%] [G loss: 4.667519]\n",
            "3701 [D loss: 0.046200, acc.: 100.00%] [G loss: 7.613704]\n",
            "3702 [D loss: 0.022538, acc.: 100.00%] [G loss: 6.606409]\n",
            "3703 [D loss: 0.212487, acc.: 89.06%] [G loss: 10.628116]\n",
            "3704 [D loss: 0.021877, acc.: 98.44%] [G loss: 12.935730]\n",
            "3705 [D loss: 0.014341, acc.: 98.44%] [G loss: 8.530767]\n",
            "3706 [D loss: 0.003552, acc.: 100.00%] [G loss: 7.930298]\n",
            "3707 [D loss: 0.003186, acc.: 100.00%] [G loss: 5.957203]\n",
            "3708 [D loss: 0.017384, acc.: 100.00%] [G loss: 6.753596]\n",
            "3709 [D loss: 0.185940, acc.: 89.06%] [G loss: 12.304921]\n",
            "3710 [D loss: 0.067282, acc.: 96.88%] [G loss: 12.391647]\n",
            "3711 [D loss: 0.026803, acc.: 100.00%] [G loss: 10.113993]\n",
            "3712 [D loss: 0.003210, acc.: 100.00%] [G loss: 9.768515]\n",
            "3713 [D loss: 0.014636, acc.: 100.00%] [G loss: 5.892920]\n",
            "3714 [D loss: 0.025275, acc.: 100.00%] [G loss: 4.852999]\n",
            "3715 [D loss: 0.008297, acc.: 100.00%] [G loss: 5.247931]\n",
            "3716 [D loss: 0.005246, acc.: 100.00%] [G loss: 3.702882]\n",
            "3717 [D loss: 0.026206, acc.: 98.44%] [G loss: 5.488564]\n",
            "3718 [D loss: 0.009603, acc.: 100.00%] [G loss: 4.978197]\n",
            "3719 [D loss: 0.056745, acc.: 100.00%] [G loss: 6.231976]\n",
            "3720 [D loss: 0.101658, acc.: 98.44%] [G loss: 8.563129]\n",
            "3721 [D loss: 0.121012, acc.: 96.88%] [G loss: 7.057480]\n",
            "3722 [D loss: 0.035042, acc.: 98.44%] [G loss: 8.188918]\n",
            "3723 [D loss: 0.004186, acc.: 100.00%] [G loss: 8.560259]\n",
            "3724 [D loss: 0.000766, acc.: 100.00%] [G loss: 6.051512]\n",
            "3725 [D loss: 0.011429, acc.: 100.00%] [G loss: 6.663567]\n",
            "3726 [D loss: 0.002471, acc.: 100.00%] [G loss: 7.340989]\n",
            "3727 [D loss: 0.007225, acc.: 100.00%] [G loss: 7.194335]\n",
            "3728 [D loss: 0.015739, acc.: 100.00%] [G loss: 5.057493]\n",
            "3729 [D loss: 0.122344, acc.: 93.75%] [G loss: 13.388552]\n",
            "3730 [D loss: 0.015729, acc.: 100.00%] [G loss: 15.685698]\n",
            "3731 [D loss: 0.342535, acc.: 82.81%] [G loss: 3.363976]\n",
            "3732 [D loss: 0.268080, acc.: 85.94%] [G loss: 12.936818]\n",
            "3733 [D loss: 0.042649, acc.: 98.44%] [G loss: 14.593885]\n",
            "3734 [D loss: 0.003333, acc.: 100.00%] [G loss: 14.306873]\n",
            "3735 [D loss: 0.336123, acc.: 84.38%] [G loss: 6.858248]\n",
            "3736 [D loss: 0.002944, acc.: 100.00%] [G loss: 5.252290]\n",
            "3737 [D loss: 0.394900, acc.: 87.50%] [G loss: 13.442723]\n",
            "3738 [D loss: 0.000017, acc.: 100.00%] [G loss: 15.026175]\n",
            "3739 [D loss: 0.005946, acc.: 100.00%] [G loss: 14.178501]\n",
            "3740 [D loss: 0.031391, acc.: 98.44%] [G loss: 12.607725]\n",
            "3741 [D loss: 0.002427, acc.: 100.00%] [G loss: 10.578606]\n",
            "3742 [D loss: 0.000531, acc.: 100.00%] [G loss: 9.256292]\n",
            "3743 [D loss: 0.000969, acc.: 100.00%] [G loss: 8.344059]\n",
            "3744 [D loss: 0.007402, acc.: 100.00%] [G loss: 5.711478]\n",
            "3745 [D loss: 0.010346, acc.: 100.00%] [G loss: 7.424979]\n",
            "3746 [D loss: 0.051023, acc.: 98.44%] [G loss: 9.417435]\n",
            "3747 [D loss: 0.012667, acc.: 100.00%] [G loss: 9.053242]\n",
            "3748 [D loss: 0.002392, acc.: 100.00%] [G loss: 6.292396]\n",
            "3749 [D loss: 0.006814, acc.: 100.00%] [G loss: 8.135902]\n",
            "3750 [D loss: 1.309467, acc.: 51.56%] [G loss: 16.118095]\n",
            "3751 [D loss: 2.848607, acc.: 50.00%] [G loss: 12.718711]\n",
            "3752 [D loss: 0.003533, acc.: 100.00%] [G loss: 9.295696]\n",
            "3753 [D loss: 0.003420, acc.: 100.00%] [G loss: 6.169541]\n",
            "3754 [D loss: 0.519404, acc.: 76.56%] [G loss: 13.222294]\n",
            "3755 [D loss: 0.123909, acc.: 93.75%] [G loss: 15.137730]\n",
            "3756 [D loss: 0.166395, acc.: 95.31%] [G loss: 12.521111]\n",
            "3757 [D loss: 0.001039, acc.: 100.00%] [G loss: 9.633614]\n",
            "3758 [D loss: 0.001583, acc.: 100.00%] [G loss: 8.894566]\n",
            "3759 [D loss: 0.001003, acc.: 100.00%] [G loss: 8.480880]\n",
            "3760 [D loss: 0.038157, acc.: 100.00%] [G loss: 4.290620]\n",
            "3761 [D loss: 0.004119, acc.: 100.00%] [G loss: 5.842386]\n",
            "3762 [D loss: 0.070376, acc.: 98.44%] [G loss: 7.309758]\n",
            "3763 [D loss: 0.020027, acc.: 100.00%] [G loss: 5.216396]\n",
            "3764 [D loss: 0.042171, acc.: 98.44%] [G loss: 6.635123]\n",
            "3765 [D loss: 0.072764, acc.: 98.44%] [G loss: 7.863527]\n",
            "3766 [D loss: 0.015066, acc.: 100.00%] [G loss: 6.434266]\n",
            "3767 [D loss: 0.115016, acc.: 96.88%] [G loss: 5.909489]\n",
            "3768 [D loss: 0.221210, acc.: 89.06%] [G loss: 9.092793]\n",
            "3769 [D loss: 0.004358, acc.: 100.00%] [G loss: 12.682343]\n",
            "3770 [D loss: 0.069297, acc.: 98.44%] [G loss: 8.727514]\n",
            "3771 [D loss: 0.035078, acc.: 100.00%] [G loss: 6.641273]\n",
            "3772 [D loss: 0.018514, acc.: 100.00%] [G loss: 8.644562]\n",
            "3773 [D loss: 0.020182, acc.: 100.00%] [G loss: 6.161033]\n",
            "3774 [D loss: 0.009755, acc.: 100.00%] [G loss: 5.711277]\n",
            "3775 [D loss: 0.006396, acc.: 100.00%] [G loss: 5.238280]\n",
            "3776 [D loss: 0.149544, acc.: 96.88%] [G loss: 4.545567]\n",
            "3777 [D loss: 0.022011, acc.: 100.00%] [G loss: 5.313467]\n",
            "3778 [D loss: 0.011086, acc.: 100.00%] [G loss: 5.395406]\n",
            "3779 [D loss: 2.137612, acc.: 34.38%] [G loss: 16.118095]\n",
            "3780 [D loss: 0.273235, acc.: 89.06%] [G loss: 16.118095]\n",
            "3781 [D loss: 0.527367, acc.: 75.00%] [G loss: 16.063816]\n",
            "3782 [D loss: 0.000942, acc.: 100.00%] [G loss: 15.957226]\n",
            "3783 [D loss: 0.000038, acc.: 100.00%] [G loss: 15.400742]\n",
            "3784 [D loss: 0.000082, acc.: 100.00%] [G loss: 14.781930]\n",
            "3785 [D loss: 0.000177, acc.: 100.00%] [G loss: 12.115763]\n",
            "3786 [D loss: 0.000379, acc.: 100.00%] [G loss: 10.308380]\n",
            "3787 [D loss: 0.000211, acc.: 100.00%] [G loss: 9.405302]\n",
            "3788 [D loss: 0.003834, acc.: 100.00%] [G loss: 6.516536]\n",
            "3789 [D loss: 0.014641, acc.: 100.00%] [G loss: 6.098603]\n",
            "3790 [D loss: 0.006050, acc.: 100.00%] [G loss: 4.491804]\n",
            "3791 [D loss: 0.017249, acc.: 98.44%] [G loss: 6.266919]\n",
            "3792 [D loss: 0.002596, acc.: 100.00%] [G loss: 5.662530]\n",
            "3793 [D loss: 0.063473, acc.: 100.00%] [G loss: 6.792983]\n",
            "3794 [D loss: 0.009734, acc.: 100.00%] [G loss: 6.460064]\n",
            "3795 [D loss: 0.004399, acc.: 100.00%] [G loss: 8.312842]\n",
            "3796 [D loss: 0.131778, acc.: 93.75%] [G loss: 9.917074]\n",
            "3797 [D loss: 0.022779, acc.: 100.00%] [G loss: 11.771758]\n",
            "3798 [D loss: 0.055354, acc.: 98.44%] [G loss: 11.215117]\n",
            "3799 [D loss: 0.028487, acc.: 98.44%] [G loss: 7.538948]\n",
            "3800 [D loss: 0.002890, acc.: 100.00%] [G loss: 6.336928]\n",
            "3801 [D loss: 0.005183, acc.: 100.00%] [G loss: 6.226912]\n",
            "3802 [D loss: 0.008930, acc.: 100.00%] [G loss: 5.985538]\n",
            "3803 [D loss: 0.034363, acc.: 100.00%] [G loss: 5.578493]\n",
            "3804 [D loss: 0.090593, acc.: 96.88%] [G loss: 3.594793]\n",
            "3805 [D loss: 0.009330, acc.: 100.00%] [G loss: 3.013726]\n",
            "3806 [D loss: 0.040121, acc.: 98.44%] [G loss: 5.023135]\n",
            "3807 [D loss: 0.018886, acc.: 100.00%] [G loss: 5.296946]\n",
            "3808 [D loss: 0.022474, acc.: 100.00%] [G loss: 5.780217]\n",
            "3809 [D loss: 0.052429, acc.: 98.44%] [G loss: 7.363485]\n",
            "3810 [D loss: 0.003636, acc.: 100.00%] [G loss: 6.955773]\n",
            "3811 [D loss: 0.028681, acc.: 100.00%] [G loss: 6.043294]\n",
            "3812 [D loss: 1.316293, acc.: 45.31%] [G loss: 16.061897]\n",
            "3813 [D loss: 0.041126, acc.: 98.44%] [G loss: 16.118095]\n",
            "3814 [D loss: 0.843163, acc.: 64.06%] [G loss: 15.793381]\n",
            "3815 [D loss: 0.000011, acc.: 100.00%] [G loss: 14.727150]\n",
            "3816 [D loss: 0.000053, acc.: 100.00%] [G loss: 14.143784]\n",
            "3817 [D loss: 0.014317, acc.: 100.00%] [G loss: 12.519508]\n",
            "3818 [D loss: 0.000090, acc.: 100.00%] [G loss: 11.639710]\n",
            "3819 [D loss: 0.000458, acc.: 100.00%] [G loss: 8.729986]\n",
            "3820 [D loss: 0.008121, acc.: 100.00%] [G loss: 7.461685]\n",
            "3821 [D loss: 0.002501, acc.: 100.00%] [G loss: 9.000019]\n",
            "3822 [D loss: 0.033499, acc.: 98.44%] [G loss: 4.191447]\n",
            "3823 [D loss: 0.469820, acc.: 81.25%] [G loss: 10.114552]\n",
            "3824 [D loss: 0.065125, acc.: 98.44%] [G loss: 13.811119]\n",
            "3825 [D loss: 0.501263, acc.: 75.00%] [G loss: 6.637376]\n",
            "3826 [D loss: 0.232423, acc.: 89.06%] [G loss: 6.547236]\n",
            "3827 [D loss: 0.039095, acc.: 100.00%] [G loss: 7.304185]\n",
            "3828 [D loss: 0.005691, acc.: 100.00%] [G loss: 6.924387]\n",
            "3829 [D loss: 0.026905, acc.: 100.00%] [G loss: 5.724525]\n",
            "3830 [D loss: 0.021826, acc.: 100.00%] [G loss: 4.581770]\n",
            "3831 [D loss: 0.255484, acc.: 87.50%] [G loss: 9.438551]\n",
            "3832 [D loss: 0.011160, acc.: 100.00%] [G loss: 12.195240]\n",
            "3833 [D loss: 0.093214, acc.: 95.31%] [G loss: 9.832916]\n",
            "3834 [D loss: 0.087558, acc.: 98.44%] [G loss: 6.667797]\n",
            "3835 [D loss: 0.012633, acc.: 100.00%] [G loss: 5.816082]\n",
            "3836 [D loss: 0.053768, acc.: 100.00%] [G loss: 4.633836]\n",
            "3837 [D loss: 0.527285, acc.: 79.69%] [G loss: 8.649509]\n",
            "3838 [D loss: 0.047195, acc.: 98.44%] [G loss: 10.145640]\n",
            "3839 [D loss: 0.006356, acc.: 100.00%] [G loss: 10.319275]\n",
            "3840 [D loss: 0.047469, acc.: 98.44%] [G loss: 8.167416]\n",
            "3841 [D loss: 0.025000, acc.: 100.00%] [G loss: 4.977652]\n",
            "3842 [D loss: 0.110773, acc.: 96.88%] [G loss: 6.200727]\n",
            "3843 [D loss: 0.004734, acc.: 100.00%] [G loss: 7.246432]\n",
            "3844 [D loss: 2.248403, acc.: 15.62%] [G loss: 12.710465]\n",
            "3845 [D loss: 0.000812, acc.: 100.00%] [G loss: 15.124712]\n",
            "3846 [D loss: 0.020627, acc.: 100.00%] [G loss: 15.514748]\n",
            "3847 [D loss: 0.069035, acc.: 96.88%] [G loss: 12.982301]\n",
            "3848 [D loss: 0.015059, acc.: 100.00%] [G loss: 11.834175]\n",
            "3849 [D loss: 0.001234, acc.: 100.00%] [G loss: 10.749843]\n",
            "3850 [D loss: 0.033350, acc.: 98.44%] [G loss: 8.493149]\n",
            "3851 [D loss: 0.011224, acc.: 100.00%] [G loss: 6.727078]\n",
            "3852 [D loss: 0.038830, acc.: 98.44%] [G loss: 5.323890]\n",
            "3853 [D loss: 0.015312, acc.: 100.00%] [G loss: 5.664199]\n",
            "3854 [D loss: 0.012415, acc.: 100.00%] [G loss: 6.324751]\n",
            "3855 [D loss: 0.178501, acc.: 93.75%] [G loss: 9.651093]\n",
            "3856 [D loss: 0.033454, acc.: 100.00%] [G loss: 10.007030]\n",
            "3857 [D loss: 0.199948, acc.: 93.75%] [G loss: 5.909021]\n",
            "3858 [D loss: 0.042924, acc.: 98.44%] [G loss: 6.582638]\n",
            "3859 [D loss: 0.019427, acc.: 100.00%] [G loss: 5.686505]\n",
            "3860 [D loss: 0.040850, acc.: 100.00%] [G loss: 6.614578]\n",
            "3861 [D loss: 0.004875, acc.: 100.00%] [G loss: 7.106934]\n",
            "3862 [D loss: 0.007387, acc.: 100.00%] [G loss: 5.907856]\n",
            "3863 [D loss: 0.039746, acc.: 98.44%] [G loss: 4.474115]\n",
            "3864 [D loss: 0.348857, acc.: 84.38%] [G loss: 4.769776]\n",
            "3865 [D loss: 0.167876, acc.: 90.62%] [G loss: 8.539762]\n",
            "3866 [D loss: 0.039806, acc.: 100.00%] [G loss: 12.394510]\n",
            "3867 [D loss: 0.009640, acc.: 100.00%] [G loss: 10.257179]\n",
            "3868 [D loss: 0.116638, acc.: 95.31%] [G loss: 8.073250]\n",
            "3869 [D loss: 0.009557, acc.: 100.00%] [G loss: 5.836243]\n",
            "3870 [D loss: 0.018878, acc.: 100.00%] [G loss: 4.537936]\n",
            "3871 [D loss: 0.050035, acc.: 98.44%] [G loss: 5.817191]\n",
            "3872 [D loss: 0.000425, acc.: 100.00%] [G loss: 8.975334]\n",
            "3873 [D loss: 0.139243, acc.: 93.75%] [G loss: 3.933551]\n",
            "3874 [D loss: 0.219916, acc.: 90.62%] [G loss: 10.189613]\n",
            "3875 [D loss: 0.001992, acc.: 100.00%] [G loss: 14.712191]\n",
            "3876 [D loss: 0.050465, acc.: 96.88%] [G loss: 10.592012]\n",
            "3877 [D loss: 0.006687, acc.: 100.00%] [G loss: 11.860582]\n",
            "3878 [D loss: 0.003021, acc.: 100.00%] [G loss: 9.949530]\n",
            "3879 [D loss: 0.003807, acc.: 100.00%] [G loss: 9.799733]\n",
            "3880 [D loss: 0.011282, acc.: 100.00%] [G loss: 8.976896]\n",
            "3881 [D loss: 0.050722, acc.: 98.44%] [G loss: 6.180239]\n",
            "3882 [D loss: 0.023211, acc.: 100.00%] [G loss: 5.818701]\n",
            "3883 [D loss: 0.121387, acc.: 93.75%] [G loss: 9.394690]\n",
            "3884 [D loss: 0.002151, acc.: 100.00%] [G loss: 11.707108]\n",
            "3885 [D loss: 0.042515, acc.: 100.00%] [G loss: 9.600679]\n",
            "3886 [D loss: 0.321894, acc.: 81.25%] [G loss: 3.442901]\n",
            "3887 [D loss: 0.469478, acc.: 76.56%] [G loss: 14.247387]\n",
            "3888 [D loss: 0.006864, acc.: 100.00%] [G loss: 16.056431]\n",
            "3889 [D loss: 1.123849, acc.: 64.06%] [G loss: 7.715844]\n",
            "3890 [D loss: 0.016503, acc.: 100.00%] [G loss: 6.403291]\n",
            "3891 [D loss: 0.160437, acc.: 95.31%] [G loss: 7.946632]\n",
            "3892 [D loss: 0.002584, acc.: 100.00%] [G loss: 8.026575]\n",
            "3893 [D loss: 0.007322, acc.: 100.00%] [G loss: 6.367773]\n",
            "3894 [D loss: 0.019391, acc.: 100.00%] [G loss: 6.875708]\n",
            "3895 [D loss: 0.002813, acc.: 100.00%] [G loss: 6.766933]\n",
            "3896 [D loss: 0.036655, acc.: 100.00%] [G loss: 6.371984]\n",
            "3897 [D loss: 0.006433, acc.: 100.00%] [G loss: 7.245005]\n",
            "3898 [D loss: 0.028705, acc.: 100.00%] [G loss: 5.689046]\n",
            "3899 [D loss: 0.001383, acc.: 100.00%] [G loss: 6.370160]\n",
            "3900 [D loss: 0.009555, acc.: 100.00%] [G loss: 5.670731]\n",
            "3901 [D loss: 0.014480, acc.: 100.00%] [G loss: 5.021605]\n",
            "3902 [D loss: 0.052709, acc.: 96.88%] [G loss: 6.436120]\n",
            "3903 [D loss: 0.012986, acc.: 100.00%] [G loss: 5.097216]\n",
            "3904 [D loss: 0.030442, acc.: 100.00%] [G loss: 5.125463]\n",
            "3905 [D loss: 0.027213, acc.: 100.00%] [G loss: 5.878004]\n",
            "3906 [D loss: 0.132084, acc.: 96.88%] [G loss: 4.754069]\n",
            "3907 [D loss: 0.041268, acc.: 100.00%] [G loss: 6.988103]\n",
            "3908 [D loss: 0.046289, acc.: 98.44%] [G loss: 6.894584]\n",
            "3909 [D loss: 0.140710, acc.: 93.75%] [G loss: 9.136430]\n",
            "3910 [D loss: 0.082049, acc.: 98.44%] [G loss: 8.690525]\n",
            "3911 [D loss: 0.006982, acc.: 100.00%] [G loss: 7.479963]\n",
            "3912 [D loss: 0.015544, acc.: 100.00%] [G loss: 5.599157]\n",
            "3913 [D loss: 0.005994, acc.: 100.00%] [G loss: 6.829622]\n",
            "3914 [D loss: 0.015609, acc.: 100.00%] [G loss: 6.837530]\n",
            "3915 [D loss: 0.005130, acc.: 100.00%] [G loss: 6.794099]\n",
            "3916 [D loss: 0.059122, acc.: 98.44%] [G loss: 6.808912]\n",
            "3917 [D loss: 0.022934, acc.: 100.00%] [G loss: 5.006442]\n",
            "3918 [D loss: 0.038573, acc.: 100.00%] [G loss: 6.958824]\n",
            "3919 [D loss: 0.027412, acc.: 100.00%] [G loss: 5.387575]\n",
            "3920 [D loss: 0.081438, acc.: 96.88%] [G loss: 8.779346]\n",
            "3921 [D loss: 0.025722, acc.: 100.00%] [G loss: 10.764059]\n",
            "3922 [D loss: 0.022490, acc.: 100.00%] [G loss: 9.669098]\n",
            "3923 [D loss: 0.013744, acc.: 100.00%] [G loss: 5.741652]\n",
            "3924 [D loss: 0.036996, acc.: 100.00%] [G loss: 4.898428]\n",
            "3925 [D loss: 0.017654, acc.: 100.00%] [G loss: 5.939567]\n",
            "3926 [D loss: 0.041791, acc.: 100.00%] [G loss: 8.165135]\n",
            "3927 [D loss: 0.014007, acc.: 100.00%] [G loss: 8.175179]\n",
            "3928 [D loss: 0.178512, acc.: 93.75%] [G loss: 4.423511]\n",
            "3929 [D loss: 0.228231, acc.: 90.62%] [G loss: 10.556667]\n",
            "3930 [D loss: 0.003863, acc.: 100.00%] [G loss: 15.398935]\n",
            "3931 [D loss: 0.174935, acc.: 90.62%] [G loss: 12.021772]\n",
            "3932 [D loss: 0.010498, acc.: 100.00%] [G loss: 9.039541]\n",
            "3933 [D loss: 0.000620, acc.: 100.00%] [G loss: 11.095239]\n",
            "3934 [D loss: 0.001102, acc.: 100.00%] [G loss: 7.370178]\n",
            "3935 [D loss: 0.027600, acc.: 98.44%] [G loss: 7.386411]\n",
            "3936 [D loss: 0.002452, acc.: 100.00%] [G loss: 6.389789]\n",
            "3937 [D loss: 0.000931, acc.: 100.00%] [G loss: 7.612533]\n",
            "3938 [D loss: 0.066981, acc.: 98.44%] [G loss: 5.046833]\n",
            "3939 [D loss: 0.029451, acc.: 100.00%] [G loss: 9.314220]\n",
            "3940 [D loss: 0.016714, acc.: 100.00%] [G loss: 5.714262]\n",
            "3941 [D loss: 0.019448, acc.: 100.00%] [G loss: 7.344306]\n",
            "3942 [D loss: 0.007807, acc.: 100.00%] [G loss: 8.941576]\n",
            "3943 [D loss: 0.009193, acc.: 100.00%] [G loss: 6.580695]\n",
            "3944 [D loss: 0.007019, acc.: 100.00%] [G loss: 5.048171]\n",
            "3945 [D loss: 0.058707, acc.: 100.00%] [G loss: 7.752653]\n",
            "3946 [D loss: 0.005880, acc.: 100.00%] [G loss: 6.909878]\n",
            "3947 [D loss: 0.006804, acc.: 100.00%] [G loss: 9.045104]\n",
            "3948 [D loss: 0.165508, acc.: 93.75%] [G loss: 3.373594]\n",
            "3949 [D loss: 0.381564, acc.: 75.00%] [G loss: 16.105631]\n",
            "3950 [D loss: 0.325488, acc.: 84.38%] [G loss: 15.749935]\n",
            "3951 [D loss: 0.005546, acc.: 100.00%] [G loss: 15.630550]\n",
            "3952 [D loss: 0.007001, acc.: 100.00%] [G loss: 14.851311]\n",
            "3953 [D loss: 0.000445, acc.: 100.00%] [G loss: 15.190228]\n",
            "3954 [D loss: 0.001637, acc.: 100.00%] [G loss: 13.440660]\n",
            "3955 [D loss: 0.000464, acc.: 100.00%] [G loss: 9.923193]\n",
            "3956 [D loss: 0.000300, acc.: 100.00%] [G loss: 10.670893]\n",
            "3957 [D loss: 0.000655, acc.: 100.00%] [G loss: 10.218730]\n",
            "3958 [D loss: 0.001821, acc.: 100.00%] [G loss: 7.972229]\n",
            "3959 [D loss: 0.001444, acc.: 100.00%] [G loss: 7.412036]\n",
            "3960 [D loss: 0.002657, acc.: 100.00%] [G loss: 6.623215]\n",
            "3961 [D loss: 0.046838, acc.: 98.44%] [G loss: 7.322761]\n",
            "3962 [D loss: 0.002190, acc.: 100.00%] [G loss: 6.748698]\n",
            "3963 [D loss: 0.004635, acc.: 100.00%] [G loss: 5.859058]\n",
            "3964 [D loss: 0.010667, acc.: 100.00%] [G loss: 5.610228]\n",
            "3965 [D loss: 0.042249, acc.: 100.00%] [G loss: 4.387958]\n",
            "3966 [D loss: 0.224597, acc.: 90.62%] [G loss: 11.788615]\n",
            "3967 [D loss: 0.082980, acc.: 96.88%] [G loss: 10.705977]\n",
            "3968 [D loss: 0.021019, acc.: 100.00%] [G loss: 12.597010]\n",
            "3969 [D loss: 0.002620, acc.: 100.00%] [G loss: 9.401037]\n",
            "3970 [D loss: 0.003184, acc.: 100.00%] [G loss: 7.431928]\n",
            "3971 [D loss: 0.004247, acc.: 100.00%] [G loss: 7.143909]\n",
            "3972 [D loss: 0.010502, acc.: 100.00%] [G loss: 7.356693]\n",
            "3973 [D loss: 0.083159, acc.: 100.00%] [G loss: 5.017570]\n",
            "3974 [D loss: 0.011387, acc.: 100.00%] [G loss: 6.254609]\n",
            "3975 [D loss: 0.031316, acc.: 100.00%] [G loss: 4.632738]\n",
            "3976 [D loss: 0.004621, acc.: 100.00%] [G loss: 6.017676]\n",
            "3977 [D loss: 0.010829, acc.: 100.00%] [G loss: 3.958296]\n",
            "3978 [D loss: 0.026445, acc.: 98.44%] [G loss: 5.985160]\n",
            "3979 [D loss: 0.013467, acc.: 100.00%] [G loss: 4.047250]\n",
            "3980 [D loss: 0.023948, acc.: 100.00%] [G loss: 6.116321]\n",
            "3981 [D loss: 0.026530, acc.: 100.00%] [G loss: 5.799445]\n",
            "3982 [D loss: 0.002884, acc.: 100.00%] [G loss: 6.778392]\n",
            "3983 [D loss: 0.032904, acc.: 100.00%] [G loss: 5.645108]\n",
            "3984 [D loss: 0.008637, acc.: 100.00%] [G loss: 5.875412]\n",
            "3985 [D loss: 0.677712, acc.: 67.19%] [G loss: 16.107113]\n",
            "3986 [D loss: 0.479131, acc.: 76.56%] [G loss: 14.751472]\n",
            "3987 [D loss: 0.000326, acc.: 100.00%] [G loss: 10.620810]\n",
            "3988 [D loss: 0.001382, acc.: 100.00%] [G loss: 9.276699]\n",
            "3989 [D loss: 0.002615, acc.: 100.00%] [G loss: 7.315077]\n",
            "3990 [D loss: 0.014745, acc.: 100.00%] [G loss: 6.841530]\n",
            "3991 [D loss: 0.040217, acc.: 100.00%] [G loss: 4.994348]\n",
            "3992 [D loss: 0.000470, acc.: 100.00%] [G loss: 8.849289]\n",
            "3993 [D loss: 0.001378, acc.: 100.00%] [G loss: 7.850903]\n",
            "3994 [D loss: 0.049593, acc.: 98.44%] [G loss: 6.001570]\n",
            "3995 [D loss: 0.036232, acc.: 96.88%] [G loss: 6.454551]\n",
            "3996 [D loss: 0.026255, acc.: 100.00%] [G loss: 6.153617]\n",
            "3997 [D loss: 0.226986, acc.: 87.50%] [G loss: 2.893708]\n",
            "3998 [D loss: 0.497121, acc.: 76.56%] [G loss: 14.251614]\n",
            "3999 [D loss: 0.004197, acc.: 100.00%] [G loss: 16.112604]\n",
            "4000 [D loss: 0.307464, acc.: 85.94%] [G loss: 14.716071]\n",
            "4001 [D loss: 0.014066, acc.: 100.00%] [G loss: 13.789996]\n",
            "4002 [D loss: 0.000712, acc.: 100.00%] [G loss: 11.792213]\n",
            "4003 [D loss: 0.001708, acc.: 100.00%] [G loss: 9.989185]\n",
            "4004 [D loss: 0.003495, acc.: 100.00%] [G loss: 7.796524]\n",
            "4005 [D loss: 0.001036, acc.: 100.00%] [G loss: 7.732065]\n",
            "4006 [D loss: 0.001687, acc.: 100.00%] [G loss: 6.197613]\n",
            "4007 [D loss: 0.016334, acc.: 100.00%] [G loss: 4.935491]\n",
            "4008 [D loss: 0.020060, acc.: 100.00%] [G loss: 4.771991]\n",
            "4009 [D loss: 0.007864, acc.: 100.00%] [G loss: 5.836964]\n",
            "4010 [D loss: 0.002143, acc.: 100.00%] [G loss: 5.524100]\n",
            "4011 [D loss: 0.006967, acc.: 100.00%] [G loss: 5.581400]\n",
            "4012 [D loss: 0.096484, acc.: 98.44%] [G loss: 7.538332]\n",
            "4013 [D loss: 0.011211, acc.: 100.00%] [G loss: 11.256077]\n",
            "4014 [D loss: 0.100892, acc.: 96.88%] [G loss: 5.389318]\n",
            "4015 [D loss: 0.011769, acc.: 100.00%] [G loss: 3.393510]\n",
            "4016 [D loss: 0.036594, acc.: 98.44%] [G loss: 6.120435]\n",
            "4017 [D loss: 0.024161, acc.: 100.00%] [G loss: 5.032264]\n",
            "4018 [D loss: 0.001954, acc.: 100.00%] [G loss: 8.304918]\n",
            "4019 [D loss: 0.005246, acc.: 100.00%] [G loss: 9.757456]\n",
            "4020 [D loss: 0.091174, acc.: 95.31%] [G loss: 4.112090]\n",
            "4021 [D loss: 0.730837, acc.: 65.62%] [G loss: 16.118095]\n",
            "4022 [D loss: 0.793842, acc.: 60.94%] [G loss: 14.753183]\n",
            "4023 [D loss: 0.000223, acc.: 100.00%] [G loss: 14.508690]\n",
            "4024 [D loss: 0.001583, acc.: 100.00%] [G loss: 14.257474]\n",
            "4025 [D loss: 0.000352, acc.: 100.00%] [G loss: 13.784617]\n",
            "4026 [D loss: 0.000408, acc.: 100.00%] [G loss: 11.020573]\n",
            "4027 [D loss: 0.012188, acc.: 100.00%] [G loss: 9.940861]\n",
            "4028 [D loss: 0.003707, acc.: 100.00%] [G loss: 8.895920]\n",
            "4029 [D loss: 0.010636, acc.: 100.00%] [G loss: 9.232712]\n",
            "4030 [D loss: 0.050541, acc.: 98.44%] [G loss: 6.022231]\n",
            "4031 [D loss: 0.003051, acc.: 100.00%] [G loss: 4.856559]\n",
            "4032 [D loss: 0.017279, acc.: 100.00%] [G loss: 4.885715]\n",
            "4033 [D loss: 0.074927, acc.: 98.44%] [G loss: 8.492273]\n",
            "4034 [D loss: 0.006519, acc.: 100.00%] [G loss: 9.164935]\n",
            "4035 [D loss: 0.007552, acc.: 100.00%] [G loss: 7.851124]\n",
            "4036 [D loss: 0.016298, acc.: 100.00%] [G loss: 6.352591]\n",
            "4037 [D loss: 0.035661, acc.: 100.00%] [G loss: 5.418459]\n",
            "4038 [D loss: 0.009802, acc.: 100.00%] [G loss: 6.107337]\n",
            "4039 [D loss: 0.136341, acc.: 96.88%] [G loss: 8.295106]\n",
            "4040 [D loss: 0.012927, acc.: 100.00%] [G loss: 8.049040]\n",
            "4041 [D loss: 0.008706, acc.: 100.00%] [G loss: 6.268083]\n",
            "4042 [D loss: 0.375819, acc.: 87.50%] [G loss: 10.061976]\n",
            "4043 [D loss: 0.000614, acc.: 100.00%] [G loss: 11.836197]\n",
            "4044 [D loss: 0.013762, acc.: 100.00%] [G loss: 11.772089]\n",
            "4045 [D loss: 0.000135, acc.: 100.00%] [G loss: 11.121597]\n",
            "4046 [D loss: 0.000492, acc.: 100.00%] [G loss: 9.471312]\n",
            "4047 [D loss: 0.004014, acc.: 100.00%] [G loss: 8.346841]\n",
            "4048 [D loss: 0.001969, acc.: 100.00%] [G loss: 8.493656]\n",
            "4049 [D loss: 0.014755, acc.: 100.00%] [G loss: 3.782770]\n",
            "4050 [D loss: 0.006472, acc.: 100.00%] [G loss: 9.252228]\n",
            "4051 [D loss: 0.016130, acc.: 100.00%] [G loss: 4.593949]\n",
            "4052 [D loss: 0.049857, acc.: 98.44%] [G loss: 7.956716]\n",
            "4053 [D loss: 0.001572, acc.: 100.00%] [G loss: 9.625050]\n",
            "4054 [D loss: 0.019254, acc.: 100.00%] [G loss: 6.340387]\n",
            "4055 [D loss: 0.014274, acc.: 100.00%] [G loss: 4.877403]\n",
            "4056 [D loss: 0.036705, acc.: 100.00%] [G loss: 4.753558]\n",
            "4057 [D loss: 0.015752, acc.: 100.00%] [G loss: 6.114873]\n",
            "4058 [D loss: 0.117544, acc.: 96.88%] [G loss: 12.709606]\n",
            "4059 [D loss: 0.171225, acc.: 93.75%] [G loss: 9.605277]\n",
            "4060 [D loss: 0.002842, acc.: 100.00%] [G loss: 7.313285]\n",
            "4061 [D loss: 0.003395, acc.: 100.00%] [G loss: 6.260778]\n",
            "4062 [D loss: 0.014775, acc.: 100.00%] [G loss: 4.632107]\n",
            "4063 [D loss: 0.004639, acc.: 100.00%] [G loss: 5.512559]\n",
            "4064 [D loss: 0.002197, acc.: 100.00%] [G loss: 6.576721]\n",
            "4065 [D loss: 0.060889, acc.: 100.00%] [G loss: 8.600265]\n",
            "4066 [D loss: 0.006320, acc.: 100.00%] [G loss: 9.826292]\n",
            "4067 [D loss: 0.248093, acc.: 87.50%] [G loss: 3.615425]\n",
            "4068 [D loss: 0.021842, acc.: 100.00%] [G loss: 3.750991]\n",
            "4069 [D loss: 0.159163, acc.: 93.75%] [G loss: 13.000145]\n",
            "4070 [D loss: 0.000381, acc.: 100.00%] [G loss: 15.904024]\n",
            "4071 [D loss: 0.004017, acc.: 100.00%] [G loss: 15.630182]\n",
            "4072 [D loss: 0.001917, acc.: 100.00%] [G loss: 15.032719]\n",
            "4073 [D loss: 0.030616, acc.: 98.44%] [G loss: 14.487945]\n",
            "4074 [D loss: 0.000626, acc.: 100.00%] [G loss: 14.594004]\n",
            "4075 [D loss: 0.001232, acc.: 100.00%] [G loss: 10.403603]\n",
            "4076 [D loss: 0.006088, acc.: 100.00%] [G loss: 8.837120]\n",
            "4077 [D loss: 0.006279, acc.: 100.00%] [G loss: 8.880154]\n",
            "4078 [D loss: 0.010751, acc.: 100.00%] [G loss: 7.604840]\n",
            "4079 [D loss: 0.006225, acc.: 100.00%] [G loss: 7.347015]\n",
            "4080 [D loss: 0.006666, acc.: 100.00%] [G loss: 4.667724]\n",
            "4081 [D loss: 0.026456, acc.: 100.00%] [G loss: 7.325651]\n",
            "4082 [D loss: 0.007123, acc.: 100.00%] [G loss: 6.306021]\n",
            "4083 [D loss: 0.020148, acc.: 98.44%] [G loss: 7.615523]\n",
            "4084 [D loss: 0.033323, acc.: 100.00%] [G loss: 6.858267]\n",
            "4085 [D loss: 0.030456, acc.: 98.44%] [G loss: 6.616804]\n",
            "4086 [D loss: 0.046840, acc.: 98.44%] [G loss: 6.153367]\n",
            "4087 [D loss: 0.030097, acc.: 100.00%] [G loss: 5.852703]\n",
            "4088 [D loss: 0.088444, acc.: 98.44%] [G loss: 9.958979]\n",
            "4089 [D loss: 0.016027, acc.: 100.00%] [G loss: 10.443416]\n",
            "4090 [D loss: 0.016647, acc.: 100.00%] [G loss: 9.525958]\n",
            "4091 [D loss: 0.001718, acc.: 100.00%] [G loss: 8.261526]\n",
            "4092 [D loss: 0.012546, acc.: 100.00%] [G loss: 8.604804]\n",
            "4093 [D loss: 0.050476, acc.: 98.44%] [G loss: 8.118099]\n",
            "4094 [D loss: 0.008931, acc.: 100.00%] [G loss: 9.450418]\n",
            "4095 [D loss: 0.023599, acc.: 100.00%] [G loss: 8.087599]\n",
            "4096 [D loss: 0.019114, acc.: 100.00%] [G loss: 6.809695]\n",
            "4097 [D loss: 0.029014, acc.: 100.00%] [G loss: 4.126099]\n",
            "4098 [D loss: 0.031917, acc.: 100.00%] [G loss: 7.448686]\n",
            "4099 [D loss: 0.001688, acc.: 100.00%] [G loss: 8.118958]\n",
            "4100 [D loss: 0.036700, acc.: 98.44%] [G loss: 6.263052]\n",
            "4101 [D loss: 0.030730, acc.: 100.00%] [G loss: 6.295890]\n",
            "4102 [D loss: 0.004593, acc.: 100.00%] [G loss: 7.251938]\n",
            "4103 [D loss: 0.026518, acc.: 100.00%] [G loss: 6.241395]\n",
            "4104 [D loss: 0.010638, acc.: 100.00%] [G loss: 5.920132]\n",
            "4105 [D loss: 0.013145, acc.: 100.00%] [G loss: 7.242039]\n",
            "4106 [D loss: 0.521409, acc.: 76.56%] [G loss: 15.642960]\n",
            "4107 [D loss: 0.013686, acc.: 100.00%] [G loss: 16.118095]\n",
            "4108 [D loss: 0.877874, acc.: 59.38%] [G loss: 8.983753]\n",
            "4109 [D loss: 0.233934, acc.: 90.62%] [G loss: 12.318501]\n",
            "4110 [D loss: 0.000076, acc.: 100.00%] [G loss: 13.334524]\n",
            "4111 [D loss: 0.058342, acc.: 96.88%] [G loss: 13.135137]\n",
            "4112 [D loss: 0.005599, acc.: 100.00%] [G loss: 11.856308]\n",
            "4113 [D loss: 0.000209, acc.: 100.00%] [G loss: 8.448925]\n",
            "4114 [D loss: 0.009033, acc.: 100.00%] [G loss: 4.558252]\n",
            "4115 [D loss: 0.002338, acc.: 100.00%] [G loss: 6.753684]\n",
            "4116 [D loss: 0.094690, acc.: 96.88%] [G loss: 7.771228]\n",
            "4117 [D loss: 0.000953, acc.: 100.00%] [G loss: 11.780432]\n",
            "4118 [D loss: 0.341321, acc.: 90.62%] [G loss: 4.824988]\n",
            "4119 [D loss: 0.799900, acc.: 64.06%] [G loss: 16.118095]\n",
            "4120 [D loss: 0.157074, acc.: 93.75%] [G loss: 16.118095]\n",
            "4121 [D loss: 0.317970, acc.: 81.25%] [G loss: 15.870878]\n",
            "4122 [D loss: 0.001907, acc.: 100.00%] [G loss: 12.510918]\n",
            "4123 [D loss: 0.000013, acc.: 100.00%] [G loss: 14.674404]\n",
            "4124 [D loss: 0.000309, acc.: 100.00%] [G loss: 10.372328]\n",
            "4125 [D loss: 0.000570, acc.: 100.00%] [G loss: 11.066675]\n",
            "4126 [D loss: 0.000299, acc.: 100.00%] [G loss: 6.375456]\n",
            "4127 [D loss: 0.037806, acc.: 98.44%] [G loss: 4.766194]\n",
            "4128 [D loss: 0.006076, acc.: 100.00%] [G loss: 7.729693]\n",
            "4129 [D loss: 0.002385, acc.: 100.00%] [G loss: 6.860759]\n",
            "4130 [D loss: 0.004409, acc.: 100.00%] [G loss: 8.579556]\n",
            "4131 [D loss: 0.009801, acc.: 100.00%] [G loss: 6.838578]\n",
            "4132 [D loss: 0.034112, acc.: 100.00%] [G loss: 2.965562]\n",
            "4133 [D loss: 0.011166, acc.: 100.00%] [G loss: 5.707189]\n",
            "4134 [D loss: 0.020824, acc.: 100.00%] [G loss: 6.275846]\n",
            "4135 [D loss: 0.088728, acc.: 93.75%] [G loss: 6.150937]\n",
            "4136 [D loss: 0.236294, acc.: 92.19%] [G loss: 10.882021]\n",
            "4137 [D loss: 0.030091, acc.: 100.00%] [G loss: 12.448190]\n",
            "4138 [D loss: 0.206073, acc.: 95.31%] [G loss: 11.347097]\n",
            "4139 [D loss: 0.071833, acc.: 98.44%] [G loss: 8.792675]\n",
            "4140 [D loss: 0.001058, acc.: 100.00%] [G loss: 7.841849]\n",
            "4141 [D loss: 0.000045, acc.: 100.00%] [G loss: 8.509299]\n",
            "4142 [D loss: 0.007110, acc.: 100.00%] [G loss: 6.509577]\n",
            "4143 [D loss: 0.025839, acc.: 100.00%] [G loss: 5.521031]\n",
            "4144 [D loss: 0.571853, acc.: 68.75%] [G loss: 16.089626]\n",
            "4145 [D loss: 0.354433, acc.: 85.94%] [G loss: 16.081978]\n",
            "4146 [D loss: 0.076327, acc.: 96.88%] [G loss: 15.953579]\n",
            "4147 [D loss: 0.008643, acc.: 100.00%] [G loss: 15.710289]\n",
            "4148 [D loss: 0.005651, acc.: 100.00%] [G loss: 15.833707]\n",
            "4149 [D loss: 0.004229, acc.: 100.00%] [G loss: 15.338537]\n",
            "4150 [D loss: 0.021245, acc.: 100.00%] [G loss: 14.305113]\n",
            "4151 [D loss: 0.003111, acc.: 100.00%] [G loss: 13.300901]\n",
            "4152 [D loss: 0.010136, acc.: 100.00%] [G loss: 8.146048]\n",
            "4153 [D loss: 0.018731, acc.: 100.00%] [G loss: 8.881557]\n",
            "4154 [D loss: 0.008419, acc.: 100.00%] [G loss: 8.409740]\n",
            "4155 [D loss: 0.000609, acc.: 100.00%] [G loss: 7.917999]\n",
            "4156 [D loss: 0.047022, acc.: 100.00%] [G loss: 4.516965]\n",
            "4157 [D loss: 0.022147, acc.: 100.00%] [G loss: 6.241002]\n",
            "4158 [D loss: 0.016370, acc.: 100.00%] [G loss: 6.567034]\n",
            "4159 [D loss: 0.005946, acc.: 100.00%] [G loss: 7.440753]\n",
            "4160 [D loss: 0.023871, acc.: 100.00%] [G loss: 4.005211]\n",
            "4161 [D loss: 0.016648, acc.: 100.00%] [G loss: 6.813321]\n",
            "4162 [D loss: 0.041886, acc.: 100.00%] [G loss: 6.822600]\n",
            "4163 [D loss: 0.001737, acc.: 100.00%] [G loss: 9.425986]\n",
            "4164 [D loss: 0.033534, acc.: 100.00%] [G loss: 3.377428]\n",
            "4165 [D loss: 0.068805, acc.: 100.00%] [G loss: 6.871080]\n",
            "4166 [D loss: 0.043067, acc.: 100.00%] [G loss: 6.266082]\n",
            "4167 [D loss: 0.005060, acc.: 100.00%] [G loss: 9.574182]\n",
            "4168 [D loss: 0.019161, acc.: 100.00%] [G loss: 7.713273]\n",
            "4169 [D loss: 0.028561, acc.: 100.00%] [G loss: 5.638119]\n",
            "4170 [D loss: 0.012305, acc.: 100.00%] [G loss: 3.707384]\n",
            "4171 [D loss: 0.016734, acc.: 100.00%] [G loss: 5.032457]\n",
            "4172 [D loss: 0.244298, acc.: 89.06%] [G loss: 14.592388]\n",
            "4173 [D loss: 0.137023, acc.: 92.19%] [G loss: 15.334271]\n",
            "4174 [D loss: 0.137772, acc.: 98.44%] [G loss: 12.108067]\n",
            "4175 [D loss: 0.000502, acc.: 100.00%] [G loss: 10.124664]\n",
            "4176 [D loss: 0.003290, acc.: 100.00%] [G loss: 8.874127]\n",
            "4177 [D loss: 0.000536, acc.: 100.00%] [G loss: 10.246332]\n",
            "4178 [D loss: 0.000305, acc.: 100.00%] [G loss: 10.153067]\n",
            "4179 [D loss: 0.033952, acc.: 98.44%] [G loss: 6.961746]\n",
            "4180 [D loss: 0.002913, acc.: 100.00%] [G loss: 8.193270]\n",
            "4181 [D loss: 0.003266, acc.: 100.00%] [G loss: 6.902673]\n",
            "4182 [D loss: 0.001800, acc.: 100.00%] [G loss: 6.794870]\n",
            "4183 [D loss: 0.039496, acc.: 98.44%] [G loss: 4.940642]\n",
            "4184 [D loss: 0.069414, acc.: 98.44%] [G loss: 8.473453]\n",
            "4185 [D loss: 0.002372, acc.: 100.00%] [G loss: 9.673475]\n",
            "4186 [D loss: 0.027842, acc.: 100.00%] [G loss: 8.852184]\n",
            "4187 [D loss: 0.047445, acc.: 100.00%] [G loss: 5.049351]\n",
            "4188 [D loss: 0.053646, acc.: 98.44%] [G loss: 6.254753]\n",
            "4189 [D loss: 0.017393, acc.: 100.00%] [G loss: 5.948477]\n",
            "4190 [D loss: 0.001743, acc.: 100.00%] [G loss: 9.142217]\n",
            "4191 [D loss: 0.008539, acc.: 100.00%] [G loss: 5.948521]\n",
            "4192 [D loss: 0.008709, acc.: 100.00%] [G loss: 6.785134]\n",
            "4193 [D loss: 0.134926, acc.: 93.75%] [G loss: 8.904583]\n",
            "4194 [D loss: 0.275476, acc.: 85.94%] [G loss: 9.223913]\n",
            "4195 [D loss: 0.006442, acc.: 100.00%] [G loss: 5.985243]\n",
            "4196 [D loss: 0.006366, acc.: 100.00%] [G loss: 5.438377]\n",
            "4197 [D loss: 0.011274, acc.: 100.00%] [G loss: 5.214747]\n",
            "4198 [D loss: 0.020006, acc.: 100.00%] [G loss: 5.137027]\n",
            "4199 [D loss: 0.023138, acc.: 100.00%] [G loss: 7.103428]\n",
            "4200 [D loss: 0.026122, acc.: 100.00%] [G loss: 7.632052]\n",
            "4201 [D loss: 0.016576, acc.: 100.00%] [G loss: 5.751987]\n",
            "4202 [D loss: 0.006644, acc.: 100.00%] [G loss: 8.874413]\n",
            "4203 [D loss: 0.008811, acc.: 100.00%] [G loss: 6.134841]\n",
            "4204 [D loss: 0.031243, acc.: 100.00%] [G loss: 6.688219]\n",
            "4205 [D loss: 0.005025, acc.: 100.00%] [G loss: 8.177252]\n",
            "4206 [D loss: 0.529384, acc.: 78.12%] [G loss: 7.713134]\n",
            "4207 [D loss: 0.000329, acc.: 100.00%] [G loss: 10.234376]\n",
            "4208 [D loss: 0.000712, acc.: 100.00%] [G loss: 10.033535]\n",
            "4209 [D loss: 0.000374, acc.: 100.00%] [G loss: 10.103073]\n",
            "4210 [D loss: 0.004857, acc.: 100.00%] [G loss: 8.001374]\n",
            "4211 [D loss: 0.004336, acc.: 100.00%] [G loss: 6.375345]\n",
            "4212 [D loss: 0.001096, acc.: 100.00%] [G loss: 6.596280]\n",
            "4213 [D loss: 0.006317, acc.: 100.00%] [G loss: 8.171540]\n",
            "4214 [D loss: 0.008321, acc.: 100.00%] [G loss: 6.333060]\n",
            "4215 [D loss: 0.012560, acc.: 100.00%] [G loss: 6.598425]\n",
            "4216 [D loss: 0.005013, acc.: 100.00%] [G loss: 7.422332]\n",
            "4217 [D loss: 0.051993, acc.: 98.44%] [G loss: 7.572244]\n",
            "4218 [D loss: 0.002220, acc.: 100.00%] [G loss: 10.624781]\n",
            "4219 [D loss: 0.003538, acc.: 100.00%] [G loss: 6.845551]\n",
            "4220 [D loss: 0.053225, acc.: 98.44%] [G loss: 7.720230]\n",
            "4221 [D loss: 0.003924, acc.: 100.00%] [G loss: 4.393845]\n",
            "4222 [D loss: 0.110545, acc.: 96.88%] [G loss: 8.608986]\n",
            "4223 [D loss: 0.046751, acc.: 100.00%] [G loss: 9.472703]\n",
            "4224 [D loss: 0.001087, acc.: 100.00%] [G loss: 11.285059]\n",
            "4225 [D loss: 0.008704, acc.: 100.00%] [G loss: 9.099399]\n",
            "4226 [D loss: 0.003574, acc.: 100.00%] [G loss: 9.075185]\n",
            "4227 [D loss: 0.009623, acc.: 100.00%] [G loss: 8.052042]\n",
            "4228 [D loss: 0.018076, acc.: 100.00%] [G loss: 6.406294]\n",
            "4229 [D loss: 0.014059, acc.: 100.00%] [G loss: 6.248737]\n",
            "4230 [D loss: 0.022072, acc.: 100.00%] [G loss: 6.048511]\n",
            "4231 [D loss: 0.023097, acc.: 100.00%] [G loss: 6.725499]\n",
            "4232 [D loss: 0.005870, acc.: 100.00%] [G loss: 7.187884]\n",
            "4233 [D loss: 0.003043, acc.: 100.00%] [G loss: 7.588786]\n",
            "4234 [D loss: 0.032457, acc.: 100.00%] [G loss: 7.299080]\n",
            "4235 [D loss: 0.007088, acc.: 100.00%] [G loss: 8.194368]\n",
            "4236 [D loss: 0.156403, acc.: 96.88%] [G loss: 4.891016]\n",
            "4237 [D loss: 0.050065, acc.: 98.44%] [G loss: 8.321694]\n",
            "4238 [D loss: 0.001147, acc.: 100.00%] [G loss: 9.030514]\n",
            "4239 [D loss: 0.000216, acc.: 100.00%] [G loss: 9.231028]\n",
            "4240 [D loss: 0.007558, acc.: 100.00%] [G loss: 7.918593]\n",
            "4241 [D loss: 0.025611, acc.: 100.00%] [G loss: 4.502889]\n",
            "4242 [D loss: 0.075226, acc.: 96.88%] [G loss: 8.565858]\n",
            "4243 [D loss: 0.016766, acc.: 100.00%] [G loss: 11.163310]\n",
            "4244 [D loss: 0.002915, acc.: 100.00%] [G loss: 12.491677]\n",
            "4245 [D loss: 0.002748, acc.: 100.00%] [G loss: 9.793877]\n",
            "4246 [D loss: 0.009957, acc.: 100.00%] [G loss: 10.101307]\n",
            "4247 [D loss: 0.009073, acc.: 100.00%] [G loss: 7.778399]\n",
            "4248 [D loss: 0.003806, acc.: 100.00%] [G loss: 7.093760]\n",
            "4249 [D loss: 0.001448, acc.: 100.00%] [G loss: 8.364570]\n",
            "4250 [D loss: 0.001436, acc.: 100.00%] [G loss: 6.712185]\n",
            "4251 [D loss: 0.020679, acc.: 100.00%] [G loss: 5.675546]\n",
            "4252 [D loss: 0.070959, acc.: 98.44%] [G loss: 5.123947]\n",
            "4253 [D loss: 0.025092, acc.: 100.00%] [G loss: 5.252090]\n",
            "4254 [D loss: 0.022927, acc.: 100.00%] [G loss: 6.913106]\n",
            "4255 [D loss: 0.005101, acc.: 100.00%] [G loss: 9.318634]\n",
            "4256 [D loss: 0.002828, acc.: 100.00%] [G loss: 8.645868]\n",
            "4257 [D loss: 0.116854, acc.: 96.88%] [G loss: 5.863390]\n",
            "4258 [D loss: 0.115320, acc.: 93.75%] [G loss: 11.223758]\n",
            "4259 [D loss: 0.068006, acc.: 96.88%] [G loss: 12.536809]\n",
            "4260 [D loss: 0.005360, acc.: 100.00%] [G loss: 12.957052]\n",
            "4261 [D loss: 0.000172, acc.: 100.00%] [G loss: 12.654165]\n",
            "4262 [D loss: 0.033055, acc.: 98.44%] [G loss: 8.205000]\n",
            "4263 [D loss: 0.002563, acc.: 100.00%] [G loss: 8.688442]\n",
            "4264 [D loss: 0.002564, acc.: 100.00%] [G loss: 5.709888]\n",
            "4265 [D loss: 0.001211, acc.: 100.00%] [G loss: 7.884591]\n",
            "4266 [D loss: 0.003489, acc.: 100.00%] [G loss: 7.576251]\n",
            "4267 [D loss: 0.033578, acc.: 98.44%] [G loss: 7.435322]\n",
            "4268 [D loss: 0.016208, acc.: 100.00%] [G loss: 6.108146]\n",
            "4269 [D loss: 0.005358, acc.: 100.00%] [G loss: 9.366879]\n",
            "4270 [D loss: 0.002689, acc.: 100.00%] [G loss: 8.789069]\n",
            "4271 [D loss: 0.011452, acc.: 100.00%] [G loss: 6.749881]\n",
            "4272 [D loss: 0.047507, acc.: 100.00%] [G loss: 8.443965]\n",
            "4273 [D loss: 0.018860, acc.: 100.00%] [G loss: 9.662401]\n",
            "4274 [D loss: 0.002797, acc.: 100.00%] [G loss: 6.660837]\n",
            "4275 [D loss: 0.018703, acc.: 100.00%] [G loss: 7.655531]\n",
            "4276 [D loss: 0.003746, acc.: 100.00%] [G loss: 6.501646]\n",
            "4277 [D loss: 0.043179, acc.: 98.44%] [G loss: 9.875822]\n",
            "4278 [D loss: 0.001700, acc.: 100.00%] [G loss: 6.836164]\n",
            "4279 [D loss: 0.011195, acc.: 100.00%] [G loss: 6.275716]\n",
            "4280 [D loss: 0.019436, acc.: 100.00%] [G loss: 5.795863]\n",
            "4281 [D loss: 0.021317, acc.: 100.00%] [G loss: 4.153946]\n",
            "4282 [D loss: 0.017080, acc.: 100.00%] [G loss: 7.690410]\n",
            "4283 [D loss: 0.014417, acc.: 100.00%] [G loss: 8.360299]\n",
            "4284 [D loss: 0.018567, acc.: 100.00%] [G loss: 7.733065]\n",
            "4285 [D loss: 0.010711, acc.: 100.00%] [G loss: 5.515958]\n",
            "4286 [D loss: 0.007756, acc.: 100.00%] [G loss: 4.239356]\n",
            "4287 [D loss: 0.015040, acc.: 100.00%] [G loss: 7.620871]\n",
            "4288 [D loss: 0.025467, acc.: 100.00%] [G loss: 5.118120]\n",
            "4289 [D loss: 0.011706, acc.: 100.00%] [G loss: 4.468929]\n",
            "4290 [D loss: 0.007385, acc.: 100.00%] [G loss: 5.376055]\n",
            "4291 [D loss: 0.017732, acc.: 100.00%] [G loss: 7.357386]\n",
            "4292 [D loss: 0.008224, acc.: 100.00%] [G loss: 6.779247]\n",
            "4293 [D loss: 0.014688, acc.: 100.00%] [G loss: 7.504609]\n",
            "4294 [D loss: 0.014061, acc.: 100.00%] [G loss: 5.302781]\n",
            "4295 [D loss: 0.015316, acc.: 100.00%] [G loss: 7.605976]\n",
            "4296 [D loss: 0.022061, acc.: 100.00%] [G loss: 7.107660]\n",
            "4297 [D loss: 0.021639, acc.: 100.00%] [G loss: 8.330639]\n",
            "4298 [D loss: 0.007156, acc.: 100.00%] [G loss: 7.298848]\n",
            "4299 [D loss: 0.004800, acc.: 100.00%] [G loss: 7.318324]\n",
            "4300 [D loss: 0.003853, acc.: 100.00%] [G loss: 5.694381]\n",
            "4301 [D loss: 0.004696, acc.: 100.00%] [G loss: 5.850119]\n",
            "4302 [D loss: 0.046176, acc.: 100.00%] [G loss: 6.547746]\n",
            "4303 [D loss: 0.021052, acc.: 100.00%] [G loss: 8.949066]\n",
            "4304 [D loss: 0.056296, acc.: 98.44%] [G loss: 5.952150]\n",
            "4305 [D loss: 0.255883, acc.: 90.62%] [G loss: 16.118095]\n",
            "4306 [D loss: 0.156632, acc.: 95.31%] [G loss: 16.058014]\n",
            "4307 [D loss: 0.084593, acc.: 95.31%] [G loss: 15.937677]\n",
            "4308 [D loss: 0.002025, acc.: 100.00%] [G loss: 15.768488]\n",
            "4309 [D loss: 0.000015, acc.: 100.00%] [G loss: 14.838264]\n",
            "4310 [D loss: 0.002920, acc.: 100.00%] [G loss: 14.376783]\n",
            "4311 [D loss: 0.000063, acc.: 100.00%] [G loss: 13.962381]\n",
            "4312 [D loss: 0.000088, acc.: 100.00%] [G loss: 11.225355]\n",
            "4313 [D loss: 0.000288, acc.: 100.00%] [G loss: 10.607213]\n",
            "4314 [D loss: 0.000556, acc.: 100.00%] [G loss: 11.503314]\n",
            "4315 [D loss: 0.000085, acc.: 100.00%] [G loss: 10.003520]\n",
            "4316 [D loss: 0.000833, acc.: 100.00%] [G loss: 8.245335]\n",
            "4317 [D loss: 0.001761, acc.: 100.00%] [G loss: 6.005270]\n",
            "4318 [D loss: 0.002538, acc.: 100.00%] [G loss: 6.583807]\n",
            "4319 [D loss: 0.013440, acc.: 100.00%] [G loss: 6.362525]\n",
            "4320 [D loss: 0.001414, acc.: 100.00%] [G loss: 7.820831]\n",
            "4321 [D loss: 0.003347, acc.: 100.00%] [G loss: 7.585093]\n",
            "4322 [D loss: 0.000787, acc.: 100.00%] [G loss: 8.090958]\n",
            "4323 [D loss: 0.011892, acc.: 100.00%] [G loss: 7.385242]\n",
            "4324 [D loss: 0.009443, acc.: 100.00%] [G loss: 6.623105]\n",
            "4325 [D loss: 0.003990, acc.: 100.00%] [G loss: 10.383906]\n",
            "4326 [D loss: 0.003305, acc.: 100.00%] [G loss: 7.232409]\n",
            "4327 [D loss: 0.003056, acc.: 100.00%] [G loss: 7.290162]\n",
            "4328 [D loss: 0.004098, acc.: 100.00%] [G loss: 6.841409]\n",
            "4329 [D loss: 0.015818, acc.: 100.00%] [G loss: 6.204551]\n",
            "4330 [D loss: 0.046649, acc.: 98.44%] [G loss: 9.792503]\n",
            "4331 [D loss: 0.005540, acc.: 100.00%] [G loss: 9.334164]\n",
            "4332 [D loss: 0.124927, acc.: 93.75%] [G loss: 4.854075]\n",
            "4333 [D loss: 0.135712, acc.: 92.19%] [G loss: 11.457230]\n",
            "4334 [D loss: 0.000726, acc.: 100.00%] [G loss: 14.568438]\n",
            "4335 [D loss: 0.122411, acc.: 96.88%] [G loss: 9.097229]\n",
            "4336 [D loss: 0.000621, acc.: 100.00%] [G loss: 6.746819]\n",
            "4337 [D loss: 0.000360, acc.: 100.00%] [G loss: 8.187716]\n",
            "4338 [D loss: 0.001396, acc.: 100.00%] [G loss: 7.518712]\n",
            "4339 [D loss: 0.013890, acc.: 100.00%] [G loss: 6.119178]\n",
            "4340 [D loss: 0.003574, acc.: 100.00%] [G loss: 6.375295]\n",
            "4341 [D loss: 0.007756, acc.: 100.00%] [G loss: 6.100604]\n",
            "4342 [D loss: 0.008911, acc.: 100.00%] [G loss: 7.306886]\n",
            "4343 [D loss: 0.001613, acc.: 100.00%] [G loss: 6.270445]\n",
            "4344 [D loss: 0.015841, acc.: 100.00%] [G loss: 5.895995]\n",
            "4345 [D loss: 0.001772, acc.: 100.00%] [G loss: 7.632201]\n",
            "4346 [D loss: 0.013614, acc.: 100.00%] [G loss: 6.164971]\n",
            "4347 [D loss: 0.044029, acc.: 100.00%] [G loss: 8.675482]\n",
            "4348 [D loss: 0.001884, acc.: 100.00%] [G loss: 12.171364]\n",
            "4349 [D loss: 0.003342, acc.: 100.00%] [G loss: 12.055166]\n",
            "4350 [D loss: 0.032558, acc.: 100.00%] [G loss: 8.968369]\n",
            "4351 [D loss: 0.007096, acc.: 100.00%] [G loss: 6.246945]\n",
            "4352 [D loss: 0.040190, acc.: 98.44%] [G loss: 8.982304]\n",
            "4353 [D loss: 0.004369, acc.: 100.00%] [G loss: 8.615099]\n",
            "4354 [D loss: 0.006457, acc.: 100.00%] [G loss: 8.916367]\n",
            "4355 [D loss: 0.000414, acc.: 100.00%] [G loss: 9.334166]\n",
            "4356 [D loss: 0.005885, acc.: 100.00%] [G loss: 7.004360]\n",
            "4357 [D loss: 0.002623, acc.: 100.00%] [G loss: 4.153655]\n",
            "4358 [D loss: 0.087872, acc.: 96.88%] [G loss: 13.176645]\n",
            "4359 [D loss: 0.036812, acc.: 98.44%] [G loss: 15.578410]\n",
            "4360 [D loss: 0.092410, acc.: 98.44%] [G loss: 10.157257]\n",
            "4361 [D loss: 0.001184, acc.: 100.00%] [G loss: 9.873127]\n",
            "4362 [D loss: 0.001831, acc.: 100.00%] [G loss: 5.296764]\n",
            "4363 [D loss: 0.008001, acc.: 100.00%] [G loss: 7.199392]\n",
            "4364 [D loss: 0.001494, acc.: 100.00%] [G loss: 5.541212]\n",
            "4365 [D loss: 0.278011, acc.: 90.62%] [G loss: 15.470108]\n",
            "4366 [D loss: 0.414879, acc.: 78.12%] [G loss: 10.930223]\n",
            "4367 [D loss: 0.004433, acc.: 100.00%] [G loss: 8.879322]\n",
            "4368 [D loss: 0.026591, acc.: 100.00%] [G loss: 6.648940]\n",
            "4369 [D loss: 0.000811, acc.: 100.00%] [G loss: 9.168318]\n",
            "4370 [D loss: 0.001873, acc.: 100.00%] [G loss: 5.986253]\n",
            "4371 [D loss: 0.004034, acc.: 100.00%] [G loss: 6.070777]\n",
            "4372 [D loss: 0.007658, acc.: 100.00%] [G loss: 6.541474]\n",
            "4373 [D loss: 0.052491, acc.: 98.44%] [G loss: 9.677423]\n",
            "4374 [D loss: 0.001488, acc.: 100.00%] [G loss: 9.021187]\n",
            "4375 [D loss: 0.028004, acc.: 98.44%] [G loss: 11.254002]\n",
            "4376 [D loss: 0.006699, acc.: 100.00%] [G loss: 6.705339]\n",
            "4377 [D loss: 0.075917, acc.: 98.44%] [G loss: 5.688040]\n",
            "4378 [D loss: 0.005050, acc.: 100.00%] [G loss: 5.779107]\n",
            "4379 [D loss: 0.046146, acc.: 98.44%] [G loss: 7.958475]\n",
            "4380 [D loss: 0.004625, acc.: 100.00%] [G loss: 9.635183]\n",
            "4381 [D loss: 0.024909, acc.: 98.44%] [G loss: 8.275537]\n",
            "4382 [D loss: 0.002236, acc.: 100.00%] [G loss: 5.807214]\n",
            "4383 [D loss: 0.031868, acc.: 98.44%] [G loss: 7.924769]\n",
            "4384 [D loss: 0.009332, acc.: 100.00%] [G loss: 10.576302]\n",
            "4385 [D loss: 0.041125, acc.: 100.00%] [G loss: 6.698332]\n",
            "4386 [D loss: 0.000443, acc.: 100.00%] [G loss: 6.116354]\n",
            "4387 [D loss: 0.086960, acc.: 96.88%] [G loss: 12.272320]\n",
            "4388 [D loss: 0.007633, acc.: 100.00%] [G loss: 14.143682]\n",
            "4389 [D loss: 0.016147, acc.: 100.00%] [G loss: 14.869125]\n",
            "4390 [D loss: 0.024290, acc.: 100.00%] [G loss: 9.786404]\n",
            "4391 [D loss: 0.024724, acc.: 100.00%] [G loss: 8.933341]\n",
            "4392 [D loss: 0.002722, acc.: 100.00%] [G loss: 7.696976]\n",
            "4393 [D loss: 0.026409, acc.: 100.00%] [G loss: 7.707123]\n",
            "4394 [D loss: 0.000701, acc.: 100.00%] [G loss: 10.980124]\n",
            "4395 [D loss: 0.000829, acc.: 100.00%] [G loss: 7.144297]\n",
            "4396 [D loss: 0.002160, acc.: 100.00%] [G loss: 9.250154]\n",
            "4397 [D loss: 0.003847, acc.: 100.00%] [G loss: 7.926941]\n",
            "4398 [D loss: 0.006200, acc.: 100.00%] [G loss: 6.666012]\n",
            "4399 [D loss: 0.007289, acc.: 100.00%] [G loss: 6.597237]\n",
            "4400 [D loss: 0.052870, acc.: 98.44%] [G loss: 3.551756]\n",
            "4401 [D loss: 0.000927, acc.: 100.00%] [G loss: 8.856009]\n",
            "4402 [D loss: 0.008496, acc.: 100.00%] [G loss: 6.770059]\n",
            "4403 [D loss: 0.023874, acc.: 100.00%] [G loss: 5.479408]\n",
            "4404 [D loss: 0.066239, acc.: 98.44%] [G loss: 10.873571]\n",
            "4405 [D loss: 0.019678, acc.: 100.00%] [G loss: 12.976768]\n",
            "4406 [D loss: 0.041888, acc.: 98.44%] [G loss: 8.800729]\n",
            "4407 [D loss: 0.000582, acc.: 100.00%] [G loss: 8.677244]\n",
            "4408 [D loss: 0.001801, acc.: 100.00%] [G loss: 7.589437]\n",
            "4409 [D loss: 0.002263, acc.: 100.00%] [G loss: 7.530292]\n",
            "4410 [D loss: 0.038427, acc.: 100.00%] [G loss: 6.659325]\n",
            "4411 [D loss: 0.000748, acc.: 100.00%] [G loss: 6.821736]\n",
            "4412 [D loss: 0.031808, acc.: 98.44%] [G loss: 4.663430]\n",
            "4413 [D loss: 0.001468, acc.: 100.00%] [G loss: 7.390540]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b8b54b939f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_epoch_no\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1821\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-ae7f0a4953ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, epochs, batch_size, save_interval, start_epoch_no)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# Train the generator (wants discriminator to mistake images as real)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}