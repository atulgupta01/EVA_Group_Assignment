{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evap2assignment9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atulgupta01/EVA_Group_Assignment/blob/master/P2S9/evap2assignment9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WroQa-GpDddC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import gym\n",
        "import pybullet_envs\n",
        "from gym import wrappers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj_WbJmADjGv",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAdApeNeEOXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#====================================\n",
        "# ReplayBuffer class\n",
        "# init function has max_size = 1e6\n",
        "# we initialize the replay mem buffer with 1e6 and then add transitions to it\n",
        "# once the buffer is full new trainsition overwrites it \n",
        "# sample fucntion takes batch size as input selects random tuple from the storage\n",
        "# appends all the return of the storage individually and returns that\n",
        "#===================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtKj1SSPD8j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ReplayBuffer(object):\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, len(self.storage), batch_size)\n",
        "        batch_states , batch_next_states, batch_actions, batch_rewards ,batch_dones = [],[],[],[],[]\n",
        "        for i in ind:\n",
        "            state, next_state, action, reward, done = self.storage[i]\n",
        "            batch_states.append(np.array(state, copy=False))\n",
        "            batch_next_state.append(np.array(next_state, copy=False))\n",
        "            batch_actions.append(np.array(action, copy=False))\n",
        "            batch_rewards.append(np.array(reward, copy=False))\n",
        "            batch_dones.append(np.array(done, copy=False))\n",
        "        return np.array(batch_states), np.array(batch_next_states), np.array(\n",
        "            batch_actions), np.array(batch_rewards).reshape(\n",
        "                -1, 1), np.array(batch_dones).reshape(-1, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivVItDTREVmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#====================================\n",
        "# Actor Model Class\n",
        "# inputs --> state dimensions, action dimensions, max action limit (to limit action predicted by some limit)\n",
        "# output --> returns predicted action (max action is multiplied at the last layer output to limit the action taken)\n",
        "#===================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGX90IG9ECoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dims, action_dim, max_action):\n",
        "        # max action is to clip in case we added too much noise\n",
        "        # state dim are state parameters\n",
        "        # action dim is number of actions\n",
        "        super(Actor, self).__init__()\n",
        "        self.layer_1 = nn.Linear(state_dims, 400)\n",
        "        self.layer_2 = nn.Linear(400, 300)\n",
        "        self.layer_3 = nn.Linear(300, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer_1(x))\n",
        "        x = F.relu(self.layer_2(x))\n",
        "        # basically x says which action to be taken and by how much the action should be taken\n",
        "        x = self.max_actions * torch.tanh(self.layer_3(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twj9fAt-EGMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#====================================\n",
        "# Critic Model Class\n",
        "# 2 Critic model is defined \n",
        "# forward fucntion says that given one action and state we predict q value\n",
        "# here we have chosen only critic one to update the actor model(somehat like gan theory) so\n",
        "# Q1 uses only the first critic network  using this we will update the actor network\n",
        "#===================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lK1R2nPELBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dims, action_dim):\n",
        "        super(Critic, self).__init__()  # activate the inheritance\n",
        "        # First Critic Network\n",
        "        self.layer_1 = nn.Linear(state_dims + action_dim, 400)\n",
        "        self.layer_2 = nn.Linear(400, 300)\n",
        "        self.layer_3 = nn.Linear(300, 1)\n",
        "        # Second Critic Network\n",
        "        self.layer_4 = nn.Linear(state_dims + action_dim, 400)\n",
        "        self.layer_5 = nn.Linear(400, 300)\n",
        "        self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):  # x -> state, u -> action\n",
        "        xu = torch.cat([x, u], 1)  # concat along axis 1\n",
        "        # forward propagatin on First Critic\n",
        "        x1 = F.relu(self.layer_1(xu))\n",
        "        x1 = F.relu(self.layer_2(x1))\n",
        "        x1 = self.layer_3(x1)\n",
        "        # forward propagatin on Second Critic\n",
        "        x2 = F.relu(self.layer_4(xu))\n",
        "        x2 = F.relu(self.layer_5(x2))\n",
        "        x2 = F.relu(self.layer_5(x2))\n",
        "\n",
        "        return x1, x2\n",
        "\n",
        "    def Q1(\n",
        "        self, x, u\n",
        "    ):  # x -> state, u -> action this is to update the actor using first critic\n",
        "        xu = torch.cat([x, u], 1)  # concat along axis 1\n",
        "        x1 = F.relu(self.layer_1(xu))\n",
        "        x1 = F.relu(self.layer_2(x1))\n",
        "        x1 = self.layer_3(x1)\n",
        "        return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EorFjgy9EiLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#====================================\n",
        "# Trainin Process\n",
        "# we have 2 actor (actor model(trained using backpropagation),actor_target(trained using polyak avg))\n",
        "# critic 1 and 2 has same dnn str\n",
        "# we have 2 critic1 (critic1 model(trained using backpropagation),critic1 target (trained using polyak avg))\n",
        "# we have 2 critic2 (critic2 model(trained using backpropagation),critic2 target (trained using polyak avg))\n",
        "#===================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpm59pRtE8Rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select the available device gpu/cpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Train Block\n",
        "class T3D(object):\n",
        "    def __init__(self, state_dims, action_dim, max_action):\n",
        "        # making sure our T3D class can work with any env\n",
        "        \n",
        "        # load actor and actor_target, inilialize actor_target with actor weights \n",
        "        self.actor = Actor(state_dims, action_dim, max_action).to(device)  # GD\n",
        "        self.actor_target = Actor(state_dims, action_dim,\n",
        "                                  max_action).to(device)  # Polyak Avg\n",
        "        # initializing with model weights to keep them same\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "        # load critic and critic_target, inilialize critic_target with actor weights \n",
        "        self.critic = Critic(state_dims, action_dim).to(device)  # GD\n",
        "        self.critic_target = Critic(state_dims,\n",
        "                                    action_dim).to(device)  # Polyak Avg\n",
        "        # initializing with model weights to keep them same\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # select state\n",
        "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "        # convert to numpy\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    ################# Step 4 ################\n",
        "    def train(self,\n",
        "              replay_buffer,\n",
        "              iterations,\n",
        "              batch_size=100,\n",
        "              discount=0.99,\n",
        "              tau=0.005,\n",
        "              policy_noise=0.2,\n",
        "              noise_clip=0.5,\n",
        "              policy_freq=2):\n",
        "        for it in range(iterations):\n",
        "            ################ Step 4 ######################\n",
        "            # smaple from a batch of transitions (s,s',a,r) from memory\n",
        "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(\n",
        "                batch_size)\n",
        "            state = torch.Tensor(batch_states).to(device)\n",
        "            next_state = torch.Tensor(batch_next_states).to(device)\n",
        "            action = torch.Tensor(batch_actions).to(device)\n",
        "            reward = torch.Tensor(batch_rewards).to(device)\n",
        "            done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "            ################# Step 5 ################\n",
        "            # from the next state s', the actor target plays the next actionss a'\n",
        "            next_action = self.actor_target.forward(next_state)\n",
        "\n",
        "            ################# Step 6 ################\n",
        "            # We add Gaussian noise to this next action a'\n",
        "            # and we clamp it in the range of values supported by the environments\n",
        "\n",
        "            noise = torch.Tensor(batch_actions).data.normal_(\n",
        "                0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (next_action + noise).clamp(-self.max_action,\n",
        "                                                      self.max_action)\n",
        "\n",
        "            ################# Step 7 ################\n",
        "            # The two Critic targets take each the couple(s',a') as input and\n",
        "            # return two Q values,q\n",
        "            target_Q1, target_Q2 = self.critic_target.forward(\n",
        "                next_state, next_action)\n",
        "\n",
        "            ################# Step 8 ################\n",
        "            # We keep the minimum of these two Q-values\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "            ################# Step 9 ################\n",
        "            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "            ################# Step 10 ################\n",
        "            # The two critic models take each the couple (s,a') as input and return two Q values\n",
        "            current_Q1, current_Q2 = self.critic.forward(state, action)\n",
        "\n",
        "            ################# Step 11 ################\n",
        "            # We compute the loss coming from 2 Critic models\n",
        "            F\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(\n",
        "                current_Q2, target_Q)\n",
        "\n",
        "            ################# Step 12 ################\n",
        "            # Backpropagate the critic loss and update the parameters of the 2 critic models with Adam optimizers\n",
        "            self.critic_optimizer.zero_grad(\n",
        "            )  # initialize critic prev grad(if any) to zero\n",
        "            critic_loss.backward()  # computing the gradients\n",
        "            self.critic_optimizer.step()  # performing the weight updates\n",
        "\n",
        "            ################# Step 13 ################\n",
        "            # Once every 2 iterations, we update our Actor model by performing gradient accent\n",
        "            # on the output of the first critic model\n",
        "\n",
        "            if it % policy_freq == 0:\n",
        "            # This is DPG part\n",
        "                actor_loss = (self.critic.Q1(state, self.actor(state)).mean())\n",
        "                self.actor_optimizer.zero_grad(\n",
        "                )  # initialize actor prev grad(if any) to zero\n",
        "                actor_loss.backward()  # computing the gradients\n",
        "                self.actor_optimizer.step()  # performing the weight updates\n",
        "\n",
        "            ################# Step 14 ################\n",
        "            # Still once every 2 iteration, we update the weights of the Critic target by Polyak averaging\n",
        "\n",
        "                for param, target_param in zip(self.actor.parameters(),\n",
        "                                               self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau)\n",
        "                                            & target_param.data)\n",
        "\n",
        "            ################# Step 15 ################\n",
        "            # Still once every 2 iteration, we update the weights of the actor target by Polyak averaging\n",
        "\n",
        "                for param, target_param in zip(self.critic.parameters(),\n",
        "                                               self.critic_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau)\n",
        "                                            & target_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}