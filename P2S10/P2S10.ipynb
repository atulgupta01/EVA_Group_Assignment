{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2S10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZE3nJ0l0676VFfdKjvcig"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jesYdPhYRbhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import copy\n",
        "from PIL import Image as PILImage\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbsPfzy-OpDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import copy\n",
        "from PIL import Image as PILImage\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "class car(object):\n",
        "\n",
        "  # x and y are center points of the car\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        x,\n",
        "        y,\n",
        "        angle,\n",
        "        ):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        (self.length, self.width) = (int(20), int(10))\n",
        "        self.angle = angle\n",
        "\n",
        "    def move(\n",
        "        self,\n",
        "        velocity_x,\n",
        "        velocity_y,\n",
        "        rotation,\n",
        "        ):\n",
        "        self.x = self.x + velocity_x\n",
        "        self.y = self.y + velocity_y\n",
        "        self.angle = self.angle + rotation\n",
        "\n",
        "        if self.angle > 360:\n",
        "            self.angle = self.angle % 360\n",
        "        elif self.angle < -360:\n",
        "            self.angle = self.angle % -360\n",
        "\n",
        "\n",
        "class city(object):\n",
        "\n",
        "    def __init__(self, city_file):\n",
        "        self.city_file = city_file\n",
        "        self.city_img = cv.imread(self.city_file)\n",
        "        (self.width, self.length, _) = self.city_img.shape\n",
        "\n",
        "    def draw_car(\n",
        "        self,\n",
        "        x,\n",
        "        y,\n",
        "        width,\n",
        "        height,\n",
        "        angle,\n",
        "        img,\n",
        "        ):\n",
        "\n",
        "        _angle = (180 - angle) * math.pi / 180.0\n",
        "        b = math.cos(_angle) * 0.5\n",
        "        a = math.sin(_angle) * 0.5\n",
        "        pt0 = (int(x - a * height - b * width), int(y + b * height - a\n",
        "               * width))\n",
        "        pt1 = (int(x + a * height - b * width), int(y - b * height - a\n",
        "               * width))\n",
        "        pt2 = (int(2 * x - pt0[0]), int(2 * y - pt0[1]))\n",
        "        pt3 = (int(2 * x - pt1[0]), int(2 * y - pt1[1]))\n",
        "        pt4 = (int((pt0[0] + pt1[0]) / 2), int((pt0[1] + pt1[1]) / 2))\n",
        "        pt5 = (int((pt0[0] + pt3[0]) / 2), int((pt0[1] + pt3[1]) / 2))\n",
        "        pt6 = (int((pt1[0] + pt2[0]) / 2), int((pt1[1] + pt2[1]) / 2))\n",
        "\n",
        "        line_color = (200, 200, 200)\n",
        "        line_thickness = 5\n",
        "\n",
        "    # print(pt0, pt1, pt2, pt3, pt4)\n",
        "\n",
        "        cv.line(img, pt2, pt3, line_color, line_thickness)\n",
        "        cv.line(img, pt3, pt5, line_color, line_thickness)\n",
        "        cv.line(img, pt6, pt2, line_color, line_thickness)\n",
        "        cv.line(img, pt5, pt4, line_color, line_thickness)\n",
        "        cv.line(img, pt6, pt4, line_color, line_thickness)\n",
        "\n",
        "    def get_current_loc_map(\n",
        "        self,\n",
        "        x,\n",
        "        y,\n",
        "        size,\n",
        "        angle=0,\n",
        "        state=False,\n",
        "        ):\n",
        "\n",
        "        newcity_img = copy.deepcopy(self.city_img)\n",
        "\n",
        "        if x - size / 2 < 0 or y - size / 2 < 0 or x + size / 2 \\\n",
        "            > self.length - 1 or y + size / 2 > self.width - 1:\n",
        "            return np.ones((size, size, 3))\n",
        "        else:\n",
        "            y = self.width - y\n",
        "\n",
        "        if state == True:\n",
        "            img_crop = self.draw_car(\n",
        "                x,\n",
        "                y,\n",
        "                20,\n",
        "                10,\n",
        "                angle,\n",
        "                newcity_img,\n",
        "                )\n",
        "\n",
        "        img_crop = newcity_img[int(y - size / 2):int(y) + int(size / 2), int(x\n",
        "                               - size / 2):int(x) + int(size / 2)]\n",
        "        img_state = np.average(img_crop, axis=2) / 255\n",
        "        return (img_crop, img_state)\n",
        "\n",
        "\n",
        "class env(object):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        car,\n",
        "        city,\n",
        "        city_map,\n",
        "        car_img,\n",
        "        ):\n",
        "        self.car = car\n",
        "        self.city = city\n",
        "        self.city_map = city_map\n",
        "        self.car_img = car_img\n",
        "        self.car_img = cv.resize(self.car_img, (self.car.length,\n",
        "                                 self.car.width))\n",
        "        self.size = 80\n",
        "\n",
        "    # car_x and car_y are center points of the car\n",
        "\n",
        "    def show_image(self):\n",
        "        newcity = copy.deepcopy(self.city)\n",
        "        car_rotated = ndimage.rotate(self.car_img, self.car.angle)\n",
        "        (car_wid, car_len, _) = car_rotated.shape\n",
        "        pos_x = self.car.x - car_len // 2\n",
        "        pos_y = newcity.width - (self.car.y - car_wid // 2)\n",
        "\n",
        "        if pos_x < 0:\n",
        "            pos_x = 0\n",
        "        elif pos_x > newcity.length:\n",
        "            pos_x = newcity.length - car_len\n",
        "\n",
        "        if pos_y > newcity.width:\n",
        "            pos_y = newcity.width - car_wid\n",
        "        elif pos_y < 0:\n",
        "            pos_y = 0\n",
        "\n",
        "        car_rotated = cv.addWeighted(newcity.city_img[pos_y:pos_y\n",
        "                + car_wid, pos_x:pos_x + car_len], 0.5, car_rotated, 1,\n",
        "                0)\n",
        "        newcity.city_img[pos_y:pos_y + car_wid, pos_x:pos_x\n",
        "                         + car_len] = car_rotated\n",
        "        return newcity\n",
        "\n",
        "    def step(self, action):\n",
        "        self.reward = 0\n",
        "        self.velocity_x = 0.5\n",
        "        self.velocity_y = 0\n",
        "        done = False\n",
        "\n",
        "        angle = math.radians(action)\n",
        "        self.velocity_x = self.velocity_x * math.cos(angle) \\\n",
        "            - self.velocity_y * math.sin(angle)\n",
        "        self.velocity_y = self.velocity_y * math.cos(angle) \\\n",
        "            + self.velocity_x * math.sin(angle)\n",
        "        self.car.move(self.velocity_x, self.velocity_y, action)\n",
        "        xx = self.goal_x - self.car.x\n",
        "        yy = self.goal_y - self.car.y\n",
        "\n",
        "        distance = np.sqrt((self.car.x - self.goal_x) ** 2 + (self.car.y\n",
        "                           - self.goal_y) ** 2)\n",
        "    \n",
        "        car_loc, _ = self.city_map.get_current_loc_map(self.car.x,\n",
        "                              self.car.y, self.size)\n",
        "        sand_quality = np.sum(car_loc)\n",
        "        sand_quality = sand_quality / (self.size * self.size * 3 * 255)\n",
        "    \n",
        "        # moving on the sand\n",
        "        # check coordinates carefully image cordinate y is inverse of car coordinate y\n",
        "    \n",
        "        sand_check = np.sum(self.city_map.city_img[int(self.city.width\n",
        "                            - self.car.y), int(self.car.x)]) / (255 * 3)\n",
        "    \n",
        "        if sand_check > 0:  # **** Check whether coords are correct\n",
        "            self.reward = self.reward - 5.0\n",
        "        else:\n",
        "    \n",
        "               # moving on the road\n",
        "    \n",
        "            self.reward = self.reward - 1.5\n",
        "    \n",
        "        if self.car.x - int(self.car.length / 2) < 5 or self.car.y \\\n",
        "            - int(self.car.width / 2) < 5 or self.car.x \\\n",
        "            - int(self.car.length / 2) > self.city_map.length - 5 \\\n",
        "            or self.car.y - int(self.car.width / 2) > self.city_map.width \\\n",
        "            - 5:\n",
        "            self.boundary_hit_count = self.boundary_hit_count + 1\n",
        "            self.reward = self.reward - 5.0\n",
        "    \n",
        "        if distance < self.last_distance:\n",
        "            self.reward = self.reward + 5\n",
        "        else:\n",
        "            self.reward = self.reward + 2\n",
        "    \n",
        "        if distance < 25:\n",
        "            self.reward = self.reward + 100\n",
        "    \n",
        "            self.goal_hit_count += 1\n",
        "    \n",
        "            if swap == 1:\n",
        "                print ('Hit the Goal 2: (' + str(goal_x) + ', ' \\\n",
        "                    + str(goal_y) + ')')\n",
        "                traversal_log.write('Train episode: '\n",
        "                                    + str(train_episode_num)\n",
        "                                    + ' Eval episode: '\n",
        "                                    + str(eval_episode_num)\n",
        "                                    + ' : Hit the Goal 2: (' + str(goal_x)\n",
        "                                    + ', ' + str(goal_y) + ')\\n')\n",
        "                self.goal_x = 575\n",
        "                self.goal_y = 530\n",
        "                self.swap = 0\n",
        "                done = True\n",
        "            else:\n",
        "                print ('Hit the Goal 1: (' + str(goal_x) + ', ' \\\n",
        "                    + str(goal_y) + ')')\n",
        "                traversal_log.write('Train episode: '\n",
        "                                    + str(train_episode_num)\n",
        "                                    + ' Eval episode: '\n",
        "                                    + str(eval_episode_num)\n",
        "                                    + ' : Hit the Goal 1: (' + str(goal_x)\n",
        "                                    + ', ' + str(goal_y) + ')\\n')\n",
        "                self.goal_x = 610\n",
        "                self.goal_y = 45\n",
        "                self.swap = 1\n",
        "                done = True\n",
        "    \n",
        "        self.last_distance = distance\n",
        "        self.current_step += 1\n",
        "    \n",
        "        img_crop = self.city.get_current_loc_map(self.car.x, self.car.y,\n",
        "                self.car.angle, state=True)\n",
        "        self.last_action = action\n",
        "        self.last_reward = self.reward\n",
        "    \n",
        "        return (img_crop, self.reward, done)\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        longueur = self.city.length\n",
        "        largeur = self.city.width\n",
        "\n",
        "        self.on_road_count = 0\n",
        "        self.off_road_count = 0\n",
        "        self.boundary_hit_count = 0\n",
        "        self.goal_hit_count = 0\n",
        "        self.episode_total_reward = 0.0\n",
        "        self.reward = 0\n",
        "        self.last_reward = 0\n",
        "        self.last_action = 0\n",
        "        self.goal_x = 575\n",
        "        self.goal_y = 530\n",
        "        self.swap = 0\n",
        "        self.last_distance = 0\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.car.angle = 0.0\n",
        "        self.car.x = np.random.randint(100, longueur - 100)\n",
        "        self.car.y = np.random.randint(100, largeur - 100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikf0RIYlSaRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "car_file = 'car.jpg'\n",
        "city_file = 'citymap.png'\n",
        "city_map_file = \"MASK1.png\"\n",
        "car_img = cv.imread(car_file)\n",
        "car1 = car(0,0,0)\n",
        "city1 = city(city_file)\n",
        "citymap1 = city(city_map_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kTDAbheShY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1: Initialize the Experience Replay memory\n",
        "# state[i] = [cropped image state] + [distance to target state]\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        (batch_states, batch_next_states, batch_actions, batch_rewards,\n",
        "         batch_dones) = ([], [], [], [], [])\n",
        "        for i in ind:\n",
        "            (state, next_state, action, reward, done) = self.storage[i]\n",
        "            batch_states.append(np.array(state, copy=False))\n",
        "            batch_next_states.append(np.array(next_state, copy=False))\n",
        "            batch_actions.append(np.array(action, copy=False))\n",
        "            batch_rewards.append(np.array(reward, copy=False))\n",
        "            batch_dones.append(np.array(done, copy=False))\n",
        "\n",
        "        return (np.array(batch_states), np.array(batch_next_states),\n",
        "                np.array(batch_actions),\n",
        "                np.array(batch_rewards).reshape(-1, 1),\n",
        "                np.array(batch_dones).reshape(-1, 1))\n",
        "\n",
        "\n",
        "# Step 2: Define CNN network for the Actor model\n",
        "# Same network is to be used for the Actor target\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim,\n",
        "        max_action,\n",
        "        ):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "    # input state image size will 40*40*1\n",
        "\n",
        "        self.convblock1 = nn.Sequential(nn.Conv2d(in_channels=1,\n",
        "                out_channels=8, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(8), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 38\n",
        "\n",
        "        self.convblock2 = nn.Sequential(nn.Conv2d(in_channels=8,\n",
        "                out_channels=16, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(16), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 36\n",
        "\n",
        "        self.convblock3 = nn.Sequential(nn.Conv2d(in_channels=16,\n",
        "                out_channels=16, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(16), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # putput_size = 34\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # output_size = 17\n",
        "        self.convblock4 = nn.Sequential(nn.Conv2d(in_channels=16,\n",
        "                out_channels=8, kernel_size=(1, 1), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(8), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 17\n",
        "\n",
        "        self.convblock5 = nn.Sequential(nn.Conv2d(in_channels=8,\n",
        "                out_channels=16, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(16), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 15\n",
        "\n",
        "        self.convblock6 = nn.Sequential(nn.Conv2d(in_channels=16,\n",
        "                out_channels=32, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 13\n",
        "\n",
        "        self.convblock7 = nn.Sequential(nn.Conv2d(in_channels=32,\n",
        "                out_channels=32, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 11\n",
        "\n",
        "        self.GAP = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "\n",
        "    # GAP to return 32 and rest of state values to be added before passing to fully connected layer\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim - 1 + 32, 400)\n",
        "        self.fc2 = nn.Linear(400, 300)\n",
        "        self.fc3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        x = self.convblock1(state[0])\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.GAP(x)\n",
        "        x = x.view(-1, 32)\n",
        "\n",
        "    # concatenate with rest of the state elements\n",
        "\n",
        "        x = torch.cat([x, state[1:]], 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.max_action * torch.tanh(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Step 3: Define neural networks for the two Critic models and Critic targets\n",
        "# Model will be same for all 4 critics\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "    # Defining the first Critic CNN based network\n",
        "\n",
        "        self.convblock1_1 = nn.Sequential(nn.Conv2d(in_channels=1,\n",
        "                out_channels=8, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(8), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 38\n",
        "\n",
        "        self.convblock1_2 = nn.Sequential(nn.Conv2d(in_channels=8,\n",
        "                out_channels=16, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(16), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 36\n",
        "\n",
        "        self.convblock1_3 = nn.Sequential(nn.Conv2d(in_channels=16,\n",
        "                out_channels=16, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(16), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # putput_size = 34\n",
        "\n",
        "        self.pool1_1 = nn.MaxPool2d(2, 2)  # output_size = 17\n",
        "        self.convblock1_4 = nn.Sequential(nn.Conv2d(in_channels=16,\n",
        "                out_channels=8, kernel_size=(1, 1), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(8), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 17\n",
        "\n",
        "        self.convblock1_5 = nn.Sequential(nn.Conv2d(in_channels=8,\n",
        "                out_channels=16, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(16), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 15\n",
        "\n",
        "        self.convblock1_6 = nn.Sequential(nn.Conv2d(in_channels=16,\n",
        "                out_channels=32, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 13\n",
        "\n",
        "        self.convblock1_7 = nn.Sequential(nn.Conv2d(in_channels=32,\n",
        "                out_channels=32, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(10), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 11\n",
        "\n",
        "        self.GAP1_1 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "\n",
        "    # GAP to return 32 and rest of state values to be added before passing to fully connected layer\n",
        "\n",
        "        self.fc1_1 = nn.Linear(state_dim - 1 + 32 + action_dim, 400)\n",
        "        self.fc1_2 = nn.Linear(400, 300)\n",
        "        self.fc1_3 = nn.Linear(300, 1)\n",
        "\n",
        "    # Defining the second Critic neural network\n",
        "\n",
        "        self.convblock2_1 = nn.Sequential(nn.Conv2d(in_channels=1,\n",
        "                out_channels=8, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(8), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 38\n",
        "\n",
        "        self.convblock2_2 = nn.Sequential(nn.Conv2d(in_channels=8,\n",
        "                out_channels=16, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(16), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 36\n",
        "\n",
        "        self.convblock2_3 = nn.Sequential(nn.Conv2d(in_channels=16,\n",
        "                out_channels=16, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(16), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # putput_size = 34\n",
        "\n",
        "        self.pool2_1 = nn.MaxPool2d(2, 2)  # output_size = 17\n",
        "        self.convblock2_4 = nn.Sequential(nn.Conv2d(in_channels=16,\n",
        "                out_channels=8, kernel_size=(1, 1), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(8), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 17\n",
        "\n",
        "        self.convblock2_5 = nn.Sequential(nn.Conv2d(in_channels=8,\n",
        "                out_channels=16, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(16), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 15\n",
        "\n",
        "        self.convblock2_6 = nn.Sequential(nn.Conv2d(in_channels=16,\n",
        "                out_channels=32, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 13\n",
        "\n",
        "        self.convblock2_7 = nn.Sequential(nn.Conv2d(in_channels=32,\n",
        "                out_channels=32, kernel_size=(3, 3), padding=0,\n",
        "                bias=False), nn.BatchNorm2d(10), nn.ReLU(),\n",
        "                nn.Dropout(0.1))  # output_size = 11\n",
        "\n",
        "        self.GAP2_1 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "\n",
        "    # we will have 32 values coming from GAP layer and (state_dim-1) other state values\n",
        "\n",
        "        self.fc2_1 = nn.Linear(state_dim - 1 + 32 + action_dim, 400)\n",
        "        self.fc2_2 = nn.Linear(400, 300)\n",
        "        self.fc2_3 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "\n",
        "    # first state element is cropped image\n",
        "    # Forward-Propagation of the first Critic Network\n",
        "\n",
        "        x1 = self.convblock1_1(state[0])\n",
        "        x1 = self.convblock1_2(x1)\n",
        "        x1 = self.convblock1_3(x1)\n",
        "        x1 = self.pool1_1(x1)\n",
        "        x1 = self.convblock1_4(x1)\n",
        "        x1 = self.convblock1_5(x1)\n",
        "        x1 = self.convblock1_6(x1)\n",
        "        x1 = self.convblock1_7(x1)\n",
        "        x1 = self.GAP1_1(x1)\n",
        "        x1 = x1.view(-1, 32)\n",
        "\n",
        "    # concatenate with rest of the state elements\n",
        "\n",
        "        x1 = torch.cat([x1, state[1:], action], 1)\n",
        "        x1 = F.relu(self.fc1_1(x1))\n",
        "        x1 = F.relu(self.fc1_2(x1))\n",
        "        x1 = self.fc1_3(x1)\n",
        "\n",
        "    # Forward-Propagation of the second Critic Network\n",
        "\n",
        "        x2 = self.convblock2_1(state[0])\n",
        "        x2 = self.convblock2_2(x2)\n",
        "        x2 = self.convblock2_3(x2)\n",
        "        x2 = self.pool2_1(x2)\n",
        "        x2 = self.convblock2_4(x2)\n",
        "        x2 = self.convblock2_5(x2)\n",
        "        x2 = self.convblock2_6(x2)\n",
        "        x2 = self.convblock2_7(x2)\n",
        "        x2 = self.GAP2_1(x2)\n",
        "        x2 = x2.view(-1, 32)\n",
        "\n",
        "    # concatenate with rest of the state elements\n",
        "\n",
        "        x2 = torch.cat([x2, state[1:], action], 1)\n",
        "        x2 = F.relu(self.fc2_1(x2))\n",
        "        x2 = F.relu(self.fc2_2(x2))\n",
        "        x2 = self.fc2_3(x2)\n",
        "\n",
        "        return (x1, x2)\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "\n",
        "    # Forward-Propagation of the first Critic Network\n",
        "\n",
        "        x1 = self.convblock1_1(state[0])\n",
        "        x1 = self.convblock1_2(x1)\n",
        "        x1 = self.convblock1_3(x1)\n",
        "        x1 = self.pool1_1(x1)\n",
        "        x1 = self.convblock1_4(x1)\n",
        "        x1 = self.convblock1_5(x1)\n",
        "        x1 = self.convblock1_6(x1)\n",
        "        x1 = self.convblock1_7(x1)\n",
        "        x1 = self.GAP1_1(x1)\n",
        "        x1 = x1.view(-1, 32)\n",
        "\n",
        "    # concatenate with rest of the state elements\n",
        "\n",
        "        x1 = torch.cat([x1, state[1:], action], 1)\n",
        "        x1 = F.relu(self.fc1_1(x1))\n",
        "        x1 = F.relu(self.fc1_2(x1))\n",
        "        x1 = self.fc1_3(x1)\n",
        "\n",
        "        return x1\n",
        "\n",
        "\n",
        "# Defining new class for step 4 to step 15\n",
        "# complete training Process is in the given class\n",
        "\n",
        "class TD3(object):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim,\n",
        "        max_action,\n",
        "        ):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim,\n",
        "                                  max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = \\\n",
        "            torch.optim.Adam(self.critic.parameters())\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        replay_buffer,\n",
        "        iterations,\n",
        "        batch_size=100,\n",
        "        discount=0.99,\n",
        "        tau=0.005,\n",
        "        policy_noise=0.2,\n",
        "        noise_clip=0.5,\n",
        "        policy_freq=2,\n",
        "        ):\n",
        "\n",
        "        for it in range(iterations):\n",
        "\n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "\n",
        "            (batch_states, batch_next_states, batch_actions,\n",
        "             batch_rewards, batch_dones) = \\\n",
        "                replay_buffer.sample(batch_size)\n",
        "            state = torch.Tensor(batch_states).to(device)\n",
        "            next_state = torch.Tensor(batch_next_states).to(device)\n",
        "            action = torch.Tensor(batch_actions).to(device)\n",
        "            reward = torch.Tensor(batch_rewards).to(device)\n",
        "            done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "\n",
        "            next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "\n",
        "            noise = torch.Tensor(batch_actions).data.normal_(0,\n",
        "                    policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (next_action + noise).clamp(-self.max_action,\n",
        "                    self.max_action)\n",
        "\n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "\n",
        "            (target_Q1, target_Q2) = self.critic_target(next_state,\n",
        "                    next_action)\n",
        "\n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "\n",
        "            target_Q = reward + ((1 - done) * discount\n",
        "                                 * target_Q).detach()\n",
        "\n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "\n",
        "            (current_Q1, current_Q2) = self.critic(state, action)\n",
        "\n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) \\\n",
        "                + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "\n",
        "            if it % policy_freq == 0:\n",
        "                actor_loss = -self.critic.Q1(state,\n",
        "                        self.actor(state)).mean()\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "\n",
        "                for (param, target_param) in \\\n",
        "                    zip(self.actor.parameters(),\n",
        "                        self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1\n",
        "                            - tau) * target_param.data)\n",
        "\n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "\n",
        "                for (param, target_param) in \\\n",
        "                    zip(self.critic.parameters(),\n",
        "                        self.critic_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1\n",
        "                            - tau) * target_param.data)\n",
        "\n",
        "  # Making a save method to save a trained model\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth'\n",
        "                   % (directory, filename))\n",
        "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth'\n",
        "                   % (directory, filename))\n",
        "\n",
        "  # Making a load method to load a pre-trained model\n",
        "\n",
        "    def load(self, filename, directory):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth'\n",
        "                                   % (directory, filename)))\n",
        "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth'\n",
        "                                    % (directory, filename)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0n7Ghq6FjT6",
        "colab_type": "text"
      },
      "source": [
        "**Function to evaluates the policy by calculating its average reward over 10 episodes**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1XWWXuaCSwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjkJaDH_F9EV",
        "colab_type": "text"
      },
      "source": [
        "**Set required parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px1GlkugCcwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = env(car1, city1, citymap1, car_img) # Instantiate the environment\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0IprWl6GGST",
        "colab_type": "text"
      },
      "source": [
        "**Define file name for the two saved models: the Actor and Critic models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxTpbN31FAJV",
        "colab_type": "code",
        "outputId": "7d8811b6-3bc9-425b-c826-7ca88972b8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "file_name = \"%s_%s\" % (\"TD3_car\", str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_car_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBYc674UGN57",
        "colab_type": "text"
      },
      "source": [
        "**Create a folder to store the trained models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH2k4DTwFX-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuLyuhNnGo9b",
        "colab_type": "text"
      },
      "source": [
        "**Set seeds and get necessary information on the states and actions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67MungvzGm96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = 2\n",
        "action_dim = 1\n",
        "max_action = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTx8yzUZGV20",
        "colab_type": "code",
        "outputId": "3deb6a06-b6f1-4d96-a25c-d234cba0eabc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env.reset()\n",
        "action = 30\n",
        "new_obs, reward, done = env.step(action)\n",
        "print(new_obs)\n",
        "print(done)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[[127, 104,  61],\n",
            "        [127, 104,  61],\n",
            "        [127, 104,  61],\n",
            "        ...,\n",
            "        [240, 244, 248],\n",
            "        [246, 252, 254],\n",
            "        [198, 193, 190]],\n",
            "\n",
            "       [[126, 103,  61],\n",
            "        [126, 104,  61],\n",
            "        [127, 104,  61],\n",
            "        ...,\n",
            "        [233, 236, 231],\n",
            "        [235, 237, 233],\n",
            "        [201, 203, 199]],\n",
            "\n",
            "       [[128, 105,  61],\n",
            "        [127, 103,  61],\n",
            "        [126, 104,  61],\n",
            "        ...,\n",
            "        [148, 142, 141],\n",
            "        [144, 134, 133],\n",
            "        [126, 120, 119]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[119,  89,  57],\n",
            "        [132, 104,  68],\n",
            "        [134, 110,  70],\n",
            "        ...,\n",
            "        [147, 145, 144],\n",
            "        [124, 124, 121],\n",
            "        [123, 113, 113]],\n",
            "\n",
            "       [[102,  72,  46],\n",
            "        [119,  91,  55],\n",
            "        [128, 101,  61],\n",
            "        ...,\n",
            "        [134, 129, 129],\n",
            "        [127, 119, 122],\n",
            "        [127, 115, 117]],\n",
            "\n",
            "       [[ 90,  61,  38],\n",
            "        [111,  81,  48],\n",
            "        [129, 100,  60],\n",
            "        ...,\n",
            "        [160, 151, 154],\n",
            "        [156, 146, 154],\n",
            "        [136, 121, 123]]], dtype=uint8), array([[0.38169935, 0.38169935, 0.38169935, 0.37908497, 0.36993464,\n",
            "        0.35686275, 0.59215686, 0.82352941, 0.70457516, 0.70326797,\n",
            "        0.70980392, 0.72679739, 0.73202614, 0.66666667, 0.5620915 ,\n",
            "        0.36993464, 0.30588235, 0.40784314, 0.39477124, 0.52026144,\n",
            "        0.59346405, 0.60915033, 0.77908497, 0.88496732, 0.96993464,\n",
            "        0.95686275, 0.95686275, 0.95686275, 0.98300654, 0.75947712],\n",
            "       [0.37908497, 0.38039216, 0.38169935, 0.37908497, 0.37385621,\n",
            "        0.36339869, 0.43267974, 0.52418301, 0.45359477, 0.51895425,\n",
            "        0.51111111, 0.49542484, 0.50588235, 0.63006536, 0.65882353,\n",
            "        0.45620915, 0.35686275, 0.39346405, 0.43398693, 0.48627451,\n",
            "        0.50457516, 0.57908497, 0.80522876, 0.80261438, 0.93464052,\n",
            "        0.96078431, 0.9372549 , 0.91503268, 0.92156863, 0.78823529],\n",
            "       [0.38431373, 0.38039216, 0.38039216, 0.37908497, 0.3869281 ,\n",
            "        0.38431373, 0.37124183, 0.36993464, 0.3869281 , 0.37124183,\n",
            "        0.35163399, 0.33856209, 0.49673203, 0.65882353, 0.63267974,\n",
            "        0.49281046, 0.37254902, 0.30196078, 0.32679739, 0.44313725,\n",
            "        0.4745098 , 0.55686275, 0.72026144, 0.77124183, 0.8379085 ,\n",
            "        0.79477124, 0.66405229, 0.56339869, 0.5372549 , 0.47712418],\n",
            "       [0.38823529, 0.38300654, 0.37908497, 0.38300654, 0.38823529,\n",
            "        0.3869281 , 0.3869281 , 0.38300654, 0.38431373, 0.38300654,\n",
            "        0.38300654, 0.37908497, 0.4496732 , 0.6745098 , 0.60653595,\n",
            "        0.53594771, 0.37647059, 0.3503268 , 0.32287582, 0.38954248,\n",
            "        0.47581699, 0.37647059, 0.66535948, 0.74509804, 0.86928105,\n",
            "        0.6130719 , 0.46535948, 0.38954248, 0.41699346, 0.41960784],\n",
            "       [0.38823529, 0.38300654, 0.37908497, 0.38562092, 0.38562092,\n",
            "        0.38169935, 0.38562092, 0.38431373, 0.38823529, 0.39084967,\n",
            "        0.39215686, 0.35816993, 0.40392157, 0.61176471, 0.63529412,\n",
            "        0.62091503, 0.44705882, 0.41437908, 0.37385621, 0.3372549 ,\n",
            "        0.50196078, 0.33333333, 0.47712418, 0.70196078, 0.87843137,\n",
            "        0.71895425, 0.39607843, 0.37908497, 0.42091503, 0.49803922],\n",
            "       [0.38039216, 0.37777778, 0.37908497, 0.37777778, 0.37647059,\n",
            "        0.37647059, 0.3751634 , 0.38562092, 0.38954248, 0.38954248,\n",
            "        0.38954248, 0.38039216, 0.39215686, 0.57647059, 0.64052288,\n",
            "        0.60392157, 0.43137255, 0.37777778, 0.47058824, 0.44313725,\n",
            "        0.47712418, 0.41830065, 0.40130719, 0.56993464, 0.91633987,\n",
            "        0.6379085 , 0.37908497, 0.37124183, 0.38954248, 0.45620915],\n",
            "       [0.37777778, 0.37777778, 0.37777778, 0.3751634 , 0.37908497,\n",
            "        0.37908497, 0.37777778, 0.3869281 , 0.39215686, 0.39084967,\n",
            "        0.39084967, 0.38562092, 0.38431373, 0.51633987, 0.66143791,\n",
            "        0.61045752, 0.45098039, 0.39607843, 0.44183007, 0.55424837,\n",
            "        0.50065359, 0.41960784, 0.34117647, 0.55686275, 0.86535948,\n",
            "        0.69542484, 0.38954248, 0.35555556, 0.35947712, 0.41045752],\n",
            "       [0.37647059, 0.3751634 , 0.3751634 , 0.37908497, 0.3751634 ,\n",
            "        0.37777778, 0.37908497, 0.37908497, 0.38039216, 0.37908497,\n",
            "        0.37908497, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.31764706, 0.36993464, 0.41699346,\n",
            "        0.42222222, 0.40261438, 0.27581699, 0.49803922, 0.8130719 ,\n",
            "        0.67712418, 0.38039216, 0.35294118, 0.37124183, 0.39346405],\n",
            "       [0.38562092, 0.3869281 , 0.38823529, 0.38300654, 0.3751634 ,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.30326797,\n",
            "        0.43529412, 0.43006536, 0.25098039, 0.45882353, 0.69673203,\n",
            "        0.68888889, 0.39215686, 0.35424837, 0.39869281, 0.42875817],\n",
            "       [0.40261438, 0.40522876, 0.40392157, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.42745098, 0.20915033, 0.30065359, 0.70326797,\n",
            "        0.78954248, 0.41045752, 0.36601307, 0.48366013, 0.50457516],\n",
            "       [0.39607843, 0.41176471, 0.40522876, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.44836601, 0.64705882,\n",
            "        0.59477124, 0.38300654, 0.42222222, 0.62222222, 0.66797386],\n",
            "       [0.39477124, 0.41045752, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.52287582, 0.3620915 , 0.46928105, 0.67973856, 0.73464052],\n",
            "       [0.39869281, 0.40130719, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.44052288, 0.6130719 , 0.63660131],\n",
            "       [0.40522876, 0.39869281, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.60784314, 0.61045752],\n",
            "       [0.39215686, 0.40261438, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.38169935, 0.38431373, 0.39477124, 0.51372549, 0.63137255,\n",
            "        0.71503268, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.60784314, 0.61568627],\n",
            "       [0.38300654, 0.39477124, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.37777778,\n",
            "        0.37647059, 0.37254902, 0.41830065, 0.58823529, 0.74509804,\n",
            "        0.74117647, 0.69281046, 0.58169935, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.59346405],\n",
            "       [0.38823529, 0.40392157, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.38954248,\n",
            "        0.37777778, 0.39477124, 0.36078431, 0.59346405, 0.73464052,\n",
            "        0.70588235, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.6745098 , 0.55424837],\n",
            "       [0.38954248, 0.40522876, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.38039216,\n",
            "        0.39738562, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.62091503, 0.65098039],\n",
            "       [0.38954248, 0.40653595, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.56993464, 0.55294118, 0.5869281 ],\n",
            "       [0.39607843, 0.40653595, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.49803922, 0.50326797, 0.55163399, 0.58823529, 0.5503268 ],\n",
            "       [0.38562092, 0.40653595, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.58039216, 0.47843137,\n",
            "        0.49411765, 0.48627451, 0.52810458, 0.61699346, 0.53071895],\n",
            "       [0.39084967, 0.40653595, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.44836601, 0.46928105, 0.6627451 , 0.48366013,\n",
            "        0.40653595, 0.42222222, 0.50849673, 0.63137255, 0.5254902 ],\n",
            "       [0.36732026, 0.40915033, 0.40653595, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.54248366,\n",
            "        0.3372549 , 0.43267974, 0.4130719 , 0.55816993, 0.61437908,\n",
            "        0.37647059, 0.51633987, 0.46535948, 0.57777778, 0.59477124],\n",
            "       [0.36732026, 0.41045752, 0.40653595, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.78431373, 0.78431373, 0.56470588, 0.59346405, 0.57385621,\n",
            "        0.43267974, 0.42222222, 0.31633987, 0.51764706, 0.70196078,\n",
            "        0.62091503, 0.5620915 , 0.44183007, 0.52941176, 0.64183007],\n",
            "       [0.3620915 , 0.41045752, 0.40653595, 0.4       , 0.38169935,\n",
            "        0.78431373, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
            "        0.38431373, 0.4496732 , 0.63660131, 0.6627451 , 0.47058824,\n",
            "        0.66928105, 0.70326797, 0.69934641, 0.68104575, 0.69150327,\n",
            "        0.65751634, 0.45098039, 0.41960784, 0.49019608, 0.68104575,\n",
            "        0.66013072, 0.73333333, 0.46143791, 0.46405229, 0.62875817],\n",
            "       [0.36601307, 0.39869281, 0.40392157, 0.4       , 0.38823529,\n",
            "        0.37908497, 0.37777778, 0.37647059, 0.38954248, 0.39477124,\n",
            "        0.3751634 , 0.37254902, 0.56601307, 0.67058824, 0.53071895,\n",
            "        0.51111111, 0.49281046, 0.47843137, 0.51111111, 0.59607843,\n",
            "        0.61830065, 0.51895425, 0.36862745, 0.40522876, 0.61830065,\n",
            "        0.63921569, 0.61176471, 0.48496732, 0.46535948, 0.59346405],\n",
            "       [0.36078431, 0.40130719, 0.40653595, 0.40653595, 0.39084967,\n",
            "        0.38169935, 0.37777778, 0.37777778, 0.36993464, 0.38562092,\n",
            "        0.38300654, 0.37124183, 0.46013072, 0.54248366, 0.48235294,\n",
            "        0.39084967, 0.3503268 , 0.34117647, 0.38431373, 0.48496732,\n",
            "        0.49542484, 0.51764706, 0.32418301, 0.42875817, 0.55947712,\n",
            "        0.52679739, 0.51895425, 0.57124183, 0.45228758, 0.53856209],\n",
            "       [0.34640523, 0.39738562, 0.41045752, 0.40653595, 0.39738562,\n",
            "        0.37777778, 0.37647059, 0.37777778, 0.36339869, 0.37385621,\n",
            "        0.37908497, 0.37777778, 0.40392157, 0.45751634, 0.44575163,\n",
            "        0.38039216, 0.38823529, 0.3620915 , 0.40261438, 0.53594771,\n",
            "        0.48366013, 0.49803922, 0.4248366 , 0.42222222, 0.57908497,\n",
            "        0.67320261, 0.60130719, 0.56993464, 0.48235294, 0.45620915],\n",
            "       [0.2875817 , 0.34640523, 0.37908497, 0.41176471, 0.40261438,\n",
            "        0.38431373, 0.37647059, 0.37908497, 0.37777778, 0.37254902,\n",
            "        0.3751634 , 0.37777778, 0.36470588, 0.4248366 , 0.45098039,\n",
            "        0.35163399, 0.39084967, 0.38169935, 0.38562092, 0.55163399,\n",
            "        0.52418301, 0.31633987, 0.43921569, 0.44183007, 0.52026144,\n",
            "        0.62222222, 0.60784314, 0.5124183 , 0.48104575, 0.46928105],\n",
            "       [0.24705882, 0.31372549, 0.37777778, 0.4248366 , 0.40522876,\n",
            "        0.3869281 , 0.37777778, 0.37908497, 0.38039216, 0.37124183,\n",
            "        0.37647059, 0.38562092, 0.38039216, 0.39607843, 0.40915033,\n",
            "        0.37647059, 0.38169935, 0.36601307, 0.35947712, 0.45098039,\n",
            "        0.50065359, 0.2627451 , 0.31372549, 0.41176471, 0.49673203,\n",
            "        0.59215686, 0.59738562, 0.60784314, 0.59607843, 0.49673203]]))\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}